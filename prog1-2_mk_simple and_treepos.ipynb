{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36973e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dev = []\n",
    "with open(\"data/TreePos_BI/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "for d in dev:\n",
    "    d[\"tree_pos\"] = [d[\"tags\"][i].split(\"_\")[0] for i in range(len(d[\"tags\"]))]\n",
    "    d[\"ner_pos\"] = [d[\"tags\"][i].split(\"_\")[1] for i in range(len(d[\"tags\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "822cffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train = []\n",
    "with open(\"data/TreePos_BI/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "for d in train:\n",
    "    d[\"tree_pos\"] = [d[\"tags\"][i].split(\"_\")[0] for i in range(len(d[\"tags\"]))]\n",
    "    d[\"ner_pos\"] = [d[\"tags\"][i].split(\"_\")[1] for i in range(len(d[\"tags\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16d68a2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# d= {'tokens': ['The',...,'.'],'tags': ['DT_O',...,'._O'],'tree_pos': ['DT',....'.'], 'ner_pos': ['O',...'O']}\n",
    "def make_data2list_Simple(data):\n",
    "    data_list = []\n",
    "    for d in dev:            \n",
    "        new_sent = []\n",
    "        before_tag = \"\"\n",
    "        new_token = []\n",
    "        for i in range(len(d[\"tags\"])):\n",
    "\n",
    "            if d[\"ner_pos\"][i] == \"O\" or i == len(d[\"tokens\"])-1:#固有表現タグがO　または　最後のトークンの時\n",
    "                if d[\"ner_pos\"][i] == before_tag:#1\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"ner_pos\"][i]])\n",
    "                    new_token = []\n",
    "                else:\n",
    "                    new_token = []#2\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"ner_pos\"][i]])\n",
    "                    new_token = []\n",
    "                    \n",
    "            else:#固有表現タグがBI#3\n",
    "                if d[\"ner_pos\"][i] == before_tag:\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_token = []\n",
    "                else:#4\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"ner_pos\"][i]])\n",
    "            before_tag = d[\"ner_pos\"][i]\n",
    "        data_list.append(new_sent)\n",
    "    return data_list\n",
    "# dev_list= [['Danish', 'rigsdaler'], 'other-currency'],[['='], 'O'],...\n",
    "\n",
    "\n",
    "def make_data2list_TreePos(data):\n",
    "    data_list = []\n",
    "    for d in dev:            \n",
    "        new_sent = []\n",
    "        before_tag = \"\"\n",
    "        new_token = []\n",
    "        new_tag = []\n",
    "        for i in range(len(d[\"tags\"])):\n",
    "\n",
    "            if d[\"ner_pos\"][i] == \"O\" or i == len(d[\"tokens\"])-1:#固有表現タグがO　または　最後のトークンの時\n",
    "                if d[\"ner_pos\"][i] == before_tag:#1\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_tag = list(set(new_tag))  #new_tagの重複削除\n",
    "                    new_tag.sort()                #new_tagの整列\n",
    "                    new_sent.append([new_token,\"__\".join(new_tag)])\n",
    "                    \n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                else:\n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                    \n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_tag = list(set(new_tag))  #new_tagの重複削除\n",
    "                    new_tag.sort()                #new_tagの整列\n",
    "                    new_sent.append([new_token,\"__\".join(new_tag)])\n",
    "                    \n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                    \n",
    "            else:#固有表現タグがBI\n",
    "                if d[\"ner_pos\"][i] == before_tag:\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                else:\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_tag = list(set(new_tag))  #new_tagの重複削除\n",
    "                    new_tag.sort()                #new_tagの整列\n",
    "                    new_sent.append([new_token,\"__\".join(new_tag)])#new_tagを__で結合\n",
    "                    \n",
    "            before_tag = d[\"ner_pos\"][i]\n",
    "        data_list.append(new_sent)\n",
    "    return data_list\n",
    "\n",
    "#交換先辞書の作成\n",
    "def make_alterna_labels(data_list):\n",
    "    label_list = set()\n",
    "    for s in data_list:\n",
    "        for t in s:\n",
    "            if t[1] != \"O\" or \"_O\" not in t[1][0]:\n",
    "                label_list.add(t[1])\n",
    "    \n",
    "    label_list = list(label_list)\n",
    "    alterna_labels = dict()\n",
    "    for label in label_list:\n",
    "        alterna_labels[label] = set()\n",
    "    for s in data_list:\n",
    "        for t in s:\n",
    "            token = \"_\".join(t[0])\n",
    "            tag = t[1]\n",
    "            if tag in label_list:\n",
    "                alterna_labels[tag].add(token)\n",
    "    \n",
    "    new_alterna_labels = dict()\n",
    "    for k,v in alterna_labels.items():#候補が空白or1つのtagは削除\n",
    "        if v==set() or len(v) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            new_alterna_labels[k] = v\n",
    "\n",
    "    return new_alterna_labels\n",
    "\n",
    "\n",
    "#Oタグのみデータを排除した増幅候補\n",
    "def make_data_has_BI(data_list):\n",
    "    data_has_BI = [] #16342(データ全体は18823)\n",
    "    for d in data_list:\n",
    "        tags_list = list(set([i[1] for i in d]))\n",
    "        if tags_list != [\"O\"]:\n",
    "            data_has_BI.append(d)\n",
    "    return data_has_BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea591264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "data_dict = dev[0:100]\n",
    "print(1)\n",
    "data_simple = make_data2list_Simple(data_dict)#tagが固有表現情報\n",
    "data_treepos = make_data2list_TreePos(data_dict)#tagが品詞＋固有表現情報\n",
    "print(2)\n",
    "alterna_labels_simple = make_alterna_labels(data_simple)#固有表現情報のみに着目した交換先辞書\n",
    "alterna_labels_treepos = make_alterna_labels(data_treepos)#品詞＋固有表現情報の交換先辞書\n",
    "data_has_BI = make_data_has_BI(data_treepos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1dc6b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Honda_J_V6', 'IBM_Personal_Computer'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alterna_labels_treepos[\"NNP_organization-company__NNP_product-other\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ef0eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in data_treepos:\n",
    "#     for row in s:\n",
    "#         if \"__\" in row[1]:\n",
    "#             print(s)\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a07e0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict = dev\n",
    "# data_treepos = make_data2list_TreePos(data_dict)#tagが品詞＋固有表現情報"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d4634ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# for s in data_treepos:\n",
    "#     for row in s:\n",
    "#         if len(row[0])>1:\n",
    "#             print(cnt)\n",
    "#             print(s)\n",
    "            \n",
    "#     cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fae5947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in alterna_labels_treepos:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "319ec721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]\n",
    "\n",
    "def Simple_TreePos_BI(datatype,data_dict,n,stoper):\n",
    "    #[[token_1,...token_n],tag],...[token_1,...token_n],tag]]状態のデータを作成\n",
    "    data_simple = make_data2list_Simple(data_dict)#tagが固有表現情報\n",
    "    data_treepos = make_data2list_TreePos(data_dict)#tagが品詞＋固有表現情報\n",
    "    \n",
    "    alterna_labels_simple = make_alterna_labels(data_simple)#固有表現情報のみに着目した交換先辞書\n",
    "    alterna_labels_treepos = make_alterna_labels(data_treepos)#品詞＋固有表現情報の交換先辞書\n",
    "    data_has_BI = make_data_has_BI(data_treepos)\n",
    "    #Oタグのみのデータを排除し，交換前候補として格納（treeposのほうが条件が厳しいのでそれを基準とする）\n",
    "    \n",
    "    #加工箇所を同じにするため，SimpleとTreePosを同時に作成する\n",
    "    data_simple_f_path = \"data/Simple_BI_x\"+str(n)+\"/\"+datatype+\".json\"\n",
    "    data_treepos_f_path = \"data/TreePos_BI_x\"+str(n)+\"/\"+datatype+\".json\"\n",
    "    memo_f_path = \"log/Simple_and_TreePos_BI_x\"+str(n)+\"_\"+datatype+\"_memo.txt\"\n",
    "\n",
    "    with open(data_simple_f_path,\"w\",encoding=\"utf-8\") as data_simple_f, \\\n",
    "    open(data_treepos_f_path,\"w\",encoding=\"utf-8\") as data_treepos_f, \\\n",
    "    open(memo_f_path,\"w\",encoding=\"utf-8\") as memo_f:\n",
    "        \n",
    "        for d in data_dict:\n",
    "            write_d = dict()\n",
    "            write_d[\"tokens\"] = d[\"tokens\"]\n",
    "            write_d[\"tags\"] = d[\"ner_pos\"]\n",
    "            data_simple_f.write(json.dumps(write_d))#Simpleにオリジナルデータを先に書込\n",
    "            data_simple_f.write(\"\\n\")\n",
    "            data_treepos_f.write(json.dumps(write_d))#TreePosにオリジナルデータを先に書込\n",
    "            data_treepos_f.write(\"\\n\")\n",
    "        \n",
    "        add_n = n*len(data_dict)-len(data_dict)#拡張データの数\n",
    "        cnt = 0\n",
    "        while cnt < add_n:\n",
    "            if cnt%10 == 0:\n",
    "                print('\\r%d / %d' %(cnt, add_n), end='')\n",
    "\n",
    "            x = random.randint(0,len(data_has_BI)-1)\n",
    "            x_s = data_has_BI[x]\n",
    "\n",
    "            new_d_simple = dict()\n",
    "            new_d_simple[\"tokens\"] = []\n",
    "            new_d_simple[\"tags\"] = []\n",
    "            \n",
    "            new_d_treepos = dict()\n",
    "            new_d_treepos[\"tokens\"] = []\n",
    "            new_d_treepos[\"tags\"] = []\n",
    "            old_tokens = []\n",
    "            \n",
    "            change = False\n",
    "            for token,tag_treepos in x_s:\n",
    "                tag_simple = tag_treepos.split(\"_\")[-1]\n",
    "                old_tokens.extend(token)\n",
    "                #固有表現であり，厳格な候補であるtreeposに交換対象が存在するとき，50%の確率で入れ替えを発生\n",
    "                if (tag_simple != \"O\") and (tag_treepos in alterna_labels_treepos) and (random.random() >= 0.5):\n",
    "                    \n",
    "                    #品詞+固有表現情報に基づく交換候補を生成\n",
    "                    token_simple = random_choice_token(alterna_labels_simple[tag_simple]-{\"_\".join(token)})\n",
    "                    token_simple = token_simple.split(\"_\")\n",
    "                    #交換したtokenを記録\n",
    "                    new_d_simple[\"tokens\"].extend(token_simple)\n",
    "                    #新しいtokenの数だけtagを追加\n",
    "                    for _ in token_simple:\n",
    "                        new_d_simple[\"tags\"].append(tag_simple)\n",
    "                        \n",
    "                    #固有表現情報に基づく交換候補を生成\n",
    "                    token_treepos = random_choice_token(alterna_labels_treepos[tag_treepos]-{\"_\".join(token)})\n",
    "                    token_treepos = token_treepos.split(\"_\")\n",
    "                    #交換したtokenを記録\n",
    "                    new_d_treepos[\"tokens\"].extend(token_treepos)\n",
    "                    #新しいtokenの数だけtagを追加\n",
    "                    for _ in token_treepos:#\n",
    "                        new_d_treepos[\"tags\"].append(tag_simple)\n",
    "                        \n",
    "                    change = True#書換えフラグ\n",
    "                else:\n",
    "                    new_d_simple[\"tokens\"].extend(token)\n",
    "                    new_d_treepos[\"tokens\"].extend(token)\n",
    "                    new_d_treepos[\"tags\"].append(tag_simple)\n",
    "\n",
    "            if change == True:#書換えが実行されたなら現在の一文をデータとして書出（改変前と同じ学習データの混入防止）\n",
    "                data_simple_f.write(json.dumps(new_d_simple))\n",
    "                data_simple_f.write(\"\\n\")\n",
    "                data_treepos_f.write(json.dumps(new_d_treepos))\n",
    "                data_treepos_f.write(\"\\n\")\n",
    "                \n",
    "                memo_f.write(str(x)+\"\\t\"+\" \".join(old_tokens)+\"\\n\")#memoに変更前と変更後を記入\n",
    "                memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d_simple[\"tokens\"])+\"\\n\")\n",
    "                memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d_treepos[\"tokens\"])+\"\\n\")\n",
    "                memo_f.write(\"\\n\")\n",
    "                cnt += 1\n",
    "            else:\n",
    "                pass\n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "24561393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18820 / 18823"
     ]
    }
   ],
   "source": [
    "Simple_TreePos_BI(\"dev\",dev,2,\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e3393bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131710 / 131719"
     ]
    }
   ],
   "source": [
    "Simple_TreePos_BI(\"train\",train,2,\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39eae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"rrr_job\"\n",
    "t = \"rrr\"\n",
    "t.split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e39c6982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The']\n",
      "['construction']\n",
      "['of']\n",
      "['the']\n",
      "['airport']\n",
      "['was']\n",
      "['done']\n",
      "['mainly']\n",
      "['by']\n",
      "['a']\n",
      "['few']\n",
      "['state']\n",
      "['owned']\n",
      "['construction']\n",
      "['companies']\n",
      "['as']\n",
      "['well']\n",
      "['as']\n",
      "['Ekovest', 'Berhad']\n",
      "['–']\n",
      "['helmed']\n",
      "['by']\n",
      "['Tan', 'Sri']\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "d = [[['The'], 'O'], [['construction'], 'O'], [['of'], 'O'], [['the'], 'O'], [['airport'], 'O'], [['was'], 'O'], [['done'], 'O'], [['mainly'], 'O'], [['by'], 'O'], [['a'], 'O'], [['few'], 'O'], [['state'], 'O'], [['owned'], 'O'], [['construction'], 'O'], [['companies'], 'O'], [['as'], 'O'], [['well'], 'O'], [['as'], 'O'], [['Ekovest', 'Berhad'], 'organization-company'], [['–'], 'O'], [['helmed'], 'O'], [['by'], 'O'], [['Tan', 'Sri'], 'person-other'], [['.'], 'O']]\n",
    "\n",
    "for token,tag in d:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3ae81ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "-1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    if random.random() >= 0.5:#50%の確率で入れ替えを発生\n",
    "        print(1)\n",
    "    else:\n",
    "        print(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad1f455",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
