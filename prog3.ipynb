{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e11d3b12",
   "metadata": {},
   "source": [
    "# SMLデータを新たに作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98c9832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train = []\n",
    "with open(\"data/TreePos_BI/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "for d in train:\n",
    "    d[\"tree_pos\"] = [d[\"tags\"][i].split(\"_\")[0] for i in range(len(d[\"tags\"]))]\n",
    "    d[\"ner_pos\"] = [d[\"tags\"][i].split(\"_\")[1] for i in range(len(d[\"tags\"]))]\n",
    "\n",
    "train_includeBI = [d for d in train if list(set(d[\"ner_pos\"])) != [\"O\"]]\n",
    "\n",
    "# with open(\"data/TreePos_BI_small/50/train_50.json\", 'w') as f:\n",
    "#     for d in train_includeBI[0:50]:\n",
    "#         f.write(json.dumps(d))\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "# with open(\"data/TreePos_BI_small/150/train_150.json\", 'w') as f:\n",
    "#     for d in train_includeBI[0:150]:\n",
    "#         f.write(json.dumps(d))\n",
    "#         f.write(\"\\n\")\n",
    "\n",
    "# with open(\"data/TreePos_BI_small/500/train_500.json\", 'w') as f:\n",
    "#     for d in train_includeBI[0:500]:\n",
    "#         f.write(json.dumps(d))\n",
    "#         f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5d9a50",
   "metadata": {},
   "source": [
    "# データ拡張の作り直し"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee5635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_50 = train_includeBI[0:50]\n",
    "train_150 = train_includeBI[0:150]\n",
    "train_500 = train_includeBI[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d15eef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d= {'tokens': ['The',...,'.'],'tags': ['DT_O',...,'._O'],'tree_pos': ['DT',....'.'], 'ner_pos': ['O',...'O']}\n",
    "def make_data2list_Simple(data):\n",
    "    data_list = []\n",
    "    for d in data:            \n",
    "        new_sent = []\n",
    "        before_tag = \"\"\n",
    "        new_token = []\n",
    "        for i in range(len(d[\"tags\"])):\n",
    "\n",
    "            if d[\"ner_pos\"][i] == \"O\" or i == len(d[\"tokens\"])-1:#固有表現タグがO　または　最後のトークンの時\n",
    "                if d[\"ner_pos\"][i] == before_tag:#1\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"ner_pos\"][i]])\n",
    "                    new_token = []\n",
    "                else:\n",
    "                    new_token = []#2\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"ner_pos\"][i]])\n",
    "                    new_token = []\n",
    "                    \n",
    "            else:#固有表現タグがBI#3\n",
    "                if d[\"ner_pos\"][i] == before_tag:\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_token = []\n",
    "                else:#4\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"ner_pos\"][i]])\n",
    "            before_tag = d[\"ner_pos\"][i]\n",
    "        data_list.append(new_sent)\n",
    "    return data_list\n",
    "# dev_list= [['Danish', 'rigsdaler'], 'other-currency'],[['='], 'O'],...\n",
    "\n",
    "\n",
    "def make_data2list_TreePos(data):\n",
    "    data_list = []\n",
    "    for d in data:            \n",
    "        new_sent = []\n",
    "        before_tag = \"\"\n",
    "        new_token = []\n",
    "        new_tag = []\n",
    "        for i in range(len(d[\"tags\"])):\n",
    "\n",
    "            if d[\"ner_pos\"][i] == \"O\" or i == len(d[\"tokens\"])-1:#固有表現タグがO　または　最後のトークンの時\n",
    "                if d[\"ner_pos\"][i] == before_tag:#1\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_tag = list(set(new_tag))  #new_tagの重複削除\n",
    "                    new_tag.sort()                #new_tagの整列\n",
    "                    new_sent.append([new_token,\"__\".join(new_tag)])\n",
    "                    \n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                else:\n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                    \n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_tag = list(set(new_tag))  #new_tagの重複削除\n",
    "                    new_tag.sort()                #new_tagの整列\n",
    "                    new_sent.append([new_token,\"__\".join(new_tag)])\n",
    "                    \n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                    \n",
    "            else:#固有表現タグがBI\n",
    "                if d[\"ner_pos\"][i] == before_tag:\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                else:\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_tag = list(set(new_tag))  #new_tagの重複削除\n",
    "                    new_tag.sort()                #new_tagの整列\n",
    "                    new_sent.append([new_token,\"__\".join(new_tag)])#new_tagを__で結合\n",
    "                    \n",
    "            before_tag = d[\"ner_pos\"][i]\n",
    "        data_list.append(new_sent)\n",
    "    return data_list\n",
    "\n",
    "#交換先辞書の作成\n",
    "def make_alterna_labels(data_list):\n",
    "    label_list = set()\n",
    "    for s in data_list:\n",
    "        for t in s:\n",
    "#             if t[1] != \"O\" and \"_O\" not in t[1][0]:\n",
    "            label_list.add(t[1])\n",
    "    \n",
    "    label_list = list(label_list)\n",
    "    alterna_labels = dict()\n",
    "    for label in label_list:\n",
    "        alterna_labels[label] = set()\n",
    "    for s in data_list:\n",
    "        for t in s:\n",
    "            token = \"_\".join(t[0])\n",
    "            tag = t[1]\n",
    "            if tag in label_list:\n",
    "                alterna_labels[tag].add(token)\n",
    "    \n",
    "    new_alterna_labels = dict()\n",
    "    for k,v in alterna_labels.items():#候補が空白or1つのtagは削除\n",
    "        if v==set() or len(v) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            new_alterna_labels[k] = v\n",
    "\n",
    "    return new_alterna_labels\n",
    "\n",
    "\n",
    "def make_alterna_labels_for_O(data_list):\n",
    "    label_list = set()\n",
    "    for s in data_list:\n",
    "        for t in s:\n",
    "            if t[1] == \"O\" or \"_O\" in t[1][0]:\n",
    "                label_list.add(t[1])\n",
    "    \n",
    "    label_list = list(label_list)\n",
    "    alterna_labels = dict()\n",
    "    for label in label_list:\n",
    "        alterna_labels[label] = set()\n",
    "    for s in data_list:\n",
    "        for t in s:\n",
    "            token = \"_\".join(t[0])\n",
    "            tag = t[1]\n",
    "            if tag in label_list:\n",
    "                alterna_labels[tag].add(token)\n",
    "    \n",
    "    new_alterna_labels = dict()\n",
    "    for k,v in alterna_labels.items():#候補が空白or1つのtagは削除\n",
    "        if v==set() or len(v) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            new_alterna_labels[k] = v\n",
    "\n",
    "    return new_alterna_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a85a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "parm_exchange = 0.5\n",
    "parm_generate_per_s = 7#オリジナルデータの何倍データを作るか（生成後データにはオリジナルデータも含む）\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]\n",
    "\n",
    "def Simple_TreePos(datatype,data_dict,BI_or_O):\n",
    "    #[[token_1,...token_n],tag],...[token_1,...token_n],tag]]状態のデータを作成\n",
    "    data_simple = make_data2list_Simple(data_dict)#tagが固有表現情報\n",
    "    data_treepos = make_data2list_TreePos(data_dict)#tagが品詞＋固有表現情報\n",
    "    alterna_labels_simple = make_alterna_labels(data_simple)#固有表現情報のみに着目した交換先辞書\n",
    "    alterna_labels_treepos = make_alterna_labels(data_treepos)#品詞＋固有表現情報の交換先辞書\n",
    "    \n",
    "    #加工箇所を同じにするため，SimpleとTreePosを同時に作成する\n",
    "    data_simple_f_path = \"data/Simple_\"+BI_or_O+\"_small/\"+datatype+\".json\"\n",
    "    data_treepos_f_path = \"data/TreePos_\"+BI_or_O+\"_small/\"+datatype+\".json\"\n",
    "    memo_f_path = \"log/TreePos_\"+BI_or_O+\"_small_\"+datatype+\"_memo.txt\"\n",
    "\n",
    "    with open(data_simple_f_path,\"w\",encoding=\"utf-8\") as data_simple_f, \\\n",
    "    open(data_treepos_f_path,\"w\",encoding=\"utf-8\") as data_treepos_f, \\\n",
    "    open(memo_f_path,\"w\",encoding=\"utf-8\") as memo_f:\n",
    "        \n",
    "        for i in range(len(data_dict)):\n",
    "            d = data_dict[i]\n",
    "            write_d = dict()\n",
    "            write_d[\"tokens\"] = d[\"tokens\"]\n",
    "            write_d[\"tags\"] = d[\"ner_pos\"]\n",
    "            data_simple_f.write(json.dumps(write_d))#Simpleにオリジナルデータを先に書込\n",
    "            data_simple_f.write(\"\\n\")\n",
    "            data_treepos_f.write(json.dumps(write_d))#TreePosにオリジナルデータを先に書込\n",
    "            data_treepos_f.write(\"\\n\")\n",
    "            \n",
    "            cnt = 0\n",
    "            roop_cnt = 0\n",
    "            while cnt<parm_generate_per_s-1 and roop_cnt<30:\n",
    "                roop_cnt += 1\n",
    "                new_d_simple = dict()\n",
    "                new_d_simple[\"tokens\"] = []\n",
    "                new_d_simple[\"tags\"] = []\n",
    "\n",
    "                new_d_treepos = dict()\n",
    "                new_d_treepos[\"tokens\"] = []\n",
    "                new_d_treepos[\"tags\"] = []\n",
    "                old_tokens = []\n",
    "\n",
    "                sentence_contain_change = False\n",
    "                \n",
    "                for token,tag_treepos in data_treepos[i]:\n",
    "                    tag_simple = tag_treepos.split(\"_\")[-1]\n",
    "                    old_tokens.extend(token)\n",
    "                    \n",
    "                    #固有表現であり，厳格な候補であるtreeposに交換対象が存在するとき，50%の確率で入れ替えを発生\n",
    "                    if  (tag_treepos in alterna_labels_treepos) and (random.random() >= parm_exchange):\n",
    "                        if ((BI_or_O == \"BI\") and (tag_simple != \"O\")) or ((BI_or_O == \"BI\") and (tag_simple == \"O\")):\n",
    "\n",
    "                            #固有表現情報に基づく交換候補を生成\n",
    "                            token_simple = random_choice_token(alterna_labels_simple[tag_simple]-{\"_\".join(token)})\n",
    "                            token_simple = token_simple.split(\"_\")\n",
    "                            #交換したtokenを記録\n",
    "                            new_d_simple[\"tokens\"].extend(token_simple)\n",
    "                            #新しいtokenの数だけtagを追加\n",
    "                            for _ in token_simple:\n",
    "                                new_d_simple[\"tags\"].append(tag_simple)\n",
    "\n",
    "                            #品詞＋固有表現情報に基づく交換候補を生成\n",
    "                            token_treepos = random_choice_token(alterna_labels_treepos[tag_treepos]-{\"_\".join(token)})\n",
    "                            token_treepos = token_treepos.split(\"_\")\n",
    "                            #交換したtokenを記録\n",
    "                            new_d_treepos[\"tokens\"].extend(token_treepos)\n",
    "                            #新しいtokenの数だけtagを追加\n",
    "                            for _ in token_treepos:#\n",
    "                                new_d_treepos[\"tags\"].append(tag_simple)\n",
    "                            sentence_contain_change = True#書換えフラグ\n",
    "\n",
    "                    else:#交換しない固有表現タグ付きのtoken群（同じタグ唖連続している場合の人まとまり）\n",
    "                        new_d_simple[\"tokens\"].extend(token)\n",
    "                        for _ in token:\n",
    "                            new_d_simple[\"tags\"].append(tag_simple)\n",
    "\n",
    "                        new_d_treepos[\"tokens\"].extend(token)\n",
    "                        for _ in token:\n",
    "                            new_d_treepos[\"tags\"].append(tag_simple)\n",
    "\n",
    "                if sentence_contain_change == True:#書換えが実行されたなら現在の一文をデータとして書出（改変前と同じ学習データの混入防止）\n",
    "                    if (len(new_d_simple[\"tokens\"]) == len(new_d_simple[\"tags\"])) and (len(new_d_treepos[\"tokens\"]) == len(new_d_treepos[\"tags\"])):\n",
    "                        data_simple_f.write(json.dumps(new_d_simple))\n",
    "                        data_simple_f.write(\"\\n\")\n",
    "                        data_treepos_f.write(json.dumps(new_d_treepos))\n",
    "                        data_treepos_f.write(\"\\n\")\n",
    "\n",
    "                        memo_f.write(str(i)+\"\\t\"+\" \".join(old_tokens)+\"\\n\")#memoに変更前と変更後を記入\n",
    "                        memo_f.write(\" \"*len(str(i))+\"\\t\"+\" \".join(new_d_simple[\"tokens\"])+\"\\n\")\n",
    "                        memo_f.write(\" \"*len(str(i))+\"\\t\"+\" \".join(new_d_treepos[\"tokens\"])+\"\\n\")\n",
    "                        memo_f.write(\"\\n\")\n",
    "                        cnt+=1\n",
    "                else:\n",
    "                    pass\n",
    "        memo_f.write(\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "021b6b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple_TreePos(\"train_50\",train_50,\"BI\")\n",
    "Simple_TreePos(\"train_150\",train_150,\"BI\")\n",
    "Simple_TreePos(\"train_500\",train_500,\"BI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ab073394",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple_TreePos(\"train_50\",train_50,\"O\")\n",
    "Simple_TreePos(\"train_150\",train_150,\"O\")\n",
    "Simple_TreePos(\"train_500\",train_500,\"O\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932311b2",
   "metadata": {},
   "source": [
    "## 11/28 ラベルごとの評価を観察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fc87b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a60d2520",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load('seqeval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeee828",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [5,7,10,15,20]:\n",
    "# for n in [3]:\n",
    "    for task_name in [\"f\",\"Simple_BI_x2\",\"Simple_O_x2\",\"TreePos_BI_x2\",\"TreePos_O_x2\"]:\n",
    "        print(n,\" \",task_name)\n",
    "\n",
    "        references = []\n",
    "        with open(\"data/f/test.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                references.append(json.loads(line)[\"tags\"])\n",
    "                \n",
    "        predictions = []\n",
    "        with open(\"result/epochs\"+str(n)+\"/\"+task_name+\"/predictions.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                predictions.append(line.replace(\"\\n\",\"\").split(\" \"))\n",
    "\n",
    "        label_set = set()\n",
    "        for l in references+predictions:\n",
    "            for label in list(set(l)):\n",
    "                label_set.add(label)\n",
    "\n",
    "        label_set.remove(\"O\")\n",
    "        label_d = dict()\n",
    "        for k in label_set:\n",
    "            label_d[k] = \"L\"+k.replace(\"/\",\"_\").replace(\"-\",\"_\")\n",
    "\n",
    "        for l in references:\n",
    "            for i in range(len(l)):\n",
    "                if l[i] in label_d:\n",
    "                    l[i] = label_d[l[i]]\n",
    "\n",
    "        for l in predictions:\n",
    "            for i in range(len(l)):\n",
    "                if l[i] in label_d:\n",
    "                    l[i] = label_d[l[i]]\n",
    "        \n",
    "        results = seqeval.compute(predictions=predictions, references=references)\n",
    "        print(1)\n",
    "        results_overall = {}\n",
    "        results_overall['overall_precision']=results['overall_precision']\n",
    "        results_overall['overall_recall']=results['overall_recall']\n",
    "        results_overall['overall_f1']=results['overall_f1']\n",
    "        results_overall['overall_accuracy']=results['overall_accuracy']\n",
    "        \n",
    "        del results['overall_precision']\n",
    "        del results['overall_recall']\n",
    "        del results['overall_f1']\n",
    "        del results['overall_accuracy']#項目に含まれる数値のみを値とするkeyを削除\n",
    "        \n",
    "        with open(\"result/all_label_result/epoch\"+str(n)+\"_\"+task_name+\"_overall.csv\", 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=[\"overall_precision\",\"overall_recall\",\"overall_f1\",\"overall_accuracy\"],lineterminator='\\n')\n",
    "            writer.writeheader()\n",
    "            writer.writerow(results_overall)\n",
    "            \n",
    "        print(2)\n",
    "        result_csv = []\n",
    "        for k in results.keys():\n",
    "            results[k][\"label\"] = k\n",
    "            result_csv.append(results[k])\n",
    "\n",
    "        with open(\"result/all_label_result/epoch\"+str(n)+\"_\"+task_name+\"_each_label.csv\", 'w') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=[\"label\",\"precision\",\"recall\",\"f1\",\"number\"],lineterminator='\\n')\n",
    "            writer.writeheader()\n",
    "            writer.writerows([results[data] for data in results])\n",
    "\n",
    "!rundll32 user32.dll,MessageBeep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debafc1d",
   "metadata": {},
   "source": [
    "## 各タグのサイズに配慮したSML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "02a7ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train = []#131719\n",
    "with open(\"data/TreePos_BI/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "for d in train:\n",
    "    d[\"tree_pos\"] = [d[\"tags\"][i].split(\"_\")[0] for i in range(len(d[\"tags\"]))]\n",
    "    d[\"ner_pos\"] = [d[\"tags\"][i].split(\"_\")[1] for i in range(len(d[\"tags\"]))]\n",
    "\n",
    "train_includeBI = [d for d in train if list(set(d[\"ner_pos\"])) != [\"O\"]]#113983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d2391de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"art-broadcastprogram\",\"art-film\",\"art-music\",\"art-other\",\"art-painting\",\"art-writtenart\",\"building-airport\",\"building-hospital\",\"building-hotel\",\"building-library\",\"building-other\",\"building-restaurant\",\"building-sportsfacility\",\"building-theater\",\"event-attack/battle/war/militaryconflict\",\"event-disaster\",\"event-election\",\"event-other\",\"event-protest\",\"event-sportsevent\",\"location-GPE\",\"location-bodiesofwater\",\"location-island\",\"location-mountain\",\"location-other\",\"location-park\",\"location-road/railway/highway/transit\",\"organization-company\",\"organization-education\",\"organization-government/governmentagency\",\"organization-media/newspaper\",\"organization-other\",\"organization-politicalparty\",\"organization-religion\",\"organization-showorganization\",\"organization-sportsleague\",\"organization-sportsteam\",\"other-astronomything\",\"other-award\",\"other-biologything\",\"other-chemicalthing\",\"other-currency\",\"other-disease\",\"other-educationaldegree\",\"other-god\",\"other-language\",\"other-law\",\"other-livingthing\",\"other-medical\",\"person-actor\",\"person-artist/author\",\"person-athlete\",\"person-director\",\"person-other\",\"person-politician\",\"person-scholar\",\"person-soldier\",\"product-airplane\",\"product-car\",\"product-food\",\"product-game\",\"product-other\",\"product-ship\",\"product-software\",\"product-train\",\"product-weapon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c39f0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 66種のラベルがkey ラベルを含むtrainの各要素を含むリストが値\n",
    "labeled_data = []\n",
    "for label in labels:\n",
    "    new_l = []\n",
    "    for d in train_includeBI:\n",
    "        if label in set(d[\"ner_pos\"]):\n",
    "            new_l.append(d)\n",
    "    labeled_data.append(new_l)#listに保存\n",
    "#     with open(\"data/f_label/\"+label+\"__\"+str(len(new_l))+\".json\",\"w\",encoding=\"utf-8\") as f:#種類別ファイルに保存\n",
    "#         for d in new_l:\n",
    "#             f.write(json.dumps(d))\n",
    "#             f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93f9c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "#各ラベルごとに10分の1のスケールとしたデータを作成\n",
    "import random\n",
    "labeled_data_elected = []#21548\n",
    "\n",
    "n = 10\n",
    "for i in range(len(labeled_data)):\n",
    "    choice_num = int(len(labeled_data[i])/n)\n",
    "    choice_list = random.sample(labeled_data[i],choice_num)\n",
    "    labeled_data_elected.extend(choice_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1a6fa5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "parm_exchange = 0.5\n",
    "parm_generate_per_s = 7#オリジナルデータの何倍データを作るか（生成後データにはオリジナルデータも含む）\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]\n",
    "\n",
    "def Simple_TreePos(datatype,data_dict,BI_or_O):\n",
    "    print(1)\n",
    "    #[[token_1,...token_n],tag],...[token_1,...token_n],tag]]状態のデータを作成\n",
    "    data_simple = make_data2list_Simple(data_dict)#tagが固有表現情報\n",
    "    data_treepos = make_data2list_TreePos(data_dict)#tagが品詞＋固有表現情報\n",
    "    alterna_labels_simple = make_alterna_labels(data_simple)#固有表現情報のみに着目した交換先辞書\n",
    "    alterna_labels_treepos = make_alterna_labels(data_treepos)#品詞＋固有表現情報の交換先辞書\n",
    "    print(2)\n",
    "    #加工箇所を同じにするため，SimpleとTreePosを同時に作成する\n",
    "    data_simple_f_path = \"data/Simple_\"+BI_or_O+\"_small/\"+datatype+\".json\"\n",
    "    data_treepos_f_path = \"data/TreePos_\"+BI_or_O+\"_small/\"+datatype+\".json\"\n",
    "    memo_f_path = \"log/TreePos_\"+BI_or_O+\"_small_\"+datatype+\"_memo.txt\"\n",
    "    print(3)\n",
    "    with open(data_simple_f_path,\"w\",encoding=\"utf-8\") as data_simple_f, \\\n",
    "    open(data_treepos_f_path,\"w\",encoding=\"utf-8\") as data_treepos_f, \\\n",
    "    open(memo_f_path,\"w\",encoding=\"utf-8\") as memo_f:\n",
    "        \n",
    "        for i in range(len(data_dict)):\n",
    "            print('\\r%d / %d' %(i, len(data_dict)), end='')\n",
    "            \n",
    "            d = data_dict[i]\n",
    "            write_d = dict()\n",
    "            write_d[\"tokens\"] = d[\"tokens\"]\n",
    "            write_d[\"tags\"] = d[\"ner_pos\"]\n",
    "            data_simple_f.write(json.dumps(write_d))#Simpleにオリジナルデータを先に書込\n",
    "            data_simple_f.write(\"\\n\")\n",
    "            data_treepos_f.write(json.dumps(write_d))#TreePosにオリジナルデータを先に書込\n",
    "            data_treepos_f.write(\"\\n\")\n",
    "            \n",
    "            cnt = 0\n",
    "            roop_cnt = 0\n",
    "            while cnt<parm_generate_per_s-1 and roop_cnt<30:\n",
    "                \n",
    "                roop_cnt += 1\n",
    "                new_d_simple = dict()\n",
    "                new_d_simple[\"tokens\"] = []\n",
    "                new_d_simple[\"tags\"] = []\n",
    "\n",
    "                new_d_treepos = dict()\n",
    "                new_d_treepos[\"tokens\"] = []\n",
    "                new_d_treepos[\"tags\"] = []\n",
    "                old_tokens = []\n",
    "\n",
    "                sentence_contain_change = False\n",
    "                \n",
    "                for token,tag_treepos in data_treepos[i]:\n",
    "                    tag_simple = tag_treepos.split(\"_\")[-1]\n",
    "                    old_tokens.extend(token)\n",
    "                    \n",
    "                    #固有表現であり，厳格な候補であるtreeposに交換対象が存在するとき，50%の確率で入れ替えを発生\n",
    "                    if  (tag_treepos in alterna_labels_treepos) and (random.random() >= parm_exchange):\n",
    "                        if ((BI_or_O == \"BI\") and (tag_simple != \"O\")) or ((BI_or_O == \"BI\") and (tag_simple == \"O\")):\n",
    "\n",
    "                            #固有表現情報に基づく交換候補を生成\n",
    "                            token_simple = random_choice_token(alterna_labels_simple[tag_simple]-{\"_\".join(token)})\n",
    "                            token_simple = token_simple.split(\"_\")\n",
    "                            #交換したtokenを記録\n",
    "                            new_d_simple[\"tokens\"].extend(token_simple)\n",
    "                            #新しいtokenの数だけtagを追加\n",
    "                            for _ in token_simple:\n",
    "                                new_d_simple[\"tags\"].append(tag_simple)\n",
    "\n",
    "                            #品詞＋固有表現情報に基づく交換候補を生成\n",
    "                            token_treepos = random_choice_token(alterna_labels_treepos[tag_treepos]-{\"_\".join(token)})\n",
    "                            token_treepos = token_treepos.split(\"_\")\n",
    "                            #交換したtokenを記録\n",
    "                            new_d_treepos[\"tokens\"].extend(token_treepos)\n",
    "                            #新しいtokenの数だけtagを追加\n",
    "                            for _ in token_treepos:#\n",
    "                                new_d_treepos[\"tags\"].append(tag_simple)\n",
    "                            sentence_contain_change = True#書換えフラグ\n",
    "\n",
    "                    else:#交換しない固有表現タグ付きのtoken群（同じタグ唖連続している場合の人まとまり）\n",
    "                        new_d_simple[\"tokens\"].extend(token)\n",
    "                        for _ in token:\n",
    "                            new_d_simple[\"tags\"].append(tag_simple)\n",
    "\n",
    "                        new_d_treepos[\"tokens\"].extend(token)\n",
    "                        for _ in token:\n",
    "                            new_d_treepos[\"tags\"].append(tag_simple)\n",
    "\n",
    "                if sentence_contain_change == True:#書換えが実行されたなら現在の一文をデータとして書出（改変前と同じ学習データの混入防止）\n",
    "                    if (len(new_d_simple[\"tokens\"]) == len(new_d_simple[\"tags\"])) and (len(new_d_treepos[\"tokens\"]) == len(new_d_treepos[\"tags\"])):\n",
    "                        data_simple_f.write(json.dumps(new_d_simple))\n",
    "                        data_simple_f.write(\"\\n\")\n",
    "                        data_treepos_f.write(json.dumps(new_d_treepos))\n",
    "                        data_treepos_f.write(\"\\n\")\n",
    "\n",
    "                        memo_f.write(str(i)+\"\\t\"+\" \".join(old_tokens)+\"\\n\")#memoに変更前と変更後を記入\n",
    "                        memo_f.write(\" \"*len(str(i))+\"\\t\"+\" \".join(new_d_simple[\"tokens\"])+\"\\n\")\n",
    "                        memo_f.write(\" \"*len(str(i))+\"\\t\"+\" \".join(new_d_treepos[\"tokens\"])+\"\\n\")\n",
    "                        memo_f.write(\"\\n\")\n",
    "                        cnt+=1\n",
    "                else:\n",
    "                    pass\n",
    "        memo_f.write(\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "976b8027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "21547 / 21548"
     ]
    }
   ],
   "source": [
    "Simple_TreePos(\"labeled_small\",labeled_data_elected,\"BI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd5ad2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "21547 / 21548"
     ]
    }
   ],
   "source": [
    "Simple_TreePos(\"labeled_small\",labeled_data_elected,\"O\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b6d757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = labeled_data_elected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1fdd2515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "data_simple = make_data2list_Simple(data_dict)#tagが固有表現情報\n",
    "print(1)\n",
    "data_treepos = make_data2list_TreePos(data_dict)#tagが品詞＋固有表現情報\n",
    "print(1)\n",
    "alterna_labels_simple = make_alterna_labels(data_simple)#固有表現情報のみに着目した交換先辞書\n",
    "print(1)\n",
    "alterna_labels_treepos = make_alterna_labels(data_treepos)#品詞＋固有表現情報の交換先辞書"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cf2f9aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Chick-fil-A', 'The_Anchor', 'The_Cheesecake', 'The_French', 'The_Mad'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alterna_labels_treepos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e794cd2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
