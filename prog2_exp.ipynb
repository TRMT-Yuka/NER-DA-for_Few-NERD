{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e97360",
   "metadata": {},
   "source": [
    "# before_remove_TreePosから最終整形を行い，データ拡張実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dev = []\n",
    "with open(\"data/TreePos_BI/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "for d in dev:\n",
    "    d[\"tree_pos\"] = [d[\"tags\"][i].split(\"_\")[0] for i in range(len(d[\"tags\"]))]\n",
    "    d[\"ner_pos\"] = [d[\"tags\"][i].split(\"_\")[1] for i in range(len(d[\"tags\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "dd24c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train = []\n",
    "with open(\"data/TreePos_BI/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "for d in train:\n",
    "    d[\"tree_pos\"] = [d[\"tags\"][i].split(\"_\")[0] for i in range(len(d[\"tags\"]))]\n",
    "    d[\"ner_pos\"] = [d[\"tags\"][i].split(\"_\")[1] for i in range(len(d[\"tags\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c5a8d28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# d= {'tokens': ['The',...,'.'],'tags': ['DT_O',...,'._O'],'tree_pos': ['DT',....'.'], 'ner_pos': ['O',...'O']}\n",
    "def make_data2list_Simple(data):\n",
    "    data_list = []\n",
    "    for d in data:            \n",
    "        new_sent = []\n",
    "        before_tag = \"\"\n",
    "        new_token = []\n",
    "        for i in range(len(d[\"tags\"])):\n",
    "\n",
    "            if d[\"ner_pos\"][i] == \"O\" or i == len(d[\"tokens\"])-1:#固有表現タグがO　または　最後のトークンの時\n",
    "                if d[\"ner_pos\"][i] == before_tag:#1\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"ner_pos\"][i]])\n",
    "                    new_token = []\n",
    "                else:\n",
    "                    new_token = []#2\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"ner_pos\"][i]])\n",
    "                    new_token = []\n",
    "                    \n",
    "            else:#固有表現タグがBI#3\n",
    "                if d[\"ner_pos\"][i] == before_tag:\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_token = []\n",
    "                else:#4\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"ner_pos\"][i]])\n",
    "            before_tag = d[\"ner_pos\"][i]\n",
    "        data_list.append(new_sent)\n",
    "    return data_list\n",
    "# dev_list= [['Danish', 'rigsdaler'], 'other-currency'],[['='], 'O'],...\n",
    "\n",
    "\n",
    "def make_data2list_TreePos(data):\n",
    "    data_list = []\n",
    "    for d in data:            \n",
    "        new_sent = []\n",
    "        before_tag = \"\"\n",
    "        new_token = []\n",
    "        new_tag = []\n",
    "        for i in range(len(d[\"tags\"])):\n",
    "\n",
    "            if d[\"ner_pos\"][i] == \"O\" or i == len(d[\"tokens\"])-1:#固有表現タグがO　または　最後のトークンの時\n",
    "                if d[\"ner_pos\"][i] == before_tag:#1\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_tag = list(set(new_tag))  #new_tagの重複削除\n",
    "                    new_tag.sort()                #new_tagの整列\n",
    "                    new_sent.append([new_token,\"__\".join(new_tag)])\n",
    "                    \n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                else:\n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                    \n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_tag = list(set(new_tag))  #new_tagの重複削除\n",
    "                    new_tag.sort()                #new_tagの整列\n",
    "                    new_sent.append([new_token,\"__\".join(new_tag)])\n",
    "                    \n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                    \n",
    "            else:#固有表現タグがBI\n",
    "                if d[\"ner_pos\"][i] == before_tag:\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                else:\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_tag = list(set(new_tag))  #new_tagの重複削除\n",
    "                    new_tag.sort()                #new_tagの整列\n",
    "                    new_sent.append([new_token,\"__\".join(new_tag)])#new_tagを__で結合\n",
    "                    \n",
    "            before_tag = d[\"ner_pos\"][i]\n",
    "        data_list.append(new_sent)\n",
    "    return data_list\n",
    "\n",
    "#交換先辞書の作成\n",
    "def make_alterna_labels(data_list):\n",
    "    label_list = set()\n",
    "    for s in data_list:\n",
    "        for t in s:\n",
    "            if t[1] != \"O\" and \"_O\" not in t[1][0]:\n",
    "                label_list.add(t[1])\n",
    "    \n",
    "    label_list = list(label_list)\n",
    "    alterna_labels = dict()\n",
    "    for label in label_list:\n",
    "        alterna_labels[label] = set()\n",
    "    for s in data_list:\n",
    "        for t in s:\n",
    "            token = \"_\".join(t[0])\n",
    "            tag = t[1]\n",
    "            if tag in label_list:\n",
    "                alterna_labels[tag].add(token)\n",
    "    \n",
    "    new_alterna_labels = dict()\n",
    "    for k,v in alterna_labels.items():#候補が空白or1つのtagは削除\n",
    "        if v==set() or len(v) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            new_alterna_labels[k] = v\n",
    "\n",
    "    return new_alterna_labels\n",
    "\n",
    "\n",
    "#Oタグのみデータを排除した増幅候補\n",
    "def make_data_has_BI(data_list):\n",
    "    data_has_BI = [] #16342(データ全体は18823)\n",
    "    for d in data_list:\n",
    "        tags_list = list(set([i[1] for i in d]))\n",
    "        if tags_list != [\"O\"]:\n",
    "            data_has_BI.append(d)\n",
    "    return data_has_BI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "241d451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]\n",
    "\n",
    "def Simple_TreePos_BI(datatype,data_dict,n):\n",
    "    #[[token_1,...token_n],tag],...[token_1,...token_n],tag]]状態のデータを作成\n",
    "    data_simple = make_data2list_Simple(data_dict)#tagが固有表現情報\n",
    "    data_treepos = make_data2list_TreePos(data_dict)#tagが品詞＋固有表現情報\n",
    "    \n",
    "    alterna_labels_simple = make_alterna_labels(data_simple)#固有表現情報のみに着目した交換先辞書\n",
    "    alterna_labels_treepos = make_alterna_labels(data_treepos)#品詞＋固有表現情報の交換先辞書\n",
    "    data_has_BI = make_data_has_BI(data_treepos)\n",
    "    #Oタグのみのデータを排除し，交換前候補として格納（treeposのほうが条件が厳しいのでそれを基準とする）\n",
    "    \n",
    "    #加工箇所を同じにするため，SimpleとTreePosを同時に作成する\n",
    "    data_simple_f_path = \"data/Simple_BI_x\"+str(n)+\"/\"+datatype+\".json\"\n",
    "    data_treepos_f_path = \"data/TreePos_BI_x\"+str(n)+\"/\"+datatype+\".json\"\n",
    "    memo_f_path = \"log/Simple_and_TreePos_BI_x\"+str(n)+\"_\"+datatype+\"_memo.txt\"\n",
    "\n",
    "    with open(data_simple_f_path,\"w\",encoding=\"utf-8\") as data_simple_f, \\\n",
    "    open(data_treepos_f_path,\"w\",encoding=\"utf-8\") as data_treepos_f, \\\n",
    "    open(memo_f_path,\"w\",encoding=\"utf-8\") as memo_f:\n",
    "        \n",
    "        for d in data_dict:\n",
    "            write_d = dict()\n",
    "            write_d[\"tokens\"] = d[\"tokens\"]\n",
    "            write_d[\"tags\"] = d[\"ner_pos\"]\n",
    "            data_simple_f.write(json.dumps(write_d))#Simpleにオリジナルデータを先に書込\n",
    "            data_simple_f.write(\"\\n\")\n",
    "            data_treepos_f.write(json.dumps(write_d))#TreePosにオリジナルデータを先に書込\n",
    "            data_treepos_f.write(\"\\n\")\n",
    "        \n",
    "        add_n = n*len(data_dict)-len(data_dict)#拡張データの数\n",
    "        add_n\n",
    "        cnt = 0\n",
    "        while cnt < add_n:\n",
    "            if cnt%10 == 0:\n",
    "                print('\\r%d / %d' %(cnt, add_n), end='')\n",
    "\n",
    "            x = random.randint(0,len(data_has_BI)-1)\n",
    "            x_s = data_has_BI[x]\n",
    "\n",
    "            new_d_simple = dict()\n",
    "            new_d_simple[\"tokens\"] = []\n",
    "            new_d_simple[\"tags\"] = []\n",
    "            \n",
    "            new_d_treepos = dict()\n",
    "            new_d_treepos[\"tokens\"] = []\n",
    "            new_d_treepos[\"tags\"] = []\n",
    "            old_tokens = []\n",
    "            \n",
    "            change = False\n",
    "            print(x_s)\n",
    "            for token,tag_treepos in x_s:\n",
    "                tag_simple = tag_treepos.split(\"_\")[-1]\n",
    "                old_tokens.extend(token)\n",
    "                #固有表現であり，厳格な候補であるtreeposに交換対象が存在するとき，50%の確率で入れ替えを発生\n",
    "                if (tag_simple != \"O\") and (tag_treepos in alterna_labels_treepos) and (random.random() >= 0.5):\n",
    "                    \n",
    "                    #固有表現情報に基づく交換候補を生成\n",
    "                    token_simple = random_choice_token(alterna_labels_simple[tag_simple]-{\"_\".join(token)})\n",
    "                    token_simple = token_simple.split(\"_\")\n",
    "                    #交換したtokenを記録\n",
    "                    new_d_simple[\"tokens\"].extend(token_simple)\n",
    "                    #新しいtokenの数だけtagを追加\n",
    "                    for _ in token_simple:\n",
    "                        new_d_simple[\"tags\"].append(tag_simple)\n",
    "                        \n",
    "                    #品詞＋固有表現情報に基づく交換候補を生成\n",
    "                    token_treepos = random_choice_token(alterna_labels_treepos[tag_treepos]-{\"_\".join(token)})\n",
    "                    token_treepos = token_treepos.split(\"_\")\n",
    "                    #交換したtokenを記録\n",
    "                    new_d_treepos[\"tokens\"].extend(token_treepos)\n",
    "                    #新しいtokenの数だけtagを追加\n",
    "                    for _ in token_treepos:#\n",
    "                        new_d_treepos[\"tags\"].append(tag_simple)\n",
    "                        \n",
    "                    change = True#書換えフラグ\n",
    "                    \n",
    "                else:#alterna_labels_treeposに採用されなかった固有表現タグ付きtoken向けに複数token向けtag追加法で実装\n",
    "                    new_d_simple[\"tokens\"].extend(token)\n",
    "                    for _ in token:\n",
    "                        new_d_simple[\"tags\"].append(tag_simple)\n",
    "\n",
    "                    new_d_treepos[\"tokens\"].extend(token)\n",
    "                    for _ in token:\n",
    "                        new_d_treepos[\"tags\"].append(tag_simple)\n",
    "\n",
    "            if change == True:#書換えが実行されたなら現在の一文をデータとして書出（改変前と同じ学習データの混入防止）\n",
    "                if (len(new_d_simple[\"tokens\"]) != len(new_d_simple[\"tags\"])) or (len(new_d_treepos[\"tokens\"]) != len(new_d_treepos[\"tags\"])):\n",
    "                    print(\"あかん失敗や\")\n",
    "                data_simple_f.write(json.dumps(new_d_simple))\n",
    "                data_simple_f.write(\"\\n\")\n",
    "                data_treepos_f.write(json.dumps(new_d_treepos))\n",
    "                data_treepos_f.write(\"\\n\")\n",
    "                \n",
    "                memo_f.write(str(x)+\"\\t\"+\" \".join(old_tokens)+\"\\n\")#memoに変更前と変更後を記入\n",
    "                memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d_simple[\"tokens\"])+\"\\n\")\n",
    "                memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d_treepos[\"tokens\"])+\"\\n\")\n",
    "                memo_f.write(\"\\n\")\n",
    "                cnt += 1\n",
    "            else:\n",
    "                pass\n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fe275c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18820 / 18823"
     ]
    }
   ],
   "source": [
    "Simple_TreePos_BI(\"dev\",dev,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "72e142ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131710 / 131719"
     ]
    }
   ],
   "source": [
    "Simple_TreePos_BI(\"train\",train,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143bf559",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef7a00",
   "metadata": {},
   "source": [
    "# DBWS2022ポスター作製"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7088c3",
   "metadata": {},
   "source": [
    "## confusion matrix生成コード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5116f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gensim\n",
    "# !pip3 install whatlies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e0ae139",
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple_predictions = []\n",
    "with open(\"result/epochs3/Simple_BI_x2/predictions.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        Simple_predictions.extend(line.replace(\"\\n\",\"\").split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62941fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "TreePos_predictions = []\n",
    "with open(\"result/epochs3/TreePos_BI_x2/predictions.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        TreePos_predictions.extend(line.replace(\"\\n\",\"\").split(\" \"))#appendにすると文ごとにリストに格納"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e77e3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "true_label = []\n",
    "with open(\"data/f/test.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        true_label.extend(json.loads(line)[\"tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86209973",
   "metadata": {},
   "source": [
    "Simple_predictions[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a5240d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "with open(\"data/labels.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        classes.append(line.replace(\"\\n\",\"\"))\n",
    "classes=sorted(set(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cf3eb89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efab8670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "103f26c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "val_mat = confusion_matrix(true_label, Simple_predictions, labels=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a473121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "val_mat2 = confusion_matrix(true_label, TreePos_predictions, labels=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9b547616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[712756,     67,     68, ...,    194,    127,    194],\n",
       "       [    87,   1115,     92, ...,      7,      0,      0],\n",
       "       [   114,     38,   1669, ...,      0,      0,      0],\n",
       "       ...,\n",
       "       [   214,      0,      0, ...,   1072,      0,      1],\n",
       "       [    78,      0,      0, ...,      1,    595,      0],\n",
       "       [   238,      0,      6, ...,      0,      7,    911]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74f92498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.savetxt(\"result/epochs3/Simple_BI_x2/confusion_matrix.csv\", val_mat, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47b87215",
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.savetxt(\"result/epochs3/TreePos_BI_x2/confusion_matrix_2.csv\", val_mat2, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b350191c",
   "metadata": {},
   "source": [
    "## 埋込可視化試み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "109c6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_open = open(\"result/epochs3/Simple_BI_x2/all_results.json\",\"r\")\n",
    "all_result = json.load(json_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "43464e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_result_d= {}\n",
    "for k,v in all_result.items():\n",
    "    k_list = k.split(\"_\")\n",
    "    tag = k_list[-2]\n",
    "    parm = k_list[-1]\n",
    "\n",
    "    if tag not in all_result_d.keys():\n",
    "        all_result_d[tag] = {}\n",
    "    if parm in [\"f1\",\"number\",\"precision\",\"recall\"]:\n",
    "        all_result_d[tag][parm] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "03618828",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "data = []\n",
    "for k,v in all_result_d.items():\n",
    "    label.append(k)\n",
    "    data.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c3314ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"result/epochs3/Simple_BI_x2/confusion_matrix_3.csv\", 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile,fieldnames=[\"f1\",\"number\",\"recall\",\"precision\"],lineterminator='\\n')\n",
    "    writer.writeheader()\n",
    "    for d in data:\n",
    "        writer.writerow(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "aa30cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = dev+train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "337b4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_simple = make_data2list_Simple(data_dict)#tagが固有表現情報\n",
    "data_treepos = make_data2list_TreePos(data_dict)#tagが品詞＋固有表現情報\n",
    "\n",
    "alterna_labels_simple = make_alterna_labels(data_simple)#固有表現情報のみに着目した交換先辞書\n",
    "# alterna_labels_treepos = make_alterna_labels(data_treepos)#品詞＋固有表現情報の交換先辞書\n",
    "# data_has_BI = make_data_has_BI(data_treepos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "96ce6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/alterna_labels_simple.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "    for k,v in alterna_labels_simple.items():\n",
    "        f.write(\"【 \"+k+\" 】\\n\")\n",
    "        f.write(\"   \".join(list(v))+\"\\n\")#memoに変更前と変更後を記入\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "98b4d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_d = dict()\n",
    "for k,v in alterna_labels_simple.items():\n",
    "    words = []\n",
    "    for token in v:\n",
    "        words.append(token.replace(\"_\",\" \"))\n",
    "    new_d[k] = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2cf887e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/alterna_labels_simple_for_dict.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(new_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d0ad02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from whatlies import EmbeddingSet\n",
    "from whatlies.language import HFTransformersLanguage\n",
    "from whatlies.transformers import Pca\n",
    "\n",
    "lang = HFTransformersLanguage('bert-base-uncased')\n",
    "\n",
    "emb = EmbeddingSet(*[lang[w] for w in words])\n",
    "emb.transform(Pca(2))\n",
    "emb.plot_interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d997e47",
   "metadata": {},
   "source": [
    "# [10/8]結果のラベル名を変更（癒着問題解決）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e0051efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "import json\n",
    "references = []\n",
    "with open(\"data/f/test.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        references.append(json.loads(line)[\"tags\"])\n",
    "\n",
    "predictions = []\n",
    "with open(\"result/epochs3/f/predictions.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        predictions.append(line.replace(\"\\n\",\"\").split(\" \"))\n",
    "\n",
    "label_set = set()\n",
    "for l in references+predictions:\n",
    "    for label in list(set(l)):\n",
    "        label_set.add(label)\n",
    "\n",
    "label_set.remove(\"O\")\n",
    "                      \n",
    "label_d = dict()\n",
    "for k in label_set:\n",
    "    label_d[k] = \"L\"+k.replace(\"/\",\"_\").replace(\"-\",\"_\")\n",
    "    \n",
    "for l in references:\n",
    "    for i in range(len(l)):\n",
    "        if l[i] in label_d:\n",
    "            l[i] = label_d[l[i]]\n",
    "\n",
    "for l in predictions:\n",
    "    for i in range(len(l)):\n",
    "        if l[i] in label_d:\n",
    "            l[i] = label_d[l[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "52b1a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "seqeval = evaluate.load('seqeval')\n",
    "results = seqeval.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c1e143e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_overall = {}\n",
    "results_overall['overall_precision']=results['overall_precision']\n",
    "results_overall['overall_recall']=results['overall_recall']\n",
    "results_overall['overall_f1']=results['overall_f1']\n",
    "results_overall['overall_accuracy']=results['overall_accuracy']\n",
    "\n",
    "del results['overall_precision']\n",
    "del results['overall_recall']\n",
    "del results['overall_f1']\n",
    "del results['overall_accuracy']\n",
    "\n",
    "result_csv = []\n",
    "for k in results.keys():\n",
    "    results[k][\"label\"] = k\n",
    "    result_csv.append(results[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "cbe0e39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall_precision': 0.6565213121194264,\n",
       " 'overall_recall': 0.6898412829456564,\n",
       " 'overall_f1': 0.6727689938708346,\n",
       " 'overall_accuracy': 0.9277197645306355}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2be772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"result/epoch3_f1_results.csv\", 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"label\",\"precision\",\"recall\",\"f1\",\"number\"],lineterminator='\\n')\n",
    "    writer.writeheader()\n",
    "    writer.writerows([results[data] for data in results])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dba953",
   "metadata": {},
   "source": [
    "## confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "02bd52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "import json\n",
    "references = []\n",
    "with open(\"data/f/test.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        references.extend(json.loads(line)[\"tags\"])\n",
    "\n",
    "predictions = []\n",
    "# with open(\"result/epochs3/f/predictions.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "with open(\"result/epochs3/TreePos_BI_x2/predictions.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        predictions.extend(line.replace(\"\\n\",\"\").split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "219aa280",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = []\n",
    "with open(\"data/labels.txt\",\"r\") as f:\n",
    "    for line in f:\n",
    "        classes.append(line.replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c9209852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "val_mat = confusion_matrix(references, predictions, labels=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "11769b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[712765,     74,     75, ...,    211,    112,    177],\n",
       "       [   102,   1109,     94, ...,      6,      0,      0],\n",
       "       [   101,     61,   1656, ...,      0,      0,      0],\n",
       "       ...,\n",
       "       [   218,      0,      0, ...,   1081,      0,      1],\n",
       "       [    87,      0,      0, ...,      0,    594,      0],\n",
       "       [   228,      0,      4, ...,      0,      7,    935]], dtype=int64)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b0be0b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "# numpy.savetxt(\"result/epochs3_f_confusion_matrix.csv\", val_mat, delimiter=\",\")\n",
    "numpy.savetxt(\"result/epochs3_TreePos_BI_x2_confusion_matrix.csv\", val_mat, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7425cae6",
   "metadata": {},
   "source": [
    "## 埋め込みの平均\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cdeb7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dev = []\n",
    "with open(\"data/f/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "train = []\n",
    "with open(\"data/f/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "31297c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d= {'tokens': ['The',...,'.'],'tags':['O',...'O']}\n",
    "def make_origin_data2list_Simple(data):\n",
    "    data_list = []\n",
    "    for d in data:            \n",
    "        new_sent = []\n",
    "        before_tag = \"\"\n",
    "        new_token = []\n",
    "        for i in range(len(d[\"tags\"])):\n",
    "\n",
    "            if d[\"tags\"][i] == \"O\" or i == len(d[\"tokens\"])-1:#固有表現タグがO　または　最後のトークンの時\n",
    "                if d[\"tags\"][i] == before_tag:#1\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"tags\"][i]])\n",
    "                    new_token = []\n",
    "                else:\n",
    "                    new_token = []#2\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"tags\"][i]])\n",
    "                    new_token = []\n",
    "                    \n",
    "            else:#固有表現タグがBI#3\n",
    "                if d[\"tags\"][i] == before_tag:\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_token = []\n",
    "                else:#4\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"tags\"][i]])\n",
    "            before_tag = d[\"tags\"][i]\n",
    "        data_list.append(new_sent)\n",
    "    return data_list\n",
    "# dev_list= [['Danish', 'rigsdaler'], 'other-currency'],[['='], 'O'],...\n",
    "\n",
    "\n",
    "#交換先辞書の作成\n",
    "def make_alterna_labels(data_list):\n",
    "    label_list = set()\n",
    "    for s in data_list:\n",
    "        for t in s:\n",
    "            if t[1] != \"O\" and \"_O\" not in t[1][0]:\n",
    "                label_list.add(t[1])\n",
    "    \n",
    "    label_list = list(label_list)\n",
    "    alterna_labels = dict()\n",
    "    for label in label_list:\n",
    "        alterna_labels[label] = set()\n",
    "    for s in data_list:\n",
    "        for t in s:\n",
    "            token = \" \".join(t[0])\n",
    "            tag = t[1]\n",
    "            if tag in label_list:\n",
    "                alterna_labels[tag].add(token)\n",
    "    \n",
    "    new_alterna_labels = dict()\n",
    "    for k,v in alterna_labels.items():#候補が空白or1つのtagは削除\n",
    "        if v==set() or len(v) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            new_alterna_labels[k] = v\n",
    "\n",
    "    return new_alterna_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "09532c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_simple = make_origin_data2list_Simple(dev+train)#tagが固有表現情報\n",
    "alterna_labels_simple = make_alterna_labels(data_simple)#固有表現情報のみに着目した交換先辞書\n",
    "# {'building-theater': {'Hull Truck',\n",
    "#   'd West',\n",
    "#   'Théâtre Municipal'..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6ae784a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = list(alterna_labels_simple['building-theater'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "43651674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hull Truck'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "45f84e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27c3667b82f4757a9f2f11174bc737e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02b556f2656487e83cccabef3c0a5fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2df6564dd042db9af71d999ad77dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c9b9cb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['building-theater', 'building-sportsfacility', 'art-broadcastprogram', 'product-airplane', 'organization-company', 'person-artist/author', 'organization-showorganization', 'building-hotel', 'event-disaster', 'person-soldier', 'product-other', 'location-other', 'other-currency', 'person-other', 'other-god', 'product-train', 'building-hospital', 'location-mountain', 'organization-other', 'building-restaurant', 'location-park', 'art-music', 'other-award', 'other-chemicalthing', 'person-politician', 'organization-media/newspaper', 'other-astronomything', 'product-food', 'product-weapon', 'organization-education', 'organization-politicalparty', 'person-actor', 'person-athlete', 'product-game', 'location-GPE', 'building-other', 'organization-sportsleague', 'other-biologything', 'location-bodiesofwater', 'other-disease', 'product-ship', 'art-painting', 'event-election', 'organization-government/governmentagency', 'organization-religion', 'other-educationaldegree', 'other-law', 'other-language', 'event-sportsevent', 'other-medical', 'event-other', 'event-protest', 'person-scholar', 'product-software', 'person-director', 'location-road/railway/highway/transit', 'event-attack/battle/war/militaryconflict', 'building-library', 'art-other', 'location-island', 'other-livingthing', 'building-airport', 'art-film', 'organization-sportsteam', 'product-car', 'art-writtenart'])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alterna_labels_simple.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f249822",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = []\n",
    "for w in list(alterna_labels_simple[\"building-theater\"]):\n",
    "    print(w)\n",
    "    input_ids = torch.tensor(tokenizer.encode(w, add_special_tokens=True)).unsqueeze(0) \n",
    "    outputs = model(input_ids)\n",
    "    new.append(outputs[0].to('cpu').detach().numpy().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "be652383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "x = []\n",
    "for i in range(10):\n",
    "    x.append(random.randint(10,99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e7f62982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "84\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(x)):\n",
    "    if x[i]%3==0:\n",
    "        print(x[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6309d8c2",
   "metadata": {},
   "source": [
    "## 埋め込み概念のサイズの話はとん挫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e210ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alterna_labels_simple[\"location-GPE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a7488663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = new[0]\n",
    "# for w,name in zip(new[1:],list(alterna_labels_simple[\"building-theater\"])[1:]):\n",
    "#     s = s + w\n",
    "#     print(name,\": \",w.shape)\n",
    "# s = s / len(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "da859570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alterna_labels_simple_vec = {}\n",
    "\n",
    "# for k in alterna_labels_simple.keys():\n",
    "#     alterna_labels_simple_vec[k] = []\n",
    "    \n",
    "#     for w in list(alterna_labels_simple[k]):\n",
    "#         input_ids = torch.tensor(tokenizer.encode(w, add_special_tokens=True)).unsqueeze(0) \n",
    "#         outputs = model(input_ids)\n",
    "#         alterna_labels_simple_vec[k].append(outputs[0].to('cpu').detach().numpy().copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e363aef",
   "metadata": {},
   "source": [
    "# Oタグのみの判定結果を返した場合の試算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0026f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "import json\n",
    "references = []\n",
    "with open(\"data/f/test.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        references.append(json.loads(line)[\"tags\"])\n",
    "\n",
    "predictions_O = []\n",
    "with open(\"result/epochs3/f/predictions.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        predictions_O.append(line.replace(\"\\n\",\"\").split(\" \"))\n",
    "\n",
    "label_set = set()\n",
    "for l in references+predictions:\n",
    "    for label in list(set(l)):\n",
    "        label_set.add(label)\n",
    "\n",
    "label_set.remove(\"O\")\n",
    "                      \n",
    "label_d = dict()\n",
    "for k in label_set:\n",
    "    label_d[k] = \"L\"+k.replace(\"/\",\"_\").replace(\"-\",\"_\")\n",
    "    \n",
    "for l in references:\n",
    "    for i in range(len(l)):\n",
    "        if l[i] in label_d:\n",
    "            l[i] = label_d[l[i]]\n",
    "\n",
    "for l in predictions_O:\n",
    "    for i in range(len(l)):\n",
    "        if l[i] in label_d:\n",
    "            l[i] = [\"O\" for i in label_d[l[i]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5faaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "seqeval = evaluate.load('seqeval')\n",
    "results = seqeval.compute(predictions=predictions_O, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0b016d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_overall = {}\n",
    "results_overall['overall_precision']=results['overall_precision']\n",
    "results_overall['overall_recall']=results['overall_recall']\n",
    "results_overall['overall_f1']=results['overall_f1']\n",
    "results_overall['overall_accuracy']=results['overall_accuracy']\n",
    "\n",
    "del results['overall_precision']\n",
    "del results['overall_recall']\n",
    "del results['overall_f1']\n",
    "del results['overall_accuracy']\n",
    "\n",
    "result_csv = []\n",
    "for k in results.keys():\n",
    "    results[k][\"label\"] = k\n",
    "    result_csv.append(results[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0f48cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"result/epoch3_f1_results_all_O.csv\", 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"label\",\"precision\",\"recall\",\"f1\",\"number\"],lineterminator='\\n')\n",
    "    writer.writeheader()\n",
    "    writer.writerows([results[data] for data in results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "af2f4ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"result/epoch3_f1_results_results_overall_all_O.csv\", 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=[\"overall_precision\",\"overall_recall\",\"overall_f1\",\"overall_accuracy\"],lineterminator='\\n')\n",
    "    writer.writeheader()\n",
    "    writer.writerow(results_overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3f583762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'overall_precision': 0.0,\n",
       " 'overall_recall': 0.0,\n",
       " 'overall_f1': 0.0,\n",
       " 'overall_accuracy': 0.7742944496196749}"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_overall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b5833",
   "metadata": {},
   "source": [
    "# 異なる結果を目視で確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7b9f726d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "import json\n",
    "\n",
    "references = []\n",
    "tokens = []\n",
    "with open(\"data/f/test.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        references.append(json.loads(line)[\"tags\"])\n",
    "        tokens.append(json.loads(line)[\"tokens\"])\n",
    "        \n",
    "predictions = []\n",
    "with open(\"result/epochs3/f/predictions.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        predictions.append(line.replace(\"\\n\",\"\").split(\" \"))\n",
    "\n",
    "label_set = set()\n",
    "for l in references+predictions:\n",
    "    for label in list(set(l)):\n",
    "        label_set.add(label)\n",
    "\n",
    "label_set.remove(\"O\")\n",
    "                      \n",
    "label_d = dict()\n",
    "for k in label_set:\n",
    "    label_d[k] = k.replace(\"/\",\"_\").replace(\"-\",\"_\")\n",
    "    \n",
    "for l in references:\n",
    "    for i in range(len(l)):\n",
    "        if l[i] in label_d:\n",
    "            l[i] = label_d[l[i]]\n",
    "\n",
    "for l in predictions:\n",
    "    for i in range(len(l)):\n",
    "        if l[i] in label_d:\n",
    "            l[i] = label_d[l[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b4eb1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_result_dict = []\n",
    "for i in range(len(references)):\n",
    "    d = {}\n",
    "    if references[i] != predictions[i]:\n",
    "        d[\"tokens\"] = tokens[i]\n",
    "        d[\"references\"] = references[i]\n",
    "        d[\"predictions\"] = predictions[i]\n",
    "        miss_result_dict.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "81d4d880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20499"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(miss_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "05df3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result/result_ans_vs_f-model.csv','w',encoding='utf-8') as file:\n",
    "    for d in miss_result_dict:\n",
    "        writer = csv.writer(file,delimiter = \",\",lineterminator='\\n')\n",
    "        writer.writerows([d[\"tokens\"],d[\"references\"],d[\"predictions\"]])\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912bf04",
   "metadata": {},
   "source": [
    "# Oタグのみでの交換を実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67247c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dev = []\n",
    "with open(\"data/TreePos_BI/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "for d in dev:\n",
    "    d[\"tree_pos\"] = [d[\"tags\"][i].split(\"_\")[0] for i in range(len(d[\"tags\"]))]\n",
    "    d[\"ner_pos\"] = [d[\"tags\"][i].split(\"_\")[1] for i in range(len(d[\"tags\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c28aa4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train = []\n",
    "with open(\"data/TreePos_BI/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "for d in train:\n",
    "    d[\"tree_pos\"] = [d[\"tags\"][i].split(\"_\")[0] for i in range(len(d[\"tags\"]))]\n",
    "    d[\"ner_pos\"] = [d[\"tags\"][i].split(\"_\")[1] for i in range(len(d[\"tags\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df48be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d= {'tokens': ['The',...,'.'],'tags': ['DT_O',...,'._O'],'tree_pos': ['DT',....'.'], 'ner_pos': ['O',...'O']}\n",
    "def make_data2list_Simple(data):\n",
    "    data_list = []\n",
    "    for d in data:            \n",
    "        new_sent = []\n",
    "        before_tag = \"\"\n",
    "        new_token = []\n",
    "        for i in range(len(d[\"tags\"])):\n",
    "\n",
    "            if d[\"ner_pos\"][i] == \"O\" or i == len(d[\"tokens\"])-1:#固有表現タグがO　または　最後のトークンの時\n",
    "                if d[\"ner_pos\"][i] == before_tag:#1\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"ner_pos\"][i]])\n",
    "                    new_token = []\n",
    "                else:\n",
    "                    new_token = []#2\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"ner_pos\"][i]])\n",
    "                    new_token = []\n",
    "                    \n",
    "            else:#固有表現タグがBI#3\n",
    "                if d[\"ner_pos\"][i] == before_tag:\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_token = []\n",
    "                else:#4\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_sent.append([new_token,d[\"ner_pos\"][i]])\n",
    "            before_tag = d[\"ner_pos\"][i]\n",
    "        data_list.append(new_sent)\n",
    "    return data_list\n",
    "# dev_list= [['Danish', 'rigsdaler'], 'other-currency'],[['='], 'O'],...\n",
    "\n",
    "\n",
    "def make_data2list_TreePos(data):\n",
    "    data_list = []\n",
    "    for d in data:            \n",
    "        new_sent = []\n",
    "        before_tag = \"\"\n",
    "        new_token = []\n",
    "        new_tag = []\n",
    "        for i in range(len(d[\"tags\"])):\n",
    "\n",
    "            if d[\"ner_pos\"][i] == \"O\" or i == len(d[\"tokens\"])-1:#固有表現タグがO　または　最後のトークンの時\n",
    "                if d[\"ner_pos\"][i] == before_tag:#1\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_tag = list(set(new_tag))  #new_tagの重複削除\n",
    "                    new_tag.sort()                #new_tagの整列\n",
    "                    new_sent.append([new_token,\"__\".join(new_tag)])\n",
    "                    \n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                else:\n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                    \n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_tag = list(set(new_tag))  #new_tagの重複削除\n",
    "                    new_tag.sort()                #new_tagの整列\n",
    "                    new_sent.append([new_token,\"__\".join(new_tag)])\n",
    "                    \n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                    \n",
    "            else:#固有表現タグがBI\n",
    "                if d[\"ner_pos\"][i] == before_tag:\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_token = []\n",
    "                    new_tag = []\n",
    "                else:\n",
    "                    new_token.append(d[\"tokens\"][i])\n",
    "                    new_tag.append(d[\"tags\"][i])\n",
    "                    \n",
    "                    new_tag = list(set(new_tag))  #new_tagの重複削除\n",
    "                    new_tag.sort()                #new_tagの整列\n",
    "                    new_sent.append([new_token,\"__\".join(new_tag)])#new_tagを__で結合\n",
    "                    \n",
    "            before_tag = d[\"ner_pos\"][i]\n",
    "        data_list.append(new_sent)\n",
    "    return data_list\n",
    "\n",
    "#交換先辞書の作成　元の定義名から改変＆名前の変更\n",
    "def make_alterna_labels_for_O(data_list):\n",
    "    label_list = set()\n",
    "    for s in data_list:\n",
    "        for t in s:\n",
    "            if t[1] == \"O\" or \"_O\" in t[1]:\n",
    "                label_list.add(t[1])\n",
    "    \n",
    "    label_list = list(label_list)\n",
    "    alterna_labels = dict()\n",
    "    for label in label_list:\n",
    "        alterna_labels[label] = set()\n",
    "    for s in data_list:\n",
    "        for t in s:\n",
    "            token = \"_\".join(t[0])\n",
    "            tag = t[1]\n",
    "            if tag in label_list:\n",
    "                alterna_labels[tag].add(token)\n",
    "    \n",
    "    new_alterna_labels = dict()\n",
    "    for k,v in alterna_labels.items():#候補が空白or1つのtagは削除\n",
    "        if v==set() or len(v) == 1:\n",
    "            pass\n",
    "        else:\n",
    "            new_alterna_labels[k] = v\n",
    "\n",
    "    return new_alterna_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5240d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]\n",
    "\n",
    "def Simple_TreePos_O(datatype,data_dict,n):#元の定義名から改変＆名前の変更\n",
    "    \n",
    "    #[[token_1,...token_n],tag],...[token_1,...token_n],tag]]状態のデータを作成\n",
    "    data_simple = make_data2list_Simple(data_dict)#tagが固有表現情報\n",
    "    data_treepos = make_data2list_TreePos(data_dict)#tagが品詞＋固有表現情報\n",
    "    \n",
    "    alterna_labels_simple = make_alterna_labels_for_O(data_simple)#固有表現情報のみに着目した交換先辞書\n",
    "    alterna_labels_treepos = make_alterna_labels_for_O(data_treepos)#品詞＋固有表現情報の交換先辞書\n",
    "    all_len = len(data_dict)\n",
    "    #今回は全てOタグを持っている為，make_data_has_BIのOタグ版は使用しない\n",
    "    \n",
    "    #加工箇所を同じにするため，SimpleとTreePosを同時に作成する\n",
    "    data_simple_f_path = \"data/Simple_O_x\"+str(n)+\"/\"+datatype+\".json\"\n",
    "    data_treepos_f_path = \"data/TreePos_O_x\"+str(n)+\"/\"+datatype+\".json\"\n",
    "    memo_f_path = \"log/Simple_and_TreePos_O_x\"+str(n)+\"_\"+datatype+\"_memo.txt\"\n",
    "\n",
    "    with open(data_simple_f_path,\"w\",encoding=\"utf-8\") as data_simple_f, \\\n",
    "    open(data_treepos_f_path,\"w\",encoding=\"utf-8\") as data_treepos_f, \\\n",
    "    open(memo_f_path,\"w\",encoding=\"utf-8\") as memo_f:\n",
    "        \n",
    "        for d in data_dict:\n",
    "            write_d = dict()\n",
    "            write_d[\"tokens\"] = d[\"tokens\"]\n",
    "            write_d[\"tags\"] = d[\"ner_pos\"]\n",
    "            data_simple_f.write(json.dumps(write_d))#Simpleにオリジナルデータを先に書込\n",
    "            data_simple_f.write(\"\\n\")\n",
    "            data_treepos_f.write(json.dumps(write_d))#TreePosにオリジナルデータを先に書込\n",
    "            data_treepos_f.write(\"\\n\")\n",
    "        \n",
    "        add_n = (n-1)*all_len#拡張データの数\n",
    "        add_n\n",
    "        cnt = 0\n",
    "        while cnt < add_n:\n",
    "            if cnt%10 == 0:\n",
    "                print('\\r%d / %d' %(cnt, add_n), end='')\n",
    "\n",
    "            x = random.randint(0,all_len-1)\n",
    "            x_s = data_treepos[x]\n",
    "\n",
    "            new_d_simple = dict()\n",
    "            new_d_simple[\"tokens\"] = []\n",
    "            new_d_simple[\"tags\"] = []\n",
    "            \n",
    "            new_d_treepos = dict()\n",
    "            new_d_treepos[\"tokens\"] = []\n",
    "            new_d_treepos[\"tags\"] = []\n",
    "            old_tokens = []\n",
    "            \n",
    "            change = False\n",
    "            for token,tag_treepos in x_s:\n",
    "                tag_simple = tag_treepos.split(\"_\")[-1]\n",
    "                old_tokens.extend(token)\n",
    "                #固有表現であり，厳格な候補であるtreeposに交換対象が存在するとき，50%の確率で入れ替えを発生\n",
    "                if (tag_simple == \"O\") and (tag_treepos in alterna_labels_treepos) and (random.random() >= 0.5):\n",
    "                    \n",
    "                    #固有表現情報に基づく交換候補を生成\n",
    "                    token_simple = random_choice_token(alterna_labels_simple[tag_simple]-{\"_\".join(token)})\n",
    "                    token_simple = token_simple.split(\"_\")\n",
    "                    #交換したtokenを記録\n",
    "                    new_d_simple[\"tokens\"].extend(token_simple)\n",
    "                    #新しいtokenの数だけtagを追加\n",
    "                    for _ in token_simple:\n",
    "                        new_d_simple[\"tags\"].append(tag_simple)\n",
    "                        \n",
    "                    #品詞＋固有表現情報に基づく交換候補を生成\n",
    "                    token_treepos = random_choice_token(alterna_labels_treepos[tag_treepos]-{\"_\".join(token)})\n",
    "                    token_treepos = token_treepos.split(\"_\")\n",
    "                    #交換したtokenを記録\n",
    "                    new_d_treepos[\"tokens\"].extend(token_treepos)\n",
    "                    #新しいtokenの数だけtagを追加\n",
    "                    for _ in token_treepos:#\n",
    "                        new_d_treepos[\"tags\"].append(tag_simple)\n",
    "                        \n",
    "                    change = True#書換えフラグ\n",
    "                    \n",
    "                else:#alterna_labels_treeposに採用されなかった固有表現タグ付きtoken向けに複数token向けtag追加法で実装\n",
    "                    new_d_simple[\"tokens\"].extend(token)\n",
    "                    for _ in token:\n",
    "                        new_d_simple[\"tags\"].append(tag_simple)\n",
    "\n",
    "                    new_d_treepos[\"tokens\"].extend(token)\n",
    "                    for _ in token:\n",
    "                        new_d_treepos[\"tags\"].append(tag_simple)\n",
    "\n",
    "            if change == True:#書換えが実行されたなら現在の一文をデータとして書出（改変前と同じ学習データの混入防止）\n",
    "                if (len(new_d_simple[\"tokens\"]) != len(new_d_simple[\"tags\"])) or (len(new_d_treepos[\"tokens\"]) != len(new_d_treepos[\"tags\"])):\n",
    "                    print(\"あかん失敗や\")\n",
    "                data_simple_f.write(json.dumps(new_d_simple))\n",
    "                data_simple_f.write(\"\\n\")\n",
    "                data_treepos_f.write(json.dumps(new_d_treepos))\n",
    "                data_treepos_f.write(\"\\n\")\n",
    "                \n",
    "                memo_f.write(str(x)+\"\\t\"+\" \".join(old_tokens)+\"\\n\")#memoに変更前と変更後を記入\n",
    "                memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d_simple[\"tokens\"])+\"\\n\")\n",
    "                memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d_treepos[\"tokens\"])+\"\\n\")\n",
    "                memo_f.write(\"\\n\")\n",
    "                cnt += 1\n",
    "            else:\n",
    "                pass\n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "112e129d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18820 / 18823"
     ]
    }
   ],
   "source": [
    "Simple_TreePos_O(\"dev\",dev,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "11492acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131710 / 131719"
     ]
    }
   ],
   "source": [
    "Simple_TreePos_O(\"train\",train,2,\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f109c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
