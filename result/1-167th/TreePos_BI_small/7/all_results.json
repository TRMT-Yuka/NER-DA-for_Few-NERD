{
    "predict_GPE_f1": 0.7383800608580522,
    "predict_GPE_number": 20409,
    "predict_GPE_precision": 0.7019697906545358,
    "predict_GPE_recall": 0.7787740702631192,
    "predict_actor_f1": 0.6293577981651377,
    "predict_actor_number": 1638,
    "predict_actor_precision": 0.6305147058823529,
    "predict_actor_recall": 0.6282051282051282,
    "predict_airplane_f1": 0.4802784222737819,
    "predict_airplane_number": 794,
    "predict_airplane_precision": 0.44516129032258067,
    "predict_airplane_recall": 0.5214105793450882,
    "predict_airport_f1": 0.6282208588957057,
    "predict_airport_number": 364,
    "predict_airport_precision": 0.5676274944567627,
    "predict_airport_recall": 0.7032967032967034,
    "predict_artist/author_f1": 0.4957627118644068,
    "predict_artist/author_number": 3464,
    "predict_artist/author_precision": 0.4853429203539823,
    "predict_artist/author_recall": 0.5066397228637414,
    "predict_astronomything_f1": 0.6016597510373444,
    "predict_astronomything_number": 678,
    "predict_astronomything_precision": 0.56640625,
    "predict_astronomything_recall": 0.6415929203539823,
    "predict_athlete_f1": 0.7668902339276951,
    "predict_athlete_number": 2907,
    "predict_athlete_precision": 0.7311291328758578,
    "predict_athlete_recall": 0.8063295493636051,
    "predict_attack/battle/war/militaryconflict_f1": 0.5938713854121709,
    "predict_attack/battle/war/militaryconflict_number": 1099,
    "predict_attack/battle/war/militaryconflict_precision": 0.5648604269293924,
    "predict_attack/battle/war/militaryconflict_recall": 0.6260236578707916,
    "predict_award_f1": 0.4006968641114983,
    "predict_award_number": 941,
    "predict_award_precision": 0.33948339483394835,
    "predict_award_recall": 0.48884165781083955,
    "predict_biologything_f1": 0.5116640746500778,
    "predict_biologything_number": 1875,
    "predict_biologything_precision": 0.4977307110438729,
    "predict_biologything_recall": 0.5264,
    "predict_bodiesofwater_f1": 0.5890968266883646,
    "predict_bodiesofwater_number": 1169,
    "predict_bodiesofwater_precision": 0.5616757176105508,
    "predict_bodiesofwater_recall": 0.6193327630453379,
    "predict_broadcastprogram_f1": 0.35668789808917195,
    "predict_broadcastprogram_number": 605,
    "predict_broadcastprogram_precision": 0.34408602150537637,
    "predict_broadcastprogram_recall": 0.3702479338842975,
    "predict_car_f1": 0.5653950953678474,
    "predict_car_number": 688,
    "predict_car_precision": 0.532051282051282,
    "predict_car_recall": 0.6031976744186046,
    "predict_chemicalthing_f1": 0.2932061978545888,
    "predict_chemicalthing_number": 1014,
    "predict_chemicalthing_precision": 0.3704819277108434,
    "predict_chemicalthing_recall": 0.24260355029585798,
    "predict_company_f1": 0.5464926590538336,
    "predict_company_number": 3903,
    "predict_company_precision": 0.5011754648429152,
    "predict_company_recall": 0.600819882141942,
    "predict_currency_f1": 0.6718489727928928,
    "predict_currency_number": 799,
    "predict_currency_precision": 0.6037924151696606,
    "predict_currency_recall": 0.7571964956195244,
    "predict_director_f1": 0.42428785607196406,
    "predict_director_number": 554,
    "predict_director_precision": 0.3628205128205128,
    "predict_director_recall": 0.5108303249097473,
    "predict_disaster_f1": 0.29279279279279274,
    "predict_disaster_number": 207,
    "predict_disaster_precision": 0.2742616033755274,
    "predict_disaster_recall": 0.3140096618357488,
    "predict_disease_f1": 0.4582080577269994,
    "predict_disease_number": 750,
    "predict_disease_precision": 0.4173055859802848,
    "predict_disease_recall": 0.508,
    "predict_education_f1": 0.6230158730158729,
    "predict_education_number": 2082,
    "predict_education_precision": 0.5757946210268948,
    "predict_education_recall": 0.6786743515850144,
    "predict_educationaldegree_f1": 0.39493136219640973,
    "predict_educationaldegree_number": 367,
    "predict_educationaldegree_precision": 0.32241379310344825,
    "predict_educationaldegree_recall": 0.5095367847411444,
    "predict_election_f1": 0.20045558086560367,
    "predict_election_number": 184,
    "predict_election_precision": 0.17254901960784313,
    "predict_election_recall": 0.2391304347826087,
    "predict_film_f1": 0.40153452685422,
    "predict_film_number": 759,
    "predict_film_precision": 0.39006211180124223,
    "predict_film_recall": 0.4137022397891963,
    "predict_food_f1": 0.4080560420315237,
    "predict_food_number": 432,
    "predict_food_precision": 0.32816901408450705,
    "predict_food_recall": 0.5393518518518519,
    "predict_game_f1": 0.2710997442455243,
    "predict_game_number": 496,
    "predict_game_precision": 0.3706293706293706,
    "predict_game_recall": 0.21370967741935484,
    "predict_god_f1": 0.5181278839815424,
    "predict_god_number": 635,
    "predict_god_precision": 0.445578231292517,
    "predict_god_recall": 0.6188976377952756,
    "predict_government/governmentagency_f1": 0.29861699125035285,
    "predict_government/governmentagency_number": 1535,
    "predict_government/governmentagency_precision": 0.2634462151394422,
    "predict_government/governmentagency_recall": 0.3446254071661238,
    "predict_hospital_f1": 0.5259259259259259,
    "predict_hospital_number": 364,
    "predict_hospital_precision": 0.47757847533632286,
    "predict_hospital_recall": 0.5851648351648352,
    "predict_hotel_f1": 0.5282331511839709,
    "predict_hotel_number": 265,
    "predict_hotel_precision": 0.5105633802816901,
    "predict_hotel_recall": 0.5471698113207547,
    "predict_island_f1": 0.4528061224489796,
    "predict_island_number": 646,
    "predict_island_precision": 0.38503253796095444,
    "predict_island_recall": 0.5495356037151703,
    "predict_language_f1": 0.6218097447795823,
    "predict_language_number": 753,
    "predict_language_precision": 0.5520082389289392,
    "predict_language_recall": 0.7118193891102258,
    "predict_law_f1": 0.38524590163934425,
    "predict_law_number": 488,
    "predict_law_precision": 0.32103825136612024,
    "predict_law_recall": 0.48155737704918034,
    "predict_library_f1": 0.5379146919431279,
    "predict_library_number": 357,
    "predict_library_precision": 0.46611909650924027,
    "predict_library_recall": 0.6358543417366946,
    "predict_livingthing_f1": 0.5215723873441994,
    "predict_livingthing_number": 865,
    "predict_livingthing_precision": 0.44553644553644556,
    "predict_livingthing_recall": 0.6289017341040463,
    "predict_loss": 0.6603252291679382,
    "predict_media/newspaper_f1": 0.5171288743882545,
    "predict_media/newspaper_number": 1234,
    "predict_media/newspaper_precision": 0.5205254515599343,
    "predict_media/newspaper_recall": 0.513776337115073,
    "predict_medical_f1": 0.26920634920634917,
    "predict_medical_number": 397,
    "predict_medical_precision": 0.1799660441426146,
    "predict_medical_recall": 0.5340050377833753,
    "predict_mountain_f1": 0.4814573845152896,
    "predict_mountain_number": 681,
    "predict_mountain_precision": 0.4322429906542056,
    "predict_mountain_recall": 0.5433186490455213,
    "predict_music_f1": 0.5666347075743049,
    "predict_music_number": 1041,
    "predict_music_precision": 0.5655502392344498,
    "predict_music_recall": 0.5677233429394812,
    "predict_other_f1": 0.4269793459552496,
    "predict_other_number": 21035,
    "predict_other_precision": 0.38997838475142466,
    "predict_other_recall": 0.47173758022343715,
    "predict_overall_accuracy": 0.8938509577071485,
    "predict_overall_f1": 0.5351215518506074,
    "predict_overall_precision": 0.49413985683945566,
    "predict_overall_recall": 0.5835156669281262,
    "predict_painting_f1": 0.0,
    "predict_painting_number": 58,
    "predict_painting_precision": 0.0,
    "predict_painting_recall": 0.0,
    "predict_park_f1": 0.27171109200343935,
    "predict_park_number": 458,
    "predict_park_precision": 0.22411347517730495,
    "predict_park_recall": 0.34497816593886466,
    "predict_politicalparty_f1": 0.5634146341463414,
    "predict_politicalparty_number": 1057,
    "predict_politicalparty_precision": 0.49394155381325733,
    "predict_politicalparty_recall": 0.6556291390728477,
    "predict_politician_f1": 0.5181104990819562,
    "predict_politician_number": 2859,
    "predict_politician_precision": 0.4955300127713921,
    "predict_politician_recall": 0.5428471493529206,
    "predict_protest_f1": 0.22292993630573252,
    "predict_protest_number": 166,
    "predict_protest_precision": 0.23648648648648649,
    "predict_protest_recall": 0.21084337349397592,
    "predict_religion_f1": 0.40329218106995884,
    "predict_religion_number": 676,
    "predict_religion_precision": 0.33463414634146343,
    "predict_religion_recall": 0.507396449704142,
    "predict_restaurant_f1": 0.25531914893617025,
    "predict_restaurant_number": 232,
    "predict_restaurant_precision": 0.25210084033613445,
    "predict_restaurant_recall": 0.25862068965517243,
    "predict_road/railway/highway/transit_f1": 0.557543391188251,
    "predict_road/railway/highway/transit_number": 1702,
    "predict_road/railway/highway/transit_precision": 0.5110132158590308,
    "predict_road/railway/highway/transit_recall": 0.6133960047003525,
    "predict_runtime": 159.6332,
    "predict_samples_per_second": 235.834,
    "predict_scholar_f1": 0.2601726263871763,
    "predict_scholar_number": 743,
    "predict_scholar_precision": 0.24004550625711035,
    "predict_scholar_recall": 0.2839838492597577,
    "predict_ship_f1": 0.39359267734553777,
    "predict_ship_number": 380,
    "predict_ship_precision": 0.3481781376518219,
    "predict_ship_recall": 0.45263157894736844,
    "predict_showorganization_f1": 0.3858632676709154,
    "predict_showorganization_number": 770,
    "predict_showorganization_precision": 0.348326359832636,
    "predict_showorganization_recall": 0.43246753246753245,
    "predict_software_f1": 0.5257784583971413,
    "predict_software_number": 889,
    "predict_software_precision": 0.48130841121495327,
    "predict_software_recall": 0.5793025871766029,
    "predict_soldier_f1": 0.36182572614107894,
    "predict_soldier_number": 647,
    "predict_soldier_precision": 0.3906810035842294,
    "predict_soldier_recall": 0.3369397217928903,
    "predict_sportsevent_f1": 0.47541444225906154,
    "predict_sportsevent_number": 1572,
    "predict_sportsevent_precision": 0.4257674886763966,
    "predict_sportsevent_recall": 0.5381679389312977,
    "predict_sportsfacility_f1": 0.522975929978118,
    "predict_sportsfacility_number": 420,
    "predict_sportsfacility_precision": 0.48380566801619435,
    "predict_sportsfacility_recall": 0.569047619047619,
    "predict_sportsleague_f1": 0.48250795093139487,
    "predict_sportsleague_number": 885,
    "predict_sportsleague_precision": 0.4034954407294833,
    "predict_sportsleague_recall": 0.6,
    "predict_sportsteam_f1": 0.6202012672381662,
    "predict_sportsteam_number": 2477,
    "predict_sportsteam_precision": 0.5759778470058844,
    "predict_sportsteam_recall": 0.6717803794913202,
    "predict_steps_per_second": 7.373,
    "predict_theater_f1": 0.5252525252525253,
    "predict_theater_number": 456,
    "predict_theater_precision": 0.4868913857677903,
    "predict_theater_recall": 0.5701754385964912,
    "predict_train_f1": 0.2933709449929478,
    "predict_train_number": 314,
    "predict_train_precision": 0.26329113924050634,
    "predict_train_recall": 0.33121019108280253,
    "predict_weapon_f1": 0.40437956204379555,
    "predict_weapon_number": 625,
    "predict_weapon_precision": 0.3718120805369127,
    "predict_weapon_recall": 0.4432,
    "predict_writtenart_f1": 0.372666431842198,
    "predict_writtenart_number": 1032,
    "predict_writtenart_precision": 0.2927504150525733,
    "predict_writtenart_recall": 0.5125968992248062
}