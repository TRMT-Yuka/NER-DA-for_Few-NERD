{
    "predict_GPE_f1": 0.760481173037227,
    "predict_GPE_number": 20409,
    "predict_GPE_precision": 0.7372343361854816,
    "predict_GPE_recall": 0.7852418050859915,
    "predict_actor_f1": 0.6477241379310346,
    "predict_actor_number": 1638,
    "predict_actor_precision": 0.5908404630095622,
    "predict_actor_recall": 0.7167277167277167,
    "predict_airplane_f1": 0.5216316440049444,
    "predict_airplane_number": 794,
    "predict_airplane_precision": 0.5121359223300971,
    "predict_airplane_recall": 0.5314861460957179,
    "predict_airport_f1": 0.6596583442838371,
    "predict_airport_number": 364,
    "predict_airport_precision": 0.6322418136020151,
    "predict_airport_recall": 0.6895604395604396,
    "predict_artist/author_f1": 0.5142299107142857,
    "predict_artist/author_number": 3464,
    "predict_artist/author_precision": 0.49757019438444927,
    "predict_artist/author_recall": 0.5320438799076213,
    "predict_astronomything_f1": 0.5885797950219619,
    "predict_astronomything_number": 678,
    "predict_astronomything_precision": 0.5843023255813954,
    "predict_astronomything_recall": 0.5929203539823009,
    "predict_athlete_f1": 0.7437734561583078,
    "predict_athlete_number": 2907,
    "predict_athlete_precision": 0.7377326565143824,
    "predict_athlete_recall": 0.7499140006879945,
    "predict_attack/battle/war/militaryconflict_f1": 0.6084388185654008,
    "predict_attack/battle/war/militaryconflict_number": 1099,
    "predict_attack/battle/war/militaryconflict_precision": 0.5672698662470496,
    "predict_attack/battle/war/militaryconflict_recall": 0.6560509554140127,
    "predict_award_f1": 0.4527736131934033,
    "predict_award_number": 941,
    "predict_award_precision": 0.42735849056603775,
    "predict_award_recall": 0.4814027630180659,
    "predict_biologything_f1": 0.47032374100719426,
    "predict_biologything_number": 1875,
    "predict_biologything_precision": 0.4065293431791683,
    "predict_biologything_recall": 0.5578666666666666,
    "predict_bodiesofwater_f1": 0.6133440514469454,
    "predict_bodiesofwater_number": 1169,
    "predict_bodiesofwater_precision": 0.5784685367702805,
    "predict_bodiesofwater_recall": 0.6526946107784432,
    "predict_broadcastprogram_f1": 0.294167371090448,
    "predict_broadcastprogram_number": 605,
    "predict_broadcastprogram_precision": 0.30103806228373703,
    "predict_broadcastprogram_recall": 0.28760330578512394,
    "predict_car_f1": 0.5787831513260531,
    "predict_car_number": 688,
    "predict_car_precision": 0.6245791245791246,
    "predict_car_recall": 0.5392441860465116,
    "predict_chemicalthing_f1": 0.3745435576421492,
    "predict_chemicalthing_number": 1014,
    "predict_chemicalthing_precision": 0.39756367663344405,
    "predict_chemicalthing_recall": 0.35404339250493094,
    "predict_company_f1": 0.5491793458727687,
    "predict_company_number": 3903,
    "predict_company_precision": 0.5157515751575158,
    "predict_company_recall": 0.5872405841660261,
    "predict_currency_f1": 0.6108140225787284,
    "predict_currency_number": 799,
    "predict_currency_precision": 0.581447963800905,
    "predict_currency_recall": 0.6433041301627034,
    "predict_director_f1": 0.45216049382716045,
    "predict_director_number": 554,
    "predict_director_precision": 0.3948787061994609,
    "predict_director_recall": 0.5288808664259927,
    "predict_disaster_f1": 0.3578947368421053,
    "predict_disaster_number": 207,
    "predict_disaster_precision": 0.3930635838150289,
    "predict_disaster_recall": 0.3285024154589372,
    "predict_disease_f1": 0.48958333333333326,
    "predict_disease_number": 750,
    "predict_disease_precision": 0.4017094017094017,
    "predict_disease_recall": 0.6266666666666667,
    "predict_education_f1": 0.6596184785106872,
    "predict_education_number": 2082,
    "predict_education_precision": 0.6324371970030851,
    "predict_education_recall": 0.6892411143131604,
    "predict_educationaldegree_f1": 0.352317880794702,
    "predict_educationaldegree_number": 367,
    "predict_educationaldegree_precision": 0.3427835051546392,
    "predict_educationaldegree_recall": 0.36239782016348776,
    "predict_election_f1": 0.11297071129707113,
    "predict_election_number": 184,
    "predict_election_precision": 0.09183673469387756,
    "predict_election_recall": 0.14673913043478262,
    "predict_film_f1": 0.4730913642052566,
    "predict_film_number": 759,
    "predict_film_precision": 0.4505363528009535,
    "predict_film_recall": 0.4980237154150198,
    "predict_food_f1": 0.3951417004048583,
    "predict_food_number": 432,
    "predict_food_precision": 0.3038605230386052,
    "predict_food_recall": 0.5648148148148148,
    "predict_game_f1": 0.49108079748163697,
    "predict_game_number": 496,
    "predict_game_precision": 0.5120350109409191,
    "predict_game_recall": 0.4717741935483871,
    "predict_god_f1": 0.4417965169569203,
    "predict_god_number": 635,
    "predict_god_precision": 0.5285087719298246,
    "predict_god_recall": 0.3795275590551181,
    "predict_government/governmentagency_f1": 0.25116279069767444,
    "predict_government/governmentagency_number": 1535,
    "predict_government/governmentagency_precision": 0.23964497041420119,
    "predict_government/governmentagency_recall": 0.26384364820846906,
    "predict_hospital_f1": 0.577023498694517,
    "predict_hospital_number": 364,
    "predict_hospital_precision": 0.5497512437810945,
    "predict_hospital_recall": 0.6071428571428571,
    "predict_hotel_f1": 0.5060240963855422,
    "predict_hotel_number": 265,
    "predict_hotel_precision": 0.4651898734177215,
    "predict_hotel_recall": 0.5547169811320755,
    "predict_island_f1": 0.5141015310233682,
    "predict_island_number": 646,
    "predict_island_precision": 0.5361344537815126,
    "predict_island_recall": 0.4938080495356037,
    "predict_language_f1": 0.5584699453551912,
    "predict_language_number": 753,
    "predict_language_precision": 0.4744661095636026,
    "predict_language_recall": 0.6786188579017264,
    "predict_law_f1": 0.369691923397169,
    "predict_law_number": 488,
    "predict_law_precision": 0.31136044880785413,
    "predict_law_recall": 0.45491803278688525,
    "predict_library_f1": 0.5296803652968037,
    "predict_library_number": 357,
    "predict_library_precision": 0.44701348747591524,
    "predict_library_recall": 0.6498599439775911,
    "predict_livingthing_f1": 0.5227513227513227,
    "predict_livingthing_number": 865,
    "predict_livingthing_precision": 0.4819512195121951,
    "predict_livingthing_recall": 0.5710982658959538,
    "predict_loss": 0.5235741138458252,
    "predict_media/newspaper_f1": 0.495253164556962,
    "predict_media/newspaper_number": 1234,
    "predict_media/newspaper_precision": 0.48377125193199383,
    "predict_media/newspaper_recall": 0.5072933549432739,
    "predict_medical_f1": 0.34795763993948564,
    "predict_medical_number": 397,
    "predict_medical_precision": 0.4356060606060606,
    "predict_medical_recall": 0.28967254408060455,
    "predict_mountain_f1": 0.5666929755327546,
    "predict_mountain_number": 681,
    "predict_mountain_precision": 0.6126279863481229,
    "predict_mountain_recall": 0.527165932452276,
    "predict_music_f1": 0.552930883639545,
    "predict_music_number": 1041,
    "predict_music_precision": 0.5076305220883535,
    "predict_music_recall": 0.6071085494716618,
    "predict_other_f1": 0.4293271542867136,
    "predict_other_number": 21035,
    "predict_other_precision": 0.3973938731738902,
    "predict_other_recall": 0.46684097932018065,
    "predict_overall_accuracy": 0.896578267940859,
    "predict_overall_f1": 0.5435085724835181,
    "predict_overall_precision": 0.5134522356137726,
    "predict_overall_recall": 0.5773025636791479,
    "predict_painting_f1": 0.0,
    "predict_painting_number": 58,
    "predict_painting_precision": 0.0,
    "predict_painting_recall": 0.0,
    "predict_park_f1": 0.44021739130434784,
    "predict_park_number": 458,
    "predict_park_precision": 0.3761609907120743,
    "predict_park_recall": 0.5305676855895196,
    "predict_politicalparty_f1": 0.5697577276524645,
    "predict_politicalparty_number": 1057,
    "predict_politicalparty_precision": 0.5100972326103216,
    "predict_politicalparty_recall": 0.6452223273415326,
    "predict_politician_f1": 0.5057513237173634,
    "predict_politician_number": 2859,
    "predict_politician_precision": 0.529029793735676,
    "predict_politician_recall": 0.484435117173837,
    "predict_protest_f1": 0.2054794520547945,
    "predict_protest_number": 166,
    "predict_protest_precision": 0.16544117647058823,
    "predict_protest_recall": 0.2710843373493976,
    "predict_religion_f1": 0.4096885813148789,
    "predict_religion_number": 676,
    "predict_religion_precision": 0.38491547464239273,
    "predict_religion_recall": 0.4378698224852071,
    "predict_restaurant_f1": 0.26146788990825687,
    "predict_restaurant_number": 232,
    "predict_restaurant_precision": 0.27941176470588236,
    "predict_restaurant_recall": 0.24568965517241378,
    "predict_road/railway/highway/transit_f1": 0.5844679778113969,
    "predict_road/railway/highway/transit_number": 1702,
    "predict_road/railway/highway/transit_precision": 0.5119257950530035,
    "predict_road/railway/highway/transit_recall": 0.6809635722679201,
    "predict_runtime": 272.0987,
    "predict_samples_per_second": 138.358,
    "predict_scholar_f1": 0.2702702702702703,
    "predict_scholar_number": 743,
    "predict_scholar_precision": 0.2589395807644883,
    "predict_scholar_recall": 0.28263795423956933,
    "predict_ship_f1": 0.47014925373134325,
    "predict_ship_number": 380,
    "predict_ship_precision": 0.44575471698113206,
    "predict_ship_recall": 0.49736842105263157,
    "predict_showorganization_f1": 0.4040955631399317,
    "predict_showorganization_number": 770,
    "predict_showorganization_precision": 0.42589928057553955,
    "predict_showorganization_recall": 0.38441558441558443,
    "predict_software_f1": 0.46007151370679383,
    "predict_software_number": 889,
    "predict_software_precision": 0.4892268694550063,
    "predict_software_recall": 0.4341957255343082,
    "predict_soldier_f1": 0.30985915492957744,
    "predict_soldier_number": 647,
    "predict_soldier_precision": 0.3339285714285714,
    "predict_soldier_recall": 0.2890262751159196,
    "predict_sportsevent_f1": 0.43995572772551195,
    "predict_sportsevent_number": 1572,
    "predict_sportsevent_precision": 0.3893241919686582,
    "predict_sportsevent_recall": 0.5057251908396947,
    "predict_sportsfacility_f1": 0.5695364238410596,
    "predict_sportsfacility_number": 420,
    "predict_sportsfacility_precision": 0.5308641975308642,
    "predict_sportsfacility_recall": 0.6142857142857143,
    "predict_sportsleague_f1": 0.40988372093023256,
    "predict_sportsleague_number": 885,
    "predict_sportsleague_precision": 0.35877862595419846,
    "predict_sportsleague_recall": 0.47796610169491527,
    "predict_sportsteam_f1": 0.6499614494988435,
    "predict_sportsteam_number": 2477,
    "predict_sportsteam_precision": 0.6219107340464773,
    "predict_sportsteam_recall": 0.6806620912394025,
    "predict_steps_per_second": 4.326,
    "predict_theater_f1": 0.5702393340270552,
    "predict_theater_number": 456,
    "predict_theater_precision": 0.5425742574257426,
    "predict_theater_recall": 0.6008771929824561,
    "predict_train_f1": 0.23309352517985613,
    "predict_train_number": 314,
    "predict_train_precision": 0.2125984251968504,
    "predict_train_recall": 0.25796178343949044,
    "predict_weapon_f1": 0.39462636439966414,
    "predict_weapon_number": 625,
    "predict_weapon_precision": 0.41519434628975266,
    "predict_weapon_recall": 0.376,
    "predict_writtenart_f1": 0.3124773960216998,
    "predict_writtenart_number": 1032,
    "predict_writtenart_precision": 0.24927870744373917,
    "predict_writtenart_recall": 0.4186046511627907
}