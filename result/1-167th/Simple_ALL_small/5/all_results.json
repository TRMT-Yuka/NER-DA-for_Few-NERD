{
    "predict_GPE_f1": 0.7558028464991879,
    "predict_GPE_number": 20409,
    "predict_GPE_precision": 0.7371779941305259,
    "predict_GPE_recall": 0.775393208878436,
    "predict_actor_f1": 0.655785485281259,
    "predict_actor_number": 1638,
    "predict_actor_precision": 0.6274400446179588,
    "predict_actor_recall": 0.6868131868131868,
    "predict_airplane_f1": 0.49211711711711714,
    "predict_airplane_number": 794,
    "predict_airplane_precision": 0.445010183299389,
    "predict_airplane_recall": 0.5503778337531486,
    "predict_airport_f1": 0.6503067484662576,
    "predict_airport_number": 364,
    "predict_airport_precision": 0.5875831485587583,
    "predict_airport_recall": 0.728021978021978,
    "predict_artist/author_f1": 0.5484573038023302,
    "predict_artist/author_number": 3464,
    "predict_artist/author_precision": 0.4927536231884058,
    "predict_artist/author_recall": 0.6183602771362586,
    "predict_astronomything_f1": 0.5839616175462646,
    "predict_astronomything_number": 678,
    "predict_astronomything_precision": 0.5454545454545454,
    "predict_astronomything_recall": 0.6283185840707964,
    "predict_athlete_f1": 0.7470597979128706,
    "predict_athlete_number": 2907,
    "predict_athlete_precision": 0.7204472843450479,
    "predict_athlete_recall": 0.7757137942896457,
    "predict_attack/battle/war/militaryconflict_f1": 0.5709515859766278,
    "predict_attack/battle/war/militaryconflict_number": 1099,
    "predict_attack/battle/war/militaryconflict_precision": 0.5273708558211256,
    "predict_attack/battle/war/militaryconflict_recall": 0.6223839854413102,
    "predict_award_f1": 0.4113545816733068,
    "predict_award_number": 941,
    "predict_award_precision": 0.38706654170571697,
    "predict_award_recall": 0.43889479277364507,
    "predict_biologything_f1": 0.49627933600457924,
    "predict_biologything_number": 1875,
    "predict_biologything_precision": 0.535515750463249,
    "predict_biologything_recall": 0.4624,
    "predict_bodiesofwater_f1": 0.6077821011673151,
    "predict_bodiesofwater_number": 1169,
    "predict_bodiesofwater_precision": 0.5574589578872234,
    "predict_bodiesofwater_recall": 0.6680923866552609,
    "predict_broadcastprogram_f1": 0.4385964912280702,
    "predict_broadcastprogram_number": 605,
    "predict_broadcastprogram_precision": 0.4672897196261682,
    "predict_broadcastprogram_recall": 0.4132231404958678,
    "predict_car_f1": 0.5510349750178445,
    "predict_car_number": 688,
    "predict_car_precision": 0.541374474053296,
    "predict_car_recall": 0.561046511627907,
    "predict_chemicalthing_f1": 0.46804759806081975,
    "predict_chemicalthing_number": 1014,
    "predict_chemicalthing_precision": 0.4231075697211155,
    "predict_chemicalthing_recall": 0.5236686390532544,
    "predict_company_f1": 0.5359555984555985,
    "predict_company_number": 3903,
    "predict_company_precision": 0.5064994298745724,
    "predict_company_recall": 0.5690494491416859,
    "predict_currency_f1": 0.6502732240437157,
    "predict_currency_number": 799,
    "predict_currency_precision": 0.577109602327837,
    "predict_currency_recall": 0.7446808510638298,
    "predict_director_f1": 0.47325769854132904,
    "predict_director_number": 554,
    "predict_director_precision": 0.4294117647058823,
    "predict_director_recall": 0.5270758122743683,
    "predict_disaster_f1": 0.36853002070393376,
    "predict_disaster_number": 207,
    "predict_disaster_precision": 0.322463768115942,
    "predict_disaster_recall": 0.42995169082125606,
    "predict_disease_f1": 0.4618357487922706,
    "predict_disease_number": 750,
    "predict_disease_precision": 0.3621212121212121,
    "predict_disease_recall": 0.6373333333333333,
    "predict_education_f1": 0.6266517357222845,
    "predict_education_number": 2082,
    "predict_education_precision": 0.5870751154007553,
    "predict_education_recall": 0.6719500480307397,
    "predict_educationaldegree_f1": 0.4027617951668585,
    "predict_educationaldegree_number": 367,
    "predict_educationaldegree_precision": 0.34860557768924305,
    "predict_educationaldegree_recall": 0.4768392370572207,
    "predict_election_f1": 0.1477832512315271,
    "predict_election_number": 184,
    "predict_election_precision": 0.13513513513513514,
    "predict_election_recall": 0.16304347826086957,
    "predict_film_f1": 0.508456082924168,
    "predict_film_number": 759,
    "predict_film_precision": 0.4338919925512104,
    "predict_film_recall": 0.613965744400527,
    "predict_food_f1": 0.3698744769874477,
    "predict_food_number": 432,
    "predict_food_precision": 0.28964613368283093,
    "predict_food_recall": 0.5115740740740741,
    "predict_game_f1": 0.48347497639282344,
    "predict_game_number": 496,
    "predict_game_precision": 0.4547069271758437,
    "predict_game_recall": 0.5161290322580645,
    "predict_god_f1": 0.4747404844290657,
    "predict_god_number": 635,
    "predict_god_precision": 0.42345679012345677,
    "predict_god_recall": 0.5401574803149606,
    "predict_government/governmentagency_f1": 0.2609763586122199,
    "predict_government/governmentagency_number": 1535,
    "predict_government/governmentagency_precision": 0.24680603948896632,
    "predict_government/governmentagency_recall": 0.2768729641693811,
    "predict_hospital_f1": 0.6093750000000001,
    "predict_hospital_number": 364,
    "predict_hospital_precision": 0.5131578947368421,
    "predict_hospital_recall": 0.75,
    "predict_hotel_f1": 0.5536332179930796,
    "predict_hotel_number": 265,
    "predict_hotel_precision": 0.5111821086261981,
    "predict_hotel_recall": 0.6037735849056604,
    "predict_island_f1": 0.5263157894736842,
    "predict_island_number": 646,
    "predict_island_precision": 0.594541910331384,
    "predict_island_recall": 0.47213622291021673,
    "predict_language_f1": 0.575682382133995,
    "predict_language_number": 753,
    "predict_language_precision": 0.540162980209546,
    "predict_language_recall": 0.6162018592297477,
    "predict_law_f1": 0.32172470978441126,
    "predict_law_number": 488,
    "predict_law_precision": 0.27019498607242337,
    "predict_law_recall": 0.3975409836065574,
    "predict_library_f1": 0.5946666666666667,
    "predict_library_number": 357,
    "predict_library_precision": 0.5674300254452926,
    "predict_library_recall": 0.6246498599439776,
    "predict_livingthing_f1": 0.5248380129589634,
    "predict_livingthing_number": 865,
    "predict_livingthing_precision": 0.49240121580547114,
    "predict_livingthing_recall": 0.561849710982659,
    "predict_loss": 0.5150847434997559,
    "predict_media/newspaper_f1": 0.5368606701940036,
    "predict_media/newspaper_number": 1234,
    "predict_media/newspaper_precision": 0.47532792004996877,
    "predict_media/newspaper_recall": 0.6166936790923825,
    "predict_medical_f1": 0.38969072164948454,
    "predict_medical_number": 397,
    "predict_medical_precision": 0.3298429319371728,
    "predict_medical_recall": 0.4760705289672544,
    "predict_mountain_f1": 0.5555555555555556,
    "predict_mountain_number": 681,
    "predict_mountain_precision": 0.4915254237288136,
    "predict_mountain_recall": 0.6387665198237885,
    "predict_music_f1": 0.5322153574580759,
    "predict_music_number": 1041,
    "predict_music_precision": 0.4922448979591837,
    "predict_music_recall": 0.579250720461095,
    "predict_other_f1": 0.42349679431640963,
    "predict_other_number": 21035,
    "predict_other_precision": 0.38897067600366053,
    "predict_other_recall": 0.46474922747801284,
    "predict_overall_accuracy": 0.8954165553811178,
    "predict_overall_f1": 0.545625243737452,
    "predict_overall_precision": 0.5113571757316512,
    "predict_overall_recall": 0.5848160838872146,
    "predict_painting_f1": 0.0,
    "predict_painting_number": 58,
    "predict_painting_precision": 0.0,
    "predict_painting_recall": 0.0,
    "predict_park_f1": 0.36813778256189444,
    "predict_park_number": 458,
    "predict_park_precision": 0.3630573248407643,
    "predict_park_recall": 0.37336244541484714,
    "predict_politicalparty_f1": 0.5670864090316979,
    "predict_politicalparty_number": 1057,
    "predict_politicalparty_precision": 0.5240770465489567,
    "predict_politicalparty_recall": 0.6177861873226111,
    "predict_politician_f1": 0.5190203230849401,
    "predict_politician_number": 2859,
    "predict_politician_precision": 0.515527950310559,
    "predict_politician_recall": 0.5225603357817419,
    "predict_protest_f1": 0.1232876712328767,
    "predict_protest_number": 166,
    "predict_protest_precision": 0.14285714285714285,
    "predict_protest_recall": 0.10843373493975904,
    "predict_religion_f1": 0.407380073800738,
    "predict_religion_number": 676,
    "predict_religion_precision": 0.406480117820324,
    "predict_religion_recall": 0.40828402366863903,
    "predict_restaurant_f1": 0.23255813953488372,
    "predict_restaurant_number": 232,
    "predict_restaurant_precision": 0.25252525252525254,
    "predict_restaurant_recall": 0.21551724137931033,
    "predict_road/railway/highway/transit_f1": 0.5993086944961447,
    "predict_road/railway/highway/transit_number": 1702,
    "predict_road/railway/highway/transit_precision": 0.5473530840213696,
    "predict_road/railway/highway/transit_recall": 0.6621621621621622,
    "predict_runtime": 234.3992,
    "predict_samples_per_second": 160.611,
    "predict_scholar_f1": 0.2593346911065852,
    "predict_scholar_number": 743,
    "predict_scholar_precision": 0.26164383561643834,
    "predict_scholar_recall": 0.2570659488559892,
    "predict_ship_f1": 0.4413043478260869,
    "predict_ship_number": 380,
    "predict_ship_precision": 0.37592592592592594,
    "predict_ship_recall": 0.5342105263157895,
    "predict_showorganization_f1": 0.3722627737226277,
    "predict_showorganization_number": 770,
    "predict_showorganization_precision": 0.425,
    "predict_showorganization_recall": 0.33116883116883117,
    "predict_software_f1": 0.45322245322245325,
    "predict_software_number": 889,
    "predict_software_precision": 0.421256038647343,
    "predict_software_recall": 0.4904386951631046,
    "predict_soldier_f1": 0.3439878234398782,
    "predict_soldier_number": 647,
    "predict_soldier_precision": 0.33883058470764615,
    "predict_soldier_recall": 0.34930448222565685,
    "predict_sportsevent_f1": 0.4317516110134739,
    "predict_sportsevent_number": 1572,
    "predict_sportsevent_precision": 0.4001085776330076,
    "predict_sportsevent_recall": 0.4688295165394402,
    "predict_sportsfacility_f1": 0.5602094240837696,
    "predict_sportsfacility_number": 420,
    "predict_sportsfacility_precision": 0.44214876033057854,
    "predict_sportsfacility_recall": 0.7642857142857142,
    "predict_sportsleague_f1": 0.4855381697486961,
    "predict_sportsleague_number": 885,
    "predict_sportsleague_precision": 0.41830065359477125,
    "predict_sportsleague_recall": 0.5785310734463277,
    "predict_sportsteam_f1": 0.6426356589147287,
    "predict_sportsteam_number": 2477,
    "predict_sportsteam_precision": 0.6179649645918748,
    "predict_sportsteam_recall": 0.6693580944691159,
    "predict_steps_per_second": 5.021,
    "predict_theater_f1": 0.6043737574552684,
    "predict_theater_number": 456,
    "predict_theater_precision": 0.5527272727272727,
    "predict_theater_recall": 0.6666666666666666,
    "predict_train_f1": 0.28353658536585363,
    "predict_train_number": 314,
    "predict_train_precision": 0.2719298245614035,
    "predict_train_recall": 0.2961783439490446,
    "predict_weapon_f1": 0.3634854771784232,
    "predict_weapon_number": 625,
    "predict_weapon_precision": 0.3775862068965517,
    "predict_weapon_recall": 0.3504,
    "predict_writtenart_f1": 0.4058219178082192,
    "predict_writtenart_number": 1032,
    "predict_writtenart_precision": 0.36349693251533743,
    "predict_writtenart_recall": 0.45930232558139533
}