{
    "predict_GPE_f1": 0.7589770579651206,
    "predict_GPE_number": 20409,
    "predict_GPE_precision": 0.7415388930441287,
    "predict_GPE_recall": 0.7772551325395659,
    "predict_actor_f1": 0.6602953953084275,
    "predict_actor_number": 1638,
    "predict_actor_precision": 0.628099173553719,
    "predict_actor_recall": 0.6959706959706959,
    "predict_airplane_f1": 0.5071599045346062,
    "predict_airplane_number": 794,
    "predict_airplane_precision": 0.481859410430839,
    "predict_airplane_recall": 0.535264483627204,
    "predict_airport_f1": 0.6642685851318945,
    "predict_airport_number": 364,
    "predict_airport_precision": 0.5893617021276596,
    "predict_airport_recall": 0.760989010989011,
    "predict_artist/author_f1": 0.5186053325520069,
    "predict_artist/author_number": 3464,
    "predict_artist/author_precision": 0.5264723378941106,
    "predict_artist/author_recall": 0.5109699769053118,
    "predict_astronomything_f1": 0.6055437100213219,
    "predict_astronomything_number": 678,
    "predict_astronomything_precision": 0.5843621399176955,
    "predict_astronomything_recall": 0.6283185840707964,
    "predict_athlete_f1": 0.7652712168105555,
    "predict_athlete_number": 2907,
    "predict_athlete_precision": 0.7267945544554455,
    "predict_athlete_recall": 0.8080495356037152,
    "predict_attack/battle/war/militaryconflict_f1": 0.5823627287853577,
    "predict_attack/battle/war/militaryconflict_number": 1099,
    "predict_attack/battle/war/militaryconflict_precision": 0.5363984674329502,
    "predict_attack/battle/war/militaryconflict_recall": 0.6369426751592356,
    "predict_award_f1": 0.4131868131868132,
    "predict_award_number": 941,
    "predict_award_precision": 0.3523238380809595,
    "predict_award_recall": 0.49946865037194477,
    "predict_biologything_f1": 0.5035425101214575,
    "predict_biologything_number": 1875,
    "predict_biologything_precision": 0.4790563312469909,
    "predict_biologything_recall": 0.5306666666666666,
    "predict_bodiesofwater_f1": 0.6062861869313483,
    "predict_bodiesofwater_number": 1169,
    "predict_bodiesofwater_precision": 0.5868694955964772,
    "predict_bodiesofwater_recall": 0.6270316509837468,
    "predict_broadcastprogram_f1": 0.35918367346938773,
    "predict_broadcastprogram_number": 605,
    "predict_broadcastprogram_precision": 0.3548387096774194,
    "predict_broadcastprogram_recall": 0.36363636363636365,
    "predict_car_f1": 0.5817438692098094,
    "predict_car_number": 688,
    "predict_car_precision": 0.5474358974358975,
    "predict_car_recall": 0.6206395348837209,
    "predict_chemicalthing_f1": 0.34698275862068967,
    "predict_chemicalthing_number": 1014,
    "predict_chemicalthing_precision": 0.3824228028503563,
    "predict_chemicalthing_recall": 0.3175542406311637,
    "predict_company_f1": 0.5632790028763183,
    "predict_company_number": 3903,
    "predict_company_precision": 0.5291600990767845,
    "predict_company_recall": 0.6021009479887266,
    "predict_currency_f1": 0.6584821428571428,
    "predict_currency_number": 799,
    "predict_currency_precision": 0.594159113796576,
    "predict_currency_recall": 0.7384230287859824,
    "predict_director_f1": 0.4244229337304542,
    "predict_director_number": 554,
    "predict_director_precision": 0.3612167300380228,
    "predict_director_recall": 0.5144404332129964,
    "predict_disaster_f1": 0.3893805309734513,
    "predict_disaster_number": 207,
    "predict_disaster_precision": 0.35918367346938773,
    "predict_disaster_recall": 0.4251207729468599,
    "predict_disease_f1": 0.49481865284974086,
    "predict_disease_number": 750,
    "predict_disease_precision": 0.4811083123425693,
    "predict_disease_recall": 0.5093333333333333,
    "predict_education_f1": 0.6623646001382807,
    "predict_education_number": 2082,
    "predict_education_precision": 0.6366858661940629,
    "predict_education_recall": 0.6902017291066282,
    "predict_educationaldegree_f1": 0.4008264462809918,
    "predict_educationaldegree_number": 367,
    "predict_educationaldegree_precision": 0.3227953410981697,
    "predict_educationaldegree_recall": 0.5286103542234333,
    "predict_election_f1": 0.16985138004246284,
    "predict_election_number": 184,
    "predict_election_precision": 0.13937282229965156,
    "predict_election_recall": 0.21739130434782608,
    "predict_film_f1": 0.4606481481481482,
    "predict_film_number": 759,
    "predict_film_precision": 0.4107327141382869,
    "predict_film_recall": 0.5243741765480896,
    "predict_food_f1": 0.3817204301075269,
    "predict_food_number": 432,
    "predict_food_precision": 0.31140350877192985,
    "predict_food_recall": 0.4930555555555556,
    "predict_game_f1": 0.27704485488126646,
    "predict_game_number": 496,
    "predict_game_precision": 0.40076335877862596,
    "predict_game_recall": 0.21169354838709678,
    "predict_god_f1": 0.4888888888888889,
    "predict_god_number": 635,
    "predict_god_precision": 0.42705882352941177,
    "predict_god_recall": 0.5716535433070866,
    "predict_government/governmentagency_f1": 0.32007073386383733,
    "predict_government/governmentagency_number": 1535,
    "predict_government/governmentagency_precision": 0.2922497308934338,
    "predict_government/governmentagency_recall": 0.3537459283387622,
    "predict_hospital_f1": 0.5688311688311687,
    "predict_hospital_number": 364,
    "predict_hospital_precision": 0.5394088669950738,
    "predict_hospital_recall": 0.6016483516483516,
    "predict_hotel_f1": 0.5259259259259259,
    "predict_hotel_number": 265,
    "predict_hotel_precision": 0.5163636363636364,
    "predict_hotel_recall": 0.5358490566037736,
    "predict_island_f1": 0.4844384303112314,
    "predict_island_number": 646,
    "predict_island_precision": 0.43028846153846156,
    "predict_island_recall": 0.5541795665634675,
    "predict_language_f1": 0.6031941031941032,
    "predict_language_number": 753,
    "predict_language_precision": 0.5611428571428572,
    "predict_language_recall": 0.6520584329349269,
    "predict_law_f1": 0.43127962085308064,
    "predict_law_number": 488,
    "predict_law_precision": 0.3508997429305913,
    "predict_law_recall": 0.5594262295081968,
    "predict_library_f1": 0.5412054120541206,
    "predict_library_number": 357,
    "predict_library_precision": 0.4824561403508772,
    "predict_library_recall": 0.6162464985994398,
    "predict_livingthing_f1": 0.5253227408142999,
    "predict_livingthing_number": 865,
    "predict_livingthing_precision": 0.46040034812880765,
    "predict_livingthing_recall": 0.6115606936416185,
    "predict_loss": 0.6557647585868835,
    "predict_media/newspaper_f1": 0.5093724531377343,
    "predict_media/newspaper_number": 1234,
    "predict_media/newspaper_precision": 0.5122950819672131,
    "predict_media/newspaper_recall": 0.506482982171799,
    "predict_medical_f1": 0.30303030303030304,
    "predict_medical_number": 397,
    "predict_medical_precision": 0.21443514644351463,
    "predict_medical_recall": 0.5163727959697733,
    "predict_mountain_f1": 0.4699140401146132,
    "predict_mountain_number": 681,
    "predict_mountain_precision": 0.45874125874125876,
    "predict_mountain_recall": 0.48164464023494863,
    "predict_music_f1": 0.5854870775347912,
    "predict_music_number": 1041,
    "predict_music_precision": 0.6065911431513903,
    "predict_music_recall": 0.5658021133525456,
    "predict_other_f1": 0.43537236942536256,
    "predict_other_number": 21035,
    "predict_other_precision": 0.40217566478646255,
    "predict_other_recall": 0.4745424292845258,
    "predict_overall_accuracy": 0.8953058126885068,
    "predict_overall_f1": 0.5509333847860692,
    "predict_overall_precision": 0.5171892263942474,
    "predict_overall_recall": 0.5893881847830574,
    "predict_painting_f1": 0.0,
    "predict_painting_number": 58,
    "predict_painting_precision": 0.0,
    "predict_painting_recall": 0.0,
    "predict_park_f1": 0.3250883392226149,
    "predict_park_number": 458,
    "predict_park_precision": 0.27299703264094954,
    "predict_park_recall": 0.4017467248908297,
    "predict_politicalparty_f1": 0.578039545645772,
    "predict_politicalparty_number": 1057,
    "predict_politicalparty_precision": 0.5204545454545455,
    "predict_politicalparty_recall": 0.6499526963103122,
    "predict_politician_f1": 0.5449682380474757,
    "predict_politician_number": 2859,
    "predict_politician_precision": 0.5219340377841819,
    "predict_politician_recall": 0.5701294158796782,
    "predict_protest_f1": 0.16470588235294117,
    "predict_protest_number": 166,
    "predict_protest_precision": 0.23595505617977527,
    "predict_protest_recall": 0.12650602409638553,
    "predict_religion_f1": 0.4179829890643986,
    "predict_religion_number": 676,
    "predict_religion_precision": 0.354639175257732,
    "predict_religion_recall": 0.5088757396449705,
    "predict_restaurant_f1": 0.2857142857142857,
    "predict_restaurant_number": 232,
    "predict_restaurant_precision": 0.35947712418300654,
    "predict_restaurant_recall": 0.23706896551724138,
    "predict_road/railway/highway/transit_f1": 0.5728051391862956,
    "predict_road/railway/highway/transit_number": 1702,
    "predict_road/railway/highway/transit_precision": 0.5260570304818093,
    "predict_road/railway/highway/transit_recall": 0.6286721504112809,
    "predict_runtime": 163.0434,
    "predict_samples_per_second": 230.902,
    "predict_scholar_f1": 0.2763157894736843,
    "predict_scholar_number": 743,
    "predict_scholar_precision": 0.2702702702702703,
    "predict_scholar_recall": 0.28263795423956933,
    "predict_ship_f1": 0.4310344827586207,
    "predict_ship_number": 380,
    "predict_ship_precision": 0.4050925925925926,
    "predict_ship_recall": 0.4605263157894737,
    "predict_showorganization_f1": 0.4121424401634559,
    "predict_showorganization_number": 770,
    "predict_showorganization_precision": 0.3743372216330859,
    "predict_showorganization_recall": 0.4584415584415584,
    "predict_software_f1": 0.5146547441629409,
    "predict_software_number": 889,
    "predict_software_precision": 0.4608540925266904,
    "predict_software_recall": 0.5826771653543307,
    "predict_soldier_f1": 0.36690085870413736,
    "predict_soldier_number": 647,
    "predict_soldier_precision": 0.37066246056782337,
    "predict_soldier_recall": 0.36321483771251933,
    "predict_sportsevent_f1": 0.476217930239262,
    "predict_sportsevent_number": 1572,
    "predict_sportsevent_precision": 0.4354243542435424,
    "predict_sportsevent_recall": 0.5254452926208651,
    "predict_sportsfacility_f1": 0.566629339305711,
    "predict_sportsfacility_number": 420,
    "predict_sportsfacility_precision": 0.5348837209302325,
    "predict_sportsfacility_recall": 0.6023809523809524,
    "predict_sportsleague_f1": 0.5046382189239332,
    "predict_sportsleague_number": 885,
    "predict_sportsleague_precision": 0.42800944138473646,
    "predict_sportsleague_recall": 0.6146892655367232,
    "predict_sportsteam_f1": 0.6572008113590264,
    "predict_sportsteam_number": 2477,
    "predict_sportsteam_precision": 0.604887983706721,
    "predict_sportsteam_recall": 0.719418651594671,
    "predict_steps_per_second": 7.219,
    "predict_theater_f1": 0.5741525423728814,
    "predict_theater_number": 456,
    "predict_theater_precision": 0.555327868852459,
    "predict_theater_recall": 0.5942982456140351,
    "predict_train_f1": 0.31850789096126253,
    "predict_train_number": 314,
    "predict_train_precision": 0.2898172323759791,
    "predict_train_recall": 0.3535031847133758,
    "predict_weapon_f1": 0.42467948717948717,
    "predict_weapon_number": 625,
    "predict_weapon_precision": 0.42536115569823435,
    "predict_weapon_recall": 0.424,
    "predict_writtenart_f1": 0.40625,
    "predict_writtenart_number": 1032,
    "predict_writtenart_precision": 0.3463114754098361,
    "predict_writtenart_recall": 0.49127906976744184
}