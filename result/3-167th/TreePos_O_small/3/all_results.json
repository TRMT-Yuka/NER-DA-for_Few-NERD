{
    "predict_GPE_f1": 0.7666843347949693,
    "predict_GPE_number": 20409,
    "predict_GPE_precision": 0.7541830592027302,
    "predict_GPE_recall": 0.7796070361115194,
    "predict_actor_f1": 0.6815186695952306,
    "predict_actor_number": 1638,
    "predict_actor_precision": 0.7010974822466107,
    "predict_actor_recall": 0.663003663003663,
    "predict_airplane_f1": 0.5465253239104829,
    "predict_airplane_number": 794,
    "predict_airplane_precision": 0.5132743362831859,
    "predict_airplane_recall": 0.5843828715365239,
    "predict_airport_f1": 0.7184466019417477,
    "predict_airport_number": 364,
    "predict_airport_precision": 0.7254901960784313,
    "predict_airport_recall": 0.7115384615384616,
    "predict_artist/author_f1": 0.5950045769582843,
    "predict_artist/author_number": 3464,
    "predict_artist/author_precision": 0.5438680372938083,
    "predict_artist/author_recall": 0.6567551963048499,
    "predict_astronomything_f1": 0.6425531914893617,
    "predict_astronomything_number": 678,
    "predict_astronomything_precision": 0.6188524590163934,
    "predict_astronomything_recall": 0.668141592920354,
    "predict_athlete_f1": 0.774529569892473,
    "predict_athlete_number": 2907,
    "predict_athlete_precision": 0.7569786535303776,
    "predict_athlete_recall": 0.7929136566907464,
    "predict_attack/battle/war/militaryconflict_f1": 0.6320201173512155,
    "predict_attack/battle/war/militaryconflict_number": 1099,
    "predict_attack/battle/war/militaryconflict_precision": 0.5858585858585859,
    "predict_attack/battle/war/militaryconflict_recall": 0.6860782529572339,
    "predict_award_f1": 0.47830474268415746,
    "predict_award_number": 941,
    "predict_award_precision": 0.4553314121037464,
    "predict_award_recall": 0.5037194473963869,
    "predict_biologything_f1": 0.5473626070146368,
    "predict_biologything_number": 1875,
    "predict_biologything_precision": 0.5675830469644902,
    "predict_biologything_recall": 0.5285333333333333,
    "predict_bodiesofwater_f1": 0.6340080971659919,
    "predict_bodiesofwater_number": 1169,
    "predict_bodiesofwater_precision": 0.6018447348193697,
    "predict_bodiesofwater_recall": 0.669803250641574,
    "predict_broadcastprogram_f1": 0.4633569739952719,
    "predict_broadcastprogram_number": 605,
    "predict_broadcastprogram_precision": 0.4427710843373494,
    "predict_broadcastprogram_recall": 0.4859504132231405,
    "predict_car_f1": 0.5622119815668202,
    "predict_car_number": 688,
    "predict_car_precision": 0.5960912052117264,
    "predict_car_recall": 0.5319767441860465,
    "predict_chemicalthing_f1": 0.4752048296679603,
    "predict_chemicalthing_number": 1014,
    "predict_chemicalthing_precision": 0.4222222222222222,
    "predict_chemicalthing_recall": 0.5433925049309665,
    "predict_company_f1": 0.5475047984644913,
    "predict_company_number": 3903,
    "predict_company_precision": 0.5147755470336115,
    "predict_company_recall": 0.5846784524724571,
    "predict_currency_f1": 0.6004962779156328,
    "predict_currency_number": 799,
    "predict_currency_precision": 0.5953259532595326,
    "predict_currency_recall": 0.6057571964956195,
    "predict_director_f1": 0.5229841748304447,
    "predict_director_number": 554,
    "predict_director_precision": 0.4489003880983182,
    "predict_director_recall": 0.6263537906137184,
    "predict_disaster_f1": 0.40983606557377045,
    "predict_disaster_number": 207,
    "predict_disaster_precision": 0.35587188612099646,
    "predict_disaster_recall": 0.4830917874396135,
    "predict_disease_f1": 0.5132408575031526,
    "predict_disease_number": 750,
    "predict_disease_precision": 0.4868421052631579,
    "predict_disease_recall": 0.5426666666666666,
    "predict_education_f1": 0.6742617995814927,
    "predict_education_number": 2082,
    "predict_education_precision": 0.6534474988733664,
    "predict_education_recall": 0.696445725264169,
    "predict_educationaldegree_f1": 0.49881796690307334,
    "predict_educationaldegree_number": 367,
    "predict_educationaldegree_precision": 0.4405010438413361,
    "predict_educationaldegree_recall": 0.5749318801089919,
    "predict_election_f1": 0.21126760563380279,
    "predict_election_number": 184,
    "predict_election_precision": 0.1859504132231405,
    "predict_election_recall": 0.24456521739130435,
    "predict_film_f1": 0.6087492298213185,
    "predict_film_number": 759,
    "predict_film_precision": 0.5717592592592593,
    "predict_film_recall": 0.6508563899868248,
    "predict_food_f1": 0.4057649667405765,
    "predict_food_number": 432,
    "predict_food_precision": 0.3893617021276596,
    "predict_food_recall": 0.4236111111111111,
    "predict_game_f1": 0.5636007827788649,
    "predict_game_number": 496,
    "predict_game_precision": 0.5475285171102662,
    "predict_game_recall": 0.5806451612903226,
    "predict_god_f1": 0.48728465955701394,
    "predict_god_number": 635,
    "predict_god_precision": 0.5085616438356164,
    "predict_god_recall": 0.46771653543307085,
    "predict_government/governmentagency_f1": 0.31721798134011875,
    "predict_government/governmentagency_number": 1535,
    "predict_government/governmentagency_precision": 0.2802197802197802,
    "predict_government/governmentagency_recall": 0.36547231270358305,
    "predict_hospital_f1": 0.596938775510204,
    "predict_hospital_number": 364,
    "predict_hospital_precision": 0.5571428571428572,
    "predict_hospital_recall": 0.6428571428571429,
    "predict_hotel_f1": 0.573055028462998,
    "predict_hotel_number": 265,
    "predict_hotel_precision": 0.5763358778625954,
    "predict_hotel_recall": 0.569811320754717,
    "predict_island_f1": 0.5627563576702215,
    "predict_island_number": 646,
    "predict_island_precision": 0.5986038394415357,
    "predict_island_recall": 0.5309597523219814,
    "predict_language_f1": 0.589566310496543,
    "predict_language_number": 753,
    "predict_language_precision": 0.5596658711217184,
    "predict_language_recall": 0.6228419654714475,
    "predict_law_f1": 0.464826357969724,
    "predict_law_number": 488,
    "predict_law_precision": 0.4110236220472441,
    "predict_law_recall": 0.5348360655737705,
    "predict_library_f1": 0.6202686202686203,
    "predict_library_number": 357,
    "predict_library_precision": 0.5497835497835498,
    "predict_library_recall": 0.711484593837535,
    "predict_livingthing_f1": 0.541466024612092,
    "predict_livingthing_number": 865,
    "predict_livingthing_precision": 0.5039840637450199,
    "predict_livingthing_recall": 0.584971098265896,
    "predict_loss": 0.6491603255271912,
    "predict_media/newspaper_f1": 0.5359281437125749,
    "predict_media/newspaper_number": 1234,
    "predict_media/newspaper_precision": 0.49791376912378305,
    "predict_media/newspaper_recall": 0.580226904376013,
    "predict_medical_f1": 0.2599118942731278,
    "predict_medical_number": 397,
    "predict_medical_precision": 0.2309197651663405,
    "predict_medical_recall": 0.2972292191435768,
    "predict_mountain_f1": 0.5846383296047726,
    "predict_mountain_number": 681,
    "predict_mountain_precision": 0.593939393939394,
    "predict_mountain_recall": 0.5756240822320118,
    "predict_music_f1": 0.6359113523292628,
    "predict_music_number": 1041,
    "predict_music_precision": 0.6008547008547008,
    "predict_music_recall": 0.675312199807877,
    "predict_other_f1": 0.4593182824258521,
    "predict_other_number": 21035,
    "predict_other_precision": 0.4297370055912197,
    "predict_other_recall": 0.4932731162348467,
    "predict_overall_accuracy": 0.902493230581486,
    "predict_overall_f1": 0.5719591640218918,
    "predict_overall_precision": 0.5457835617979635,
    "predict_overall_recall": 0.6007719935598398,
    "predict_painting_f1": 0.42666666666666664,
    "predict_painting_number": 58,
    "predict_painting_precision": 0.34782608695652173,
    "predict_painting_recall": 0.5517241379310345,
    "predict_park_f1": 0.42376237623762375,
    "predict_park_number": 458,
    "predict_park_precision": 0.38768115942028986,
    "predict_park_recall": 0.4672489082969432,
    "predict_politicalparty_f1": 0.5432098765432098,
    "predict_politicalparty_number": 1057,
    "predict_politicalparty_precision": 0.5256637168141592,
    "predict_politicalparty_recall": 0.5619678334910123,
    "predict_politician_f1": 0.5156391249322003,
    "predict_politician_number": 2859,
    "predict_politician_precision": 0.5336826347305389,
    "predict_politician_recall": 0.4987757957327737,
    "predict_protest_f1": 0.18902439024390244,
    "predict_protest_number": 166,
    "predict_protest_precision": 0.19135802469135801,
    "predict_protest_recall": 0.18674698795180722,
    "predict_religion_f1": 0.3986254295532646,
    "predict_religion_number": 676,
    "predict_religion_precision": 0.3722721437740693,
    "predict_religion_recall": 0.4289940828402367,
    "predict_restaurant_f1": 0.2975206611570248,
    "predict_restaurant_number": 232,
    "predict_restaurant_precision": 0.2857142857142857,
    "predict_restaurant_recall": 0.3103448275862069,
    "predict_road/railway/highway/transit_f1": 0.5804214457188583,
    "predict_road/railway/highway/transit_number": 1702,
    "predict_road/railway/highway/transit_precision": 0.5315095261358085,
    "predict_road/railway/highway/transit_recall": 0.6392479435957696,
    "predict_runtime": 305.0337,
    "predict_samples_per_second": 123.419,
    "predict_scholar_f1": 0.3427004797806717,
    "predict_scholar_number": 743,
    "predict_scholar_precision": 0.34916201117318435,
    "predict_scholar_recall": 0.3364737550471063,
    "predict_ship_f1": 0.48443843031123146,
    "predict_ship_number": 380,
    "predict_ship_precision": 0.4986072423398329,
    "predict_ship_recall": 0.4710526315789474,
    "predict_showorganization_f1": 0.3976143141153081,
    "predict_showorganization_number": 770,
    "predict_showorganization_precision": 0.4059539918809202,
    "predict_showorganization_recall": 0.38961038961038963,
    "predict_software_f1": 0.4789382573571841,
    "predict_software_number": 889,
    "predict_software_precision": 0.49170616113744076,
    "predict_software_recall": 0.4668166479190101,
    "predict_soldier_f1": 0.36008091706001344,
    "predict_soldier_number": 647,
    "predict_soldier_precision": 0.319377990430622,
    "predict_soldier_recall": 0.4126738794435858,
    "predict_sportsevent_f1": 0.4336336336336336,
    "predict_sportsevent_number": 1572,
    "predict_sportsevent_precision": 0.41069397042093286,
    "predict_sportsevent_recall": 0.4592875318066158,
    "predict_sportsfacility_f1": 0.6020833333333333,
    "predict_sportsfacility_number": 420,
    "predict_sportsfacility_precision": 0.5351851851851852,
    "predict_sportsfacility_recall": 0.6880952380952381,
    "predict_sportsleague_f1": 0.45756823821339954,
    "predict_sportsleague_number": 885,
    "predict_sportsleague_precision": 0.4079646017699115,
    "predict_sportsleague_recall": 0.5209039548022599,
    "predict_sportsteam_f1": 0.6423803166126263,
    "predict_sportsteam_number": 2477,
    "predict_sportsteam_precision": 0.6088214027476501,
    "predict_sportsteam_recall": 0.6798546628986677,
    "predict_steps_per_second": 7.714,
    "predict_theater_f1": 0.5663716814159293,
    "predict_theater_number": 456,
    "predict_theater_precision": 0.5133689839572193,
    "predict_theater_recall": 0.631578947368421,
    "predict_train_f1": 0.3971428571428572,
    "predict_train_number": 314,
    "predict_train_precision": 0.3601036269430052,
    "predict_train_recall": 0.4426751592356688,
    "predict_weapon_f1": 0.4342948717948718,
    "predict_weapon_number": 625,
    "predict_weapon_precision": 0.434991974317817,
    "predict_weapon_recall": 0.4336,
    "predict_writtenart_f1": 0.49187527448397017,
    "predict_writtenart_number": 1032,
    "predict_writtenart_precision": 0.4497991967871486,
    "predict_writtenart_recall": 0.5426356589147286
}