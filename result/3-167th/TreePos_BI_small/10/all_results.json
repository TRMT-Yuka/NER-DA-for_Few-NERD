{
    "predict_GPE_f1": 0.7559260466632516,
    "predict_GPE_number": 20409,
    "predict_GPE_precision": 0.7202378311221547,
    "predict_GPE_recall": 0.7953353912489588,
    "predict_actor_f1": 0.6744904261890056,
    "predict_actor_number": 1638,
    "predict_actor_precision": 0.6825,
    "predict_actor_recall": 0.6666666666666666,
    "predict_airplane_f1": 0.5059190031152647,
    "predict_airplane_number": 794,
    "predict_airplane_precision": 0.5006165228113441,
    "predict_airplane_recall": 0.5113350125944585,
    "predict_airport_f1": 0.6528623629719854,
    "predict_airport_number": 364,
    "predict_airport_precision": 0.5864332603938731,
    "predict_airport_recall": 0.7362637362637363,
    "predict_artist/author_f1": 0.5778182290989468,
    "predict_artist/author_number": 3464,
    "predict_artist/author_precision": 0.5256683226874852,
    "predict_artist/author_recall": 0.6414549653579676,
    "predict_astronomything_f1": 0.6637618010167029,
    "predict_astronomything_number": 678,
    "predict_astronomything_precision": 0.65379113018598,
    "predict_astronomything_recall": 0.6740412979351033,
    "predict_athlete_f1": 0.7822805578342904,
    "predict_athlete_number": 2907,
    "predict_athlete_precision": 0.7478042659974906,
    "predict_athlete_recall": 0.8200894392844857,
    "predict_attack/battle/war/militaryconflict_f1": 0.6214315266859743,
    "predict_attack/battle/war/militaryconflict_number": 1099,
    "predict_attack/battle/war/militaryconflict_precision": 0.5698027314112292,
    "predict_attack/battle/war/militaryconflict_recall": 0.6833484986351228,
    "predict_award_f1": 0.45170193192272307,
    "predict_award_number": 941,
    "predict_award_precision": 0.3982157339821573,
    "predict_award_recall": 0.5217853347502657,
    "predict_biologything_f1": 0.5318761384335154,
    "predict_biologything_number": 1875,
    "predict_biologything_precision": 0.5193089430894309,
    "predict_biologything_recall": 0.5450666666666667,
    "predict_bodiesofwater_f1": 0.6202876020209872,
    "predict_bodiesofwater_number": 1169,
    "predict_bodiesofwater_precision": 0.5683760683760684,
    "predict_bodiesofwater_recall": 0.6826347305389222,
    "predict_broadcastprogram_f1": 0.3541341653666147,
    "predict_broadcastprogram_number": 605,
    "predict_broadcastprogram_precision": 0.3353028064992615,
    "predict_broadcastprogram_recall": 0.3752066115702479,
    "predict_car_f1": 0.5597269624573379,
    "predict_car_number": 688,
    "predict_car_precision": 0.5276705276705277,
    "predict_car_recall": 0.5959302325581395,
    "predict_chemicalthing_f1": 0.49201801064265255,
    "predict_chemicalthing_number": 1014,
    "predict_chemicalthing_precision": 0.4205738278516445,
    "predict_chemicalthing_recall": 0.5927021696252466,
    "predict_company_f1": 0.5609426782864858,
    "predict_company_number": 3903,
    "predict_company_precision": 0.5384071630537229,
    "predict_company_recall": 0.5854470919805278,
    "predict_currency_f1": 0.6828703703703702,
    "predict_currency_number": 799,
    "predict_currency_precision": 0.635091496232508,
    "predict_currency_recall": 0.7384230287859824,
    "predict_director_f1": 0.5384615384615384,
    "predict_director_number": 554,
    "predict_director_precision": 0.5336879432624113,
    "predict_director_recall": 0.5433212996389891,
    "predict_disaster_f1": 0.33820459290187893,
    "predict_disaster_number": 207,
    "predict_disaster_precision": 0.2977941176470588,
    "predict_disaster_recall": 0.391304347826087,
    "predict_disease_f1": 0.5352266521026761,
    "predict_disease_number": 750,
    "predict_disease_precision": 0.4532839962997225,
    "predict_disease_recall": 0.6533333333333333,
    "predict_education_f1": 0.6555830150527971,
    "predict_education_number": 2082,
    "predict_education_precision": 0.6158716758125792,
    "predict_education_recall": 0.7007684918347743,
    "predict_educationaldegree_f1": 0.4226254002134472,
    "predict_educationaldegree_number": 367,
    "predict_educationaldegree_precision": 0.3473684210526316,
    "predict_educationaldegree_recall": 0.5395095367847411,
    "predict_election_f1": 0.2185567010309278,
    "predict_election_number": 184,
    "predict_election_precision": 0.1760797342192691,
    "predict_election_recall": 0.28804347826086957,
    "predict_film_f1": 0.5707547169811321,
    "predict_film_number": 759,
    "predict_film_precision": 0.5165421558164355,
    "predict_film_recall": 0.6376811594202898,
    "predict_food_f1": 0.4139834406623735,
    "predict_food_number": 432,
    "predict_food_precision": 0.3435114503816794,
    "predict_food_recall": 0.5208333333333334,
    "predict_game_f1": 0.5754245754245754,
    "predict_game_number": 496,
    "predict_game_precision": 0.5702970297029702,
    "predict_game_recall": 0.5806451612903226,
    "predict_god_f1": 0.5175159235668789,
    "predict_god_number": 635,
    "predict_god_precision": 0.5233494363929146,
    "predict_god_recall": 0.5118110236220472,
    "predict_government/governmentagency_f1": 0.305365296803653,
    "predict_government/governmentagency_number": 1535,
    "predict_government/governmentagency_precision": 0.27171152869476894,
    "predict_government/governmentagency_recall": 0.3485342019543974,
    "predict_hospital_f1": 0.45964432284541723,
    "predict_hospital_number": 364,
    "predict_hospital_precision": 0.45776566757493187,
    "predict_hospital_recall": 0.46153846153846156,
    "predict_hotel_f1": 0.5130890052356022,
    "predict_hotel_number": 265,
    "predict_hotel_precision": 0.4772727272727273,
    "predict_hotel_recall": 0.5547169811320755,
    "predict_island_f1": 0.5319148936170214,
    "predict_island_number": 646,
    "predict_island_precision": 0.5642361111111112,
    "predict_island_recall": 0.5030959752321982,
    "predict_language_f1": 0.6279904306220097,
    "predict_language_number": 753,
    "predict_language_precision": 0.5712731229597389,
    "predict_language_recall": 0.6972111553784861,
    "predict_law_f1": 0.4436974789915966,
    "predict_law_number": 488,
    "predict_law_precision": 0.37606837606837606,
    "predict_law_recall": 0.5409836065573771,
    "predict_library_f1": 0.547683923705722,
    "predict_library_number": 357,
    "predict_library_precision": 0.53315649867374,
    "predict_library_recall": 0.5630252100840336,
    "predict_livingthing_f1": 0.5183218268720128,
    "predict_livingthing_number": 865,
    "predict_livingthing_precision": 0.4793713163064833,
    "predict_livingthing_recall": 0.5641618497109827,
    "predict_loss": 0.7590892910957336,
    "predict_media/newspaper_f1": 0.5228185469149325,
    "predict_media/newspaper_number": 1234,
    "predict_media/newspaper_precision": 0.4757475083056478,
    "predict_media/newspaper_recall": 0.580226904376013,
    "predict_medical_f1": 0.34807417974322397,
    "predict_medical_number": 397,
    "predict_medical_precision": 0.40131578947368424,
    "predict_medical_recall": 0.30730478589420657,
    "predict_mountain_f1": 0.5645863570391872,
    "predict_mountain_number": 681,
    "predict_mountain_precision": 0.5581061692969871,
    "predict_mountain_recall": 0.5712187958883994,
    "predict_music_f1": 0.6531924666972899,
    "predict_music_number": 1041,
    "predict_music_precision": 0.6258802816901409,
    "predict_music_recall": 0.6829971181556196,
    "predict_other_f1": 0.4573383388497679,
    "predict_other_number": 21035,
    "predict_other_precision": 0.4292385956133767,
    "predict_other_recall": 0.4893748514380794,
    "predict_overall_accuracy": 0.9015150034634234,
    "predict_overall_f1": 0.56692170732881,
    "predict_overall_precision": 0.5310770118465894,
    "predict_overall_recall": 0.6079552491433762,
    "predict_painting_f1": 0.2653061224489796,
    "predict_painting_number": 58,
    "predict_painting_precision": 0.325,
    "predict_painting_recall": 0.22413793103448276,
    "predict_park_f1": 0.41552990556138514,
    "predict_park_number": 458,
    "predict_park_precision": 0.4,
    "predict_park_recall": 0.43231441048034935,
    "predict_politicalparty_f1": 0.5706168831168831,
    "predict_politicalparty_number": 1057,
    "predict_politicalparty_precision": 0.4996446339729922,
    "predict_politicalparty_recall": 0.6650898770104068,
    "predict_politician_f1": 0.5281370737443306,
    "predict_politician_number": 2859,
    "predict_politician_precision": 0.5080801551389786,
    "predict_politician_recall": 0.5498426023084995,
    "predict_protest_f1": 0.13253012048192772,
    "predict_protest_number": 166,
    "predict_protest_precision": 0.13253012048192772,
    "predict_protest_recall": 0.13253012048192772,
    "predict_religion_f1": 0.41246684350132623,
    "predict_religion_number": 676,
    "predict_religion_precision": 0.3737980769230769,
    "predict_religion_recall": 0.46005917159763315,
    "predict_restaurant_f1": 0.2603036876355749,
    "predict_restaurant_number": 232,
    "predict_restaurant_precision": 0.26200873362445415,
    "predict_restaurant_recall": 0.25862068965517243,
    "predict_road/railway/highway/transit_f1": 0.5765488991295442,
    "predict_road/railway/highway/transit_number": 1702,
    "predict_road/railway/highway/transit_precision": 0.5108892921960072,
    "predict_road/railway/highway/transit_recall": 0.6615746180963572,
    "predict_runtime": 156.3925,
    "predict_samples_per_second": 240.721,
    "predict_scholar_f1": 0.31884057971014496,
    "predict_scholar_number": 743,
    "predict_scholar_precision": 0.3271954674220963,
    "predict_scholar_recall": 0.31090174966352624,
    "predict_ship_f1": 0.47144592952612396,
    "predict_ship_number": 380,
    "predict_ship_precision": 0.43792325056433407,
    "predict_ship_recall": 0.5105263157894737,
    "predict_showorganization_f1": 0.39783001808318263,
    "predict_showorganization_number": 770,
    "predict_showorganization_precision": 0.3712035995500562,
    "predict_showorganization_recall": 0.42857142857142855,
    "predict_software_f1": 0.4919957310565635,
    "predict_software_number": 889,
    "predict_software_precision": 0.46802030456852795,
    "predict_software_recall": 0.5185601799775028,
    "predict_soldier_f1": 0.4085576259489303,
    "predict_soldier_number": 647,
    "predict_soldier_precision": 0.3690773067331671,
    "predict_soldier_recall": 0.4574961360123648,
    "predict_sportsevent_f1": 0.4263100436681223,
    "predict_sportsevent_number": 1572,
    "predict_sportsevent_precision": 0.37332695984703634,
    "predict_sportsevent_recall": 0.49681933842239184,
    "predict_sportsfacility_f1": 0.5534188034188035,
    "predict_sportsfacility_number": 420,
    "predict_sportsfacility_precision": 0.501937984496124,
    "predict_sportsfacility_recall": 0.6166666666666667,
    "predict_sportsleague_f1": 0.42653061224489797,
    "predict_sportsleague_number": 885,
    "predict_sportsleague_precision": 0.38883720930232557,
    "predict_sportsleague_recall": 0.47231638418079097,
    "predict_sportsteam_f1": 0.6406340614730331,
    "predict_sportsteam_number": 2477,
    "predict_sportsteam_precision": 0.6146142433234422,
    "predict_sportsteam_recall": 0.6689543802987485,
    "predict_steps_per_second": 7.526,
    "predict_theater_f1": 0.5596153846153846,
    "predict_theater_number": 456,
    "predict_theater_precision": 0.4982876712328767,
    "predict_theater_recall": 0.6381578947368421,
    "predict_train_f1": 0.4134366925064599,
    "predict_train_number": 314,
    "predict_train_precision": 0.34782608695652173,
    "predict_train_recall": 0.5095541401273885,
    "predict_weapon_f1": 0.4258841234010534,
    "predict_weapon_number": 625,
    "predict_weapon_precision": 0.40198863636363635,
    "predict_weapon_recall": 0.4528,
    "predict_writtenart_f1": 0.44821583986074853,
    "predict_writtenart_number": 1032,
    "predict_writtenart_precision": 0.4067930489731438,
    "predict_writtenart_recall": 0.499031007751938
}