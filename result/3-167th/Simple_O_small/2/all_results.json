{
    "predict_GPE_f1": 0.7703061321201782,
    "predict_GPE_number": 20409,
    "predict_GPE_precision": 0.7458132599220004,
    "predict_GPE_recall": 0.7964623450438532,
    "predict_actor_f1": 0.685190489830994,
    "predict_actor_number": 1638,
    "predict_actor_precision": 0.6454398273070696,
    "predict_actor_recall": 0.7301587301587301,
    "predict_airplane_f1": 0.5313653136531366,
    "predict_airplane_number": 794,
    "predict_airplane_precision": 0.5192307692307693,
    "predict_airplane_recall": 0.5440806045340051,
    "predict_airport_f1": 0.6865315852205006,
    "predict_airport_number": 364,
    "predict_airport_precision": 0.6063157894736843,
    "predict_airport_recall": 0.7912087912087912,
    "predict_artist/author_f1": 0.6070504968014157,
    "predict_artist/author_number": 3464,
    "predict_artist/author_precision": 0.5742982230234355,
    "predict_artist/author_recall": 0.6437644341801386,
    "predict_astronomything_f1": 0.6334089191232048,
    "predict_astronomything_number": 678,
    "predict_astronomything_precision": 0.6496124031007752,
    "predict_astronomything_recall": 0.6179941002949852,
    "predict_athlete_f1": 0.7806035944387928,
    "predict_athlete_number": 2907,
    "predict_athlete_precision": 0.7696422601136743,
    "predict_athlete_recall": 0.7918816649466804,
    "predict_attack/battle/war/militaryconflict_f1": 0.6381718154887854,
    "predict_attack/battle/war/militaryconflict_number": 1099,
    "predict_attack/battle/war/militaryconflict_precision": 0.5965189873417721,
    "predict_attack/battle/war/militaryconflict_recall": 0.6860782529572339,
    "predict_award_f1": 0.46710816777041947,
    "predict_award_number": 941,
    "predict_award_precision": 0.3995468277945619,
    "predict_award_recall": 0.5621679064824655,
    "predict_biologything_f1": 0.5299635226680562,
    "predict_biologything_number": 1875,
    "predict_biologything_precision": 0.5180845644421803,
    "predict_biologything_recall": 0.5424,
    "predict_bodiesofwater_f1": 0.6404494382022473,
    "predict_bodiesofwater_number": 1169,
    "predict_bodiesofwater_precision": 0.6031746031746031,
    "predict_bodiesofwater_recall": 0.6826347305389222,
    "predict_broadcastprogram_f1": 0.4065180102915952,
    "predict_broadcastprogram_number": 605,
    "predict_broadcastprogram_precision": 0.42245989304812837,
    "predict_broadcastprogram_recall": 0.39173553719008264,
    "predict_car_f1": 0.5696969696969697,
    "predict_car_number": 688,
    "predict_car_precision": 0.5307402760351317,
    "predict_car_recall": 0.6148255813953488,
    "predict_chemicalthing_f1": 0.4820982514571191,
    "predict_chemicalthing_number": 1014,
    "predict_chemicalthing_precision": 0.41714697406340057,
    "predict_chemicalthing_recall": 0.5710059171597633,
    "predict_company_f1": 0.5714285714285715,
    "predict_company_number": 3903,
    "predict_company_precision": 0.5372124492557511,
    "predict_company_recall": 0.6102997694081476,
    "predict_currency_f1": 0.6585662211421628,
    "predict_currency_number": 799,
    "predict_currency_precision": 0.6399055489964581,
    "predict_currency_recall": 0.6783479349186483,
    "predict_director_f1": 0.5994876174210076,
    "predict_director_number": 554,
    "predict_director_precision": 0.5688816855753647,
    "predict_director_recall": 0.6335740072202166,
    "predict_disaster_f1": 0.3801295896328294,
    "predict_disaster_number": 207,
    "predict_disaster_precision": 0.34375,
    "predict_disaster_recall": 0.4251207729468599,
    "predict_disease_f1": 0.5189807592303692,
    "predict_disease_number": 750,
    "predict_disease_precision": 0.4254049445865303,
    "predict_disease_recall": 0.6653333333333333,
    "predict_education_f1": 0.6967441860465117,
    "predict_education_number": 2082,
    "predict_education_precision": 0.6753832281334535,
    "predict_education_recall": 0.7195004803073968,
    "predict_educationaldegree_f1": 0.4225663716814159,
    "predict_educationaldegree_number": 367,
    "predict_educationaldegree_precision": 0.35567970204841715,
    "predict_educationaldegree_recall": 0.5204359673024523,
    "predict_election_f1": 0.18773946360153257,
    "predict_election_number": 184,
    "predict_election_precision": 0.14497041420118342,
    "predict_election_recall": 0.266304347826087,
    "predict_film_f1": 0.5820543093270366,
    "predict_film_number": 759,
    "predict_film_precision": 0.5272727272727272,
    "predict_film_recall": 0.6495388669301713,
    "predict_food_f1": 0.43179587831207067,
    "predict_food_number": 432,
    "predict_food_precision": 0.3747870528109029,
    "predict_food_recall": 0.5092592592592593,
    "predict_game_f1": 0.5985552115583075,
    "predict_game_number": 496,
    "predict_game_precision": 0.6131078224101479,
    "predict_game_recall": 0.5846774193548387,
    "predict_god_f1": 0.5524956970740104,
    "predict_god_number": 635,
    "predict_god_precision": 0.6091081593927894,
    "predict_god_recall": 0.5055118110236221,
    "predict_government/governmentagency_f1": 0.3001503759398496,
    "predict_government/governmentagency_number": 1535,
    "predict_government/governmentagency_precision": 0.2787709497206704,
    "predict_government/governmentagency_recall": 0.3250814332247557,
    "predict_hospital_f1": 0.5962264150943397,
    "predict_hospital_number": 364,
    "predict_hospital_precision": 0.5498839907192575,
    "predict_hospital_recall": 0.6510989010989011,
    "predict_hotel_f1": 0.542314335060449,
    "predict_hotel_number": 265,
    "predict_hotel_precision": 0.5,
    "predict_hotel_recall": 0.5924528301886792,
    "predict_island_f1": 0.5515548281505729,
    "predict_island_number": 646,
    "predict_island_precision": 0.5850694444444444,
    "predict_island_recall": 0.521671826625387,
    "predict_language_f1": 0.6379624014554275,
    "predict_language_number": 753,
    "predict_language_precision": 0.5870535714285714,
    "predict_language_recall": 0.6985391766268261,
    "predict_law_f1": 0.4607762180016516,
    "predict_law_number": 488,
    "predict_law_precision": 0.38589211618257263,
    "predict_law_recall": 0.5717213114754098,
    "predict_library_f1": 0.6455026455026455,
    "predict_library_number": 357,
    "predict_library_precision": 0.6115288220551378,
    "predict_library_recall": 0.6834733893557423,
    "predict_livingthing_f1": 0.53125,
    "predict_livingthing_number": 865,
    "predict_livingthing_precision": 0.4709562109025916,
    "predict_livingthing_recall": 0.6092485549132948,
    "predict_loss": 0.4414011240005493,
    "predict_media/newspaper_f1": 0.5346385542168675,
    "predict_media/newspaper_number": 1234,
    "predict_media/newspaper_precision": 0.49929676511954996,
    "predict_media/newspaper_recall": 0.5753646677471637,
    "predict_medical_f1": 0.36965517241379303,
    "predict_medical_number": 397,
    "predict_medical_precision": 0.40853658536585363,
    "predict_medical_recall": 0.33753148614609574,
    "predict_mountain_f1": 0.5856929955290611,
    "predict_mountain_number": 681,
    "predict_mountain_precision": 0.594553706505295,
    "predict_mountain_recall": 0.5770925110132159,
    "predict_music_f1": 0.6541244573082489,
    "predict_music_number": 1041,
    "predict_music_precision": 0.6569767441860465,
    "predict_music_recall": 0.6512968299711815,
    "predict_other_f1": 0.47044539874541896,
    "predict_other_number": 21035,
    "predict_other_precision": 0.446292978414811,
    "predict_other_recall": 0.49736154028999285,
    "predict_overall_accuracy": 0.9049512840723779,
    "predict_overall_f1": 0.581691603410703,
    "predict_overall_precision": 0.5512302844946271,
    "predict_overall_recall": 0.6157164678198407,
    "predict_painting_f1": 0.11594202898550723,
    "predict_painting_number": 58,
    "predict_painting_precision": 0.36363636363636365,
    "predict_painting_recall": 0.06896551724137931,
    "predict_park_f1": 0.45614035087719296,
    "predict_park_number": 458,
    "predict_park_precision": 0.4324853228962818,
    "predict_park_recall": 0.48253275109170307,
    "predict_politicalparty_f1": 0.5735042735042736,
    "predict_politicalparty_number": 1057,
    "predict_politicalparty_precision": 0.5229929851909587,
    "predict_politicalparty_recall": 0.6348155156102177,
    "predict_politician_f1": 0.5513567504577992,
    "predict_politician_number": 2859,
    "predict_politician_precision": 0.5260482846251588,
    "predict_politician_recall": 0.5792235047219307,
    "predict_protest_f1": 0.17006802721088438,
    "predict_protest_number": 166,
    "predict_protest_precision": 0.1953125,
    "predict_protest_recall": 0.15060240963855423,
    "predict_religion_f1": 0.416988416988417,
    "predict_religion_number": 676,
    "predict_religion_precision": 0.3690205011389522,
    "predict_religion_recall": 0.47928994082840237,
    "predict_restaurant_f1": 0.2321899736147757,
    "predict_restaurant_number": 232,
    "predict_restaurant_precision": 0.29931972789115646,
    "predict_restaurant_recall": 0.1896551724137931,
    "predict_road/railway/highway/transit_f1": 0.5868886576482831,
    "predict_road/railway/highway/transit_number": 1702,
    "predict_road/railway/highway/transit_precision": 0.5266106442577031,
    "predict_road/railway/highway/transit_recall": 0.6627497062279671,
    "predict_runtime": 152.3781,
    "predict_samples_per_second": 247.063,
    "predict_scholar_f1": 0.30536451169188444,
    "predict_scholar_number": 743,
    "predict_scholar_precision": 0.31223628691983124,
    "predict_scholar_recall": 0.29878869448183043,
    "predict_ship_f1": 0.5146666666666667,
    "predict_ship_number": 380,
    "predict_ship_precision": 0.5216216216216216,
    "predict_ship_recall": 0.5078947368421053,
    "predict_showorganization_f1": 0.4373401534526854,
    "predict_showorganization_number": 770,
    "predict_showorganization_precision": 0.43073047858942065,
    "predict_showorganization_recall": 0.44415584415584414,
    "predict_software_f1": 0.5079872204472844,
    "predict_software_number": 889,
    "predict_software_precision": 0.48230535894843274,
    "predict_software_recall": 0.5365579302587177,
    "predict_soldier_f1": 0.421129860601614,
    "predict_soldier_number": 647,
    "predict_soldier_precision": 0.40083798882681565,
    "predict_soldier_recall": 0.4435857805255023,
    "predict_sportsevent_f1": 0.4244300591049817,
    "predict_sportsevent_number": 1572,
    "predict_sportsevent_precision": 0.3806158505805149,
    "predict_sportsevent_recall": 0.4796437659033079,
    "predict_sportsfacility_f1": 0.5728952772073923,
    "predict_sportsfacility_number": 420,
    "predict_sportsfacility_precision": 0.5036101083032491,
    "predict_sportsfacility_recall": 0.6642857142857143,
    "predict_sportsleague_f1": 0.47719298245614034,
    "predict_sportsleague_number": 885,
    "predict_sportsleague_precision": 0.42882882882882883,
    "predict_sportsleague_recall": 0.5378531073446328,
    "predict_sportsteam_f1": 0.6517874030641196,
    "predict_sportsteam_number": 2477,
    "predict_sportsteam_precision": 0.6131672597864769,
    "predict_sportsteam_recall": 0.6955995155429956,
    "predict_steps_per_second": 7.724,
    "predict_theater_f1": 0.626865671641791,
    "predict_theater_number": 456,
    "predict_theater_precision": 0.5737704918032787,
    "predict_theater_recall": 0.6907894736842105,
    "predict_train_f1": 0.4267782426778243,
    "predict_train_number": 314,
    "predict_train_precision": 0.37965260545905705,
    "predict_train_recall": 0.4872611464968153,
    "predict_weapon_f1": 0.4469096671949287,
    "predict_weapon_number": 625,
    "predict_weapon_precision": 0.4427001569858713,
    "predict_weapon_recall": 0.4512,
    "predict_writtenart_f1": 0.46528403967538323,
    "predict_writtenart_number": 1032,
    "predict_writtenart_precision": 0.4350758853288364,
    "predict_writtenart_recall": 0.5
}