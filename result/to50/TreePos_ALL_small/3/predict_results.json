{
    "predict_GPE_f1": 0.7347232207045291,
    "predict_GPE_number": 20409,
    "predict_GPE_precision": 0.7190094273251724,
    "predict_GPE_recall": 0.751139203292665,
    "predict_actor_f1": 0.6555082166768107,
    "predict_actor_number": 1638,
    "predict_actor_precision": 0.6535194174757282,
    "predict_actor_recall": 0.6575091575091575,
    "predict_airplane_f1": 0.4710894704808278,
    "predict_airplane_number": 794,
    "predict_airplane_precision": 0.4558303886925795,
    "predict_airplane_recall": 0.48740554156171284,
    "predict_airport_f1": 0.5338441890166028,
    "predict_airport_number": 364,
    "predict_airport_precision": 0.4988066825775656,
    "predict_airport_recall": 0.5741758241758241,
    "predict_artist/author_f1": 0.4961728950923008,
    "predict_artist/author_number": 3464,
    "predict_artist/author_precision": 0.5167239762425758,
    "predict_artist/author_recall": 0.47719399538106233,
    "predict_astronomything_f1": 0.5948509485094852,
    "predict_astronomything_number": 678,
    "predict_astronomything_precision": 0.550125313283208,
    "predict_astronomything_recall": 0.6474926253687315,
    "predict_athlete_f1": 0.7434210526315789,
    "predict_athlete_number": 2907,
    "predict_athlete_precision": 0.7122596911440278,
    "predict_athlete_recall": 0.7774337805297558,
    "predict_attack/battle/war/militaryconflict_f1": 0.5970394736842105,
    "predict_attack/battle/war/militaryconflict_number": 1099,
    "predict_attack/battle/war/militaryconflict_precision": 0.5446361590397599,
    "predict_attack/battle/war/militaryconflict_recall": 0.6606005459508644,
    "predict_award_f1": 0.47778874629812434,
    "predict_award_number": 941,
    "predict_award_precision": 0.44608294930875575,
    "predict_award_recall": 0.5143464399574921,
    "predict_biologything_f1": 0.5306994127068874,
    "predict_biologything_number": 1875,
    "predict_biologything_precision": 0.5312667022982362,
    "predict_biologything_recall": 0.5301333333333333,
    "predict_bodiesofwater_f1": 0.5853061224489795,
    "predict_bodiesofwater_number": 1169,
    "predict_bodiesofwater_precision": 0.5597189695550351,
    "predict_bodiesofwater_recall": 0.613344739093242,
    "predict_broadcastprogram_f1": 0.4410828025477707,
    "predict_broadcastprogram_number": 605,
    "predict_broadcastprogram_precision": 0.4254992319508449,
    "predict_broadcastprogram_recall": 0.4578512396694215,
    "predict_car_f1": 0.5121638924455827,
    "predict_car_number": 688,
    "predict_car_precision": 0.4576659038901602,
    "predict_car_recall": 0.5813953488372093,
    "predict_chemicalthing_f1": 0.4385120350109409,
    "predict_chemicalthing_number": 1014,
    "predict_chemicalthing_precision": 0.3941778127458694,
    "predict_chemicalthing_recall": 0.4940828402366864,
    "predict_company_f1": 0.5283018867924528,
    "predict_company_number": 3903,
    "predict_company_precision": 0.5032467532467533,
    "predict_company_recall": 0.5559825775044838,
    "predict_currency_f1": 0.66289592760181,
    "predict_currency_number": 799,
    "predict_currency_precision": 0.6047471620227038,
    "predict_currency_recall": 0.7334167709637046,
    "predict_director_f1": 0.4240177909562639,
    "predict_director_number": 554,
    "predict_director_precision": 0.35974842767295595,
    "predict_director_recall": 0.516245487364621,
    "predict_disaster_f1": 0.39316239316239315,
    "predict_disaster_number": 207,
    "predict_disaster_precision": 0.3524904214559387,
    "predict_disaster_recall": 0.4444444444444444,
    "predict_disease_f1": 0.5346889952153111,
    "predict_disease_number": 750,
    "predict_disease_precision": 0.4848156182212581,
    "predict_disease_recall": 0.596,
    "predict_education_f1": 0.6179072276159654,
    "predict_education_number": 2082,
    "predict_education_precision": 0.5609087348217783,
    "predict_education_recall": 0.6878001921229587,
    "predict_educationaldegree_f1": 0.41251448435689453,
    "predict_educationaldegree_number": 367,
    "predict_educationaldegree_precision": 0.3588709677419355,
    "predict_educationaldegree_recall": 0.48501362397820164,
    "predict_election_f1": 0.12037037037037038,
    "predict_election_number": 184,
    "predict_election_precision": 0.10483870967741936,
    "predict_election_recall": 0.14130434782608695,
    "predict_film_f1": 0.5777518928363424,
    "predict_film_number": 759,
    "predict_film_precision": 0.5177453027139874,
    "predict_film_recall": 0.6534914361001317,
    "predict_food_f1": 0.3529411764705882,
    "predict_food_number": 432,
    "predict_food_precision": 0.30998248686514884,
    "predict_food_recall": 0.4097222222222222,
    "predict_game_f1": 0.5258799171842651,
    "predict_game_number": 496,
    "predict_game_precision": 0.5404255319148936,
    "predict_game_recall": 0.5120967741935484,
    "predict_god_f1": 0.492269635126778,
    "predict_god_number": 635,
    "predict_god_precision": 0.40529531568228105,
    "predict_god_recall": 0.6267716535433071,
    "predict_government/governmentagency_f1": 0.2960545832097301,
    "predict_government/governmentagency_number": 1535,
    "predict_government/governmentagency_precision": 0.2717864923747277,
    "predict_government/governmentagency_recall": 0.3250814332247557,
    "predict_hospital_f1": 0.4895104895104895,
    "predict_hospital_number": 364,
    "predict_hospital_precision": 0.4985754985754986,
    "predict_hospital_recall": 0.4807692307692308,
    "predict_hotel_f1": 0.5025996533795495,
    "predict_hotel_number": 265,
    "predict_hotel_precision": 0.46474358974358976,
    "predict_hotel_recall": 0.5471698113207547,
    "predict_island_f1": 0.5215384615384615,
    "predict_island_number": 646,
    "predict_island_precision": 0.518348623853211,
    "predict_island_recall": 0.5247678018575851,
    "predict_language_f1": 0.604978354978355,
    "predict_language_number": 753,
    "predict_language_precision": 0.5105022831050229,
    "predict_language_recall": 0.7423638778220452,
    "predict_law_f1": 0.4381491973559962,
    "predict_law_number": 488,
    "predict_law_precision": 0.4063047285464098,
    "predict_law_recall": 0.47540983606557374,
    "predict_library_f1": 0.5579710144927537,
    "predict_library_number": 357,
    "predict_library_precision": 0.49044585987261147,
    "predict_library_recall": 0.6470588235294118,
    "predict_livingthing_f1": 0.5213890286864621,
    "predict_livingthing_number": 865,
    "predict_livingthing_precision": 0.46167557932263814,
    "predict_livingthing_recall": 0.5988439306358382,
    "predict_loss": 0.6721433401107788,
    "predict_media/newspaper_f1": 0.4928,
    "predict_media/newspaper_number": 1234,
    "predict_media/newspaper_precision": 0.48657187993680884,
    "predict_media/newspaper_recall": 0.4991896272285251,
    "predict_medical_f1": 0.3401015228426396,
    "predict_medical_number": 397,
    "predict_medical_precision": 0.34271099744245526,
    "predict_medical_recall": 0.33753148614609574,
    "predict_mountain_f1": 0.5059399021663172,
    "predict_mountain_number": 681,
    "predict_mountain_precision": 0.4826666666666667,
    "predict_mountain_recall": 0.5315712187958884,
    "predict_music_f1": 0.5966386554621849,
    "predict_music_number": 1041,
    "predict_music_precision": 0.5803814713896458,
    "predict_music_recall": 0.6138328530259366,
    "predict_other_f1": 0.42074189837424847,
    "predict_other_number": 21035,
    "predict_other_precision": 0.3871225435373063,
    "predict_other_recall": 0.4607558830520561,
    "predict_overall_accuracy": 0.8951755271677883,
    "predict_overall_f1": 0.5377158363308916,
    "predict_overall_precision": 0.5076116544006453,
    "predict_overall_recall": 0.571615819675515,
    "predict_painting_f1": 0.14207650273224043,
    "predict_painting_number": 58,
    "predict_painting_precision": 0.104,
    "predict_painting_recall": 0.22413793103448276,
    "predict_park_f1": 0.3831967213114755,
    "predict_park_number": 458,
    "predict_park_precision": 0.361003861003861,
    "predict_park_recall": 0.40829694323144106,
    "predict_politicalparty_f1": 0.5140905209222887,
    "predict_politicalparty_number": 1057,
    "predict_politicalparty_precision": 0.46848249027237354,
    "predict_politicalparty_recall": 0.5695364238410596,
    "predict_politician_f1": 0.4889573703133025,
    "predict_politician_number": 2859,
    "predict_politician_precision": 0.4788732394366197,
    "predict_politician_recall": 0.4994753410283316,
    "predict_protest_f1": 0.16428571428571426,
    "predict_protest_number": 166,
    "predict_protest_precision": 0.20175438596491227,
    "predict_protest_recall": 0.13855421686746988,
    "predict_religion_f1": 0.418468731735827,
    "predict_religion_number": 676,
    "predict_religion_precision": 0.3458937198067633,
    "predict_religion_recall": 0.5295857988165681,
    "predict_restaurant_f1": 0.23720930232558138,
    "predict_restaurant_number": 232,
    "predict_restaurant_precision": 0.25757575757575757,
    "predict_restaurant_recall": 0.21982758620689655,
    "predict_road/railway/highway/transit_f1": 0.5306559571619812,
    "predict_road/railway/highway/transit_number": 1702,
    "predict_road/railway/highway/transit_precision": 0.48745696015740286,
    "predict_road/railway/highway/transit_recall": 0.582256169212691,
    "predict_runtime": 260.0766,
    "predict_samples_per_second": 144.753,
    "predict_scholar_f1": 0.2593068035943517,
    "predict_scholar_number": 743,
    "predict_scholar_precision": 0.24785276073619633,
    "predict_scholar_recall": 0.2718707940780619,
    "predict_ship_f1": 0.48010269576379977,
    "predict_ship_number": 380,
    "predict_ship_precision": 0.46867167919799496,
    "predict_ship_recall": 0.4921052631578947,
    "predict_showorganization_f1": 0.3865435356200528,
    "predict_showorganization_number": 770,
    "predict_showorganization_precision": 0.39276139410187666,
    "predict_showorganization_recall": 0.38051948051948054,
    "predict_software_f1": 0.5011086474501107,
    "predict_software_number": 889,
    "predict_software_precision": 0.49398907103825135,
    "predict_software_recall": 0.5084364454443194,
    "predict_soldier_f1": 0.3464684014869889,
    "predict_soldier_number": 647,
    "predict_soldier_precision": 0.333810888252149,
    "predict_soldier_recall": 0.3601236476043277,
    "predict_sportsevent_f1": 0.42390011890606427,
    "predict_sportsevent_number": 1572,
    "predict_sportsevent_precision": 0.3978794642857143,
    "predict_sportsevent_recall": 0.4535623409669211,
    "predict_sportsfacility_f1": 0.5469223007063573,
    "predict_sportsfacility_number": 420,
    "predict_sportsfacility_precision": 0.4746059544658494,
    "predict_sportsfacility_recall": 0.6452380952380953,
    "predict_sportsleague_f1": 0.4574321214910262,
    "predict_sportsleague_number": 885,
    "predict_sportsleague_precision": 0.3858695652173913,
    "predict_sportsleague_recall": 0.5615819209039548,
    "predict_sportsteam_f1": 0.6380126182965299,
    "predict_sportsteam_number": 2477,
    "predict_sportsteam_precision": 0.6235067437379576,
    "predict_sportsteam_recall": 0.6532095276544206,
    "predict_steps_per_second": 4.526,
    "predict_theater_f1": 0.5532831001076426,
    "predict_theater_number": 456,
    "predict_theater_precision": 0.5433403805496829,
    "predict_theater_recall": 0.5635964912280702,
    "predict_train_f1": 0.321483771251932,
    "predict_train_number": 314,
    "predict_train_precision": 0.3123123123123123,
    "predict_train_recall": 0.33121019108280253,
    "predict_weapon_f1": 0.3964968152866242,
    "predict_weapon_number": 625,
    "predict_weapon_precision": 0.39461172741679873,
    "predict_weapon_recall": 0.3984,
    "predict_writtenart_f1": 0.44568484328037783,
    "predict_writtenart_number": 1032,
    "predict_writtenart_precision": 0.40015420200462604,
    "predict_writtenart_recall": 0.502906976744186
}