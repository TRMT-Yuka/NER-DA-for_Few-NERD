{
    "predict_GPE_f1": 0.7373577159917897,
    "predict_GPE_number": 20409,
    "predict_GPE_precision": 0.7036459956372701,
    "predict_GPE_recall": 0.774462247047871,
    "predict_actor_f1": 0.5747707872273159,
    "predict_actor_number": 1638,
    "predict_actor_precision": 0.5960655737704919,
    "predict_actor_recall": 0.554945054945055,
    "predict_airplane_f1": 0.49176172370088717,
    "predict_airplane_number": 794,
    "predict_airplane_precision": 0.49489795918367346,
    "predict_airplane_recall": 0.48866498740554154,
    "predict_airport_f1": 0.6236297198538368,
    "predict_airport_number": 364,
    "predict_airport_precision": 0.5601750547045952,
    "predict_airport_recall": 0.7032967032967034,
    "predict_artist/author_f1": 0.5204831932773109,
    "predict_artist/author_number": 3464,
    "predict_artist/author_precision": 0.4773603082851638,
    "predict_artist/author_recall": 0.5721709006928406,
    "predict_astronomything_f1": 0.5977337110481588,
    "predict_astronomything_number": 678,
    "predict_astronomything_precision": 0.5749318801089919,
    "predict_astronomything_recall": 0.6224188790560472,
    "predict_athlete_f1": 0.72448,
    "predict_athlete_number": 2907,
    "predict_athlete_precision": 0.6772360155548908,
    "predict_athlete_recall": 0.7788097695218438,
    "predict_attack/battle/war/militaryconflict_f1": 0.6057395143487859,
    "predict_attack/battle/war/militaryconflict_number": 1099,
    "predict_attack/battle/war/militaryconflict_precision": 0.5883361921097771,
    "predict_attack/battle/war/militaryconflict_recall": 0.6242038216560509,
    "predict_award_f1": 0.40059790732436473,
    "predict_award_number": 941,
    "predict_award_precision": 0.3771106941838649,
    "predict_award_recall": 0.42720510095642933,
    "predict_biologything_f1": 0.501114137162664,
    "predict_biologything_number": 1875,
    "predict_biologything_precision": 0.4676524953789279,
    "predict_biologything_recall": 0.5397333333333333,
    "predict_bodiesofwater_f1": 0.5933503836317136,
    "predict_bodiesofwater_number": 1169,
    "predict_bodiesofwater_precision": 0.5913338997451147,
    "predict_bodiesofwater_recall": 0.5953806672369547,
    "predict_broadcastprogram_f1": 0.3472962680883473,
    "predict_broadcastprogram_number": 605,
    "predict_broadcastprogram_precision": 0.3220338983050847,
    "predict_broadcastprogram_recall": 0.3768595041322314,
    "predict_car_f1": 0.5195195195195194,
    "predict_car_number": 688,
    "predict_car_precision": 0.5372670807453416,
    "predict_car_recall": 0.502906976744186,
    "predict_chemicalthing_f1": 0.48149760557248583,
    "predict_chemicalthing_number": 1014,
    "predict_chemicalthing_precision": 0.4310210444271239,
    "predict_chemicalthing_recall": 0.5453648915187377,
    "predict_company_f1": 0.5186813186813187,
    "predict_company_number": 3903,
    "predict_company_precision": 0.5234864300626305,
    "predict_company_recall": 0.5139636177299514,
    "predict_currency_f1": 0.6335403726708075,
    "predict_currency_number": 799,
    "predict_currency_precision": 0.5771604938271605,
    "predict_currency_recall": 0.7021276595744681,
    "predict_director_f1": 0.5052083333333335,
    "predict_director_number": 554,
    "predict_director_precision": 0.4866220735785953,
    "predict_director_recall": 0.5252707581227437,
    "predict_disaster_f1": 0.35820895522388063,
    "predict_disaster_number": 207,
    "predict_disaster_precision": 0.36923076923076925,
    "predict_disaster_recall": 0.34782608695652173,
    "predict_disease_f1": 0.5081472540736272,
    "predict_disease_number": 750,
    "predict_disease_precision": 0.46416758544652703,
    "predict_disease_recall": 0.5613333333333334,
    "predict_education_f1": 0.6229228802726885,
    "predict_education_number": 2082,
    "predict_education_precision": 0.5597243491577335,
    "predict_education_recall": 0.7022094140249759,
    "predict_educationaldegree_f1": 0.4436701509872241,
    "predict_educationaldegree_number": 367,
    "predict_educationaldegree_precision": 0.3866396761133603,
    "predict_educationaldegree_recall": 0.5204359673024523,
    "predict_election_f1": 0.13186813186813187,
    "predict_election_number": 184,
    "predict_election_precision": 0.11070110701107011,
    "predict_election_recall": 0.16304347826086957,
    "predict_film_f1": 0.5075318655851679,
    "predict_film_number": 759,
    "predict_film_precision": 0.452947259565667,
    "predict_film_recall": 0.5770750988142292,
    "predict_food_f1": 0.3879472693032015,
    "predict_food_number": 432,
    "predict_food_precision": 0.326984126984127,
    "predict_food_recall": 0.47685185185185186,
    "predict_game_f1": 0.46292585170340683,
    "predict_game_number": 496,
    "predict_game_precision": 0.4601593625498008,
    "predict_game_recall": 0.4657258064516129,
    "predict_god_f1": 0.4547677261613692,
    "predict_god_number": 635,
    "predict_god_precision": 0.47128378378378377,
    "predict_god_recall": 0.4393700787401575,
    "predict_government/governmentagency_f1": 0.300187617260788,
    "predict_government/governmentagency_number": 1535,
    "predict_government/governmentagency_precision": 0.28863499699338546,
    "predict_government/governmentagency_recall": 0.3127035830618892,
    "predict_hospital_f1": 0.5703225806451613,
    "predict_hospital_number": 364,
    "predict_hospital_precision": 0.537712895377129,
    "predict_hospital_recall": 0.6071428571428571,
    "predict_hotel_f1": 0.45075757575757575,
    "predict_hotel_number": 265,
    "predict_hotel_precision": 0.4524714828897338,
    "predict_hotel_recall": 0.4490566037735849,
    "predict_island_f1": 0.4918541505042669,
    "predict_island_number": 646,
    "predict_island_precision": 0.49300155520995337,
    "predict_island_recall": 0.4907120743034056,
    "predict_language_f1": 0.5898030127462339,
    "predict_language_number": 753,
    "predict_language_precision": 0.5231243576567317,
    "predict_language_recall": 0.6759628154050464,
    "predict_law_f1": 0.39577836411609496,
    "predict_law_number": 488,
    "predict_law_precision": 0.34668721109399075,
    "predict_law_recall": 0.4610655737704918,
    "predict_library_f1": 0.5216316440049443,
    "predict_library_number": 357,
    "predict_library_precision": 0.4668141592920354,
    "predict_library_recall": 0.5910364145658263,
    "predict_livingthing_f1": 0.4537281861448969,
    "predict_livingthing_number": 865,
    "predict_livingthing_precision": 0.41812865497076024,
    "predict_livingthing_recall": 0.4959537572254335,
    "predict_loss": 0.6701778769493103,
    "predict_media/newspaper_f1": 0.4894283476898982,
    "predict_media/newspaper_number": 1234,
    "predict_media/newspaper_precision": 0.4734848484848485,
    "predict_media/newspaper_recall": 0.506482982171799,
    "predict_medical_f1": 0.33602150537634407,
    "predict_medical_number": 397,
    "predict_medical_precision": 0.36023054755043227,
    "predict_medical_recall": 0.3148614609571788,
    "predict_mountain_f1": 0.4519621109607578,
    "predict_mountain_number": 681,
    "predict_mountain_precision": 0.4190715181932246,
    "predict_mountain_recall": 0.49045521292217326,
    "predict_music_f1": 0.5562847046945988,
    "predict_music_number": 1041,
    "predict_music_precision": 0.5861702127659575,
    "predict_music_recall": 0.5292987512007685,
    "predict_other_f1": 0.41509351224539015,
    "predict_other_number": 21035,
    "predict_other_precision": 0.3852038740131847,
    "predict_other_recall": 0.4500118849536487,
    "predict_overall_accuracy": 0.8949963845768001,
    "predict_overall_f1": 0.5343057334730794,
    "predict_overall_precision": 0.5042034506758489,
    "predict_overall_recall": 0.5682306072740784,
    "predict_painting_f1": 0.3366336633663366,
    "predict_painting_number": 58,
    "predict_painting_precision": 0.3953488372093023,
    "predict_painting_recall": 0.29310344827586204,
    "predict_park_f1": 0.3761194029850747,
    "predict_park_number": 458,
    "predict_park_precision": 0.34552102376599636,
    "predict_park_recall": 0.4126637554585153,
    "predict_politicalparty_f1": 0.5420240137221269,
    "predict_politicalparty_number": 1057,
    "predict_politicalparty_precision": 0.4956862745098039,
    "predict_politicalparty_recall": 0.597918637653737,
    "predict_politician_f1": 0.48663393344244404,
    "predict_politician_number": 2859,
    "predict_politician_precision": 0.5068181818181818,
    "predict_politician_recall": 0.46799580272822666,
    "predict_protest_f1": 0.22573363431151242,
    "predict_protest_number": 166,
    "predict_protest_precision": 0.18050541516245489,
    "predict_protest_recall": 0.30120481927710846,
    "predict_religion_f1": 0.45141822570911294,
    "predict_religion_number": 676,
    "predict_religion_precision": 0.381243628950051,
    "predict_religion_recall": 0.5532544378698225,
    "predict_restaurant_f1": 0.2378640776699029,
    "predict_restaurant_number": 232,
    "predict_restaurant_precision": 0.2722222222222222,
    "predict_restaurant_recall": 0.21120689655172414,
    "predict_road/railway/highway/transit_f1": 0.5508607198748044,
    "predict_road/railway/highway/transit_number": 1702,
    "predict_road/railway/highway/transit_precision": 0.49530956848030017,
    "predict_road/railway/highway/transit_recall": 0.6204465334900118,
    "predict_runtime": 275.0614,
    "predict_samples_per_second": 136.868,
    "predict_scholar_f1": 0.2703832752613241,
    "predict_scholar_number": 743,
    "predict_scholar_precision": 0.28034682080924855,
    "predict_scholar_recall": 0.2611036339165545,
    "predict_ship_f1": 0.45745856353591163,
    "predict_ship_number": 380,
    "predict_ship_precision": 0.3942857142857143,
    "predict_ship_recall": 0.5447368421052632,
    "predict_showorganization_f1": 0.3855737704918033,
    "predict_showorganization_number": 770,
    "predict_showorganization_precision": 0.3894039735099338,
    "predict_showorganization_recall": 0.38181818181818183,
    "predict_software_f1": 0.42201834862385323,
    "predict_software_number": 889,
    "predict_software_precision": 0.4304093567251462,
    "predict_software_recall": 0.41394825646794153,
    "predict_soldier_f1": 0.37276478679504815,
    "predict_soldier_number": 647,
    "predict_soldier_precision": 0.3358116480793061,
    "predict_soldier_recall": 0.4188562596599691,
    "predict_sportsevent_f1": 0.42080237741456167,
    "predict_sportsevent_number": 1572,
    "predict_sportsevent_precision": 0.39486893474623536,
    "predict_sportsevent_recall": 0.45038167938931295,
    "predict_sportsfacility_f1": 0.5286160249739854,
    "predict_sportsfacility_number": 420,
    "predict_sportsfacility_precision": 0.46950092421441775,
    "predict_sportsfacility_recall": 0.6047619047619047,
    "predict_sportsleague_f1": 0.46781789638932497,
    "predict_sportsleague_number": 885,
    "predict_sportsleague_precision": 0.43567251461988304,
    "predict_sportsleague_recall": 0.5050847457627119,
    "predict_sportsteam_f1": 0.6186504927975739,
    "predict_sportsteam_number": 2477,
    "predict_sportsteam_precision": 0.5830653804930332,
    "predict_sportsteam_recall": 0.6588615260395639,
    "predict_steps_per_second": 4.279,
    "predict_theater_f1": 0.5988258317025441,
    "predict_theater_number": 456,
    "predict_theater_precision": 0.5406360424028268,
    "predict_theater_recall": 0.6710526315789473,
    "predict_train_f1": 0.40976933514246955,
    "predict_train_number": 314,
    "predict_train_precision": 0.35697399527186763,
    "predict_train_recall": 0.48089171974522293,
    "predict_weapon_f1": 0.42812500000000003,
    "predict_weapon_number": 625,
    "predict_weapon_precision": 0.4183206106870229,
    "predict_weapon_recall": 0.4384,
    "predict_writtenart_f1": 0.4099173553719008,
    "predict_writtenart_number": 1032,
    "predict_writtenart_precision": 0.3573487031700288,
    "predict_writtenart_recall": 0.4806201550387597
}