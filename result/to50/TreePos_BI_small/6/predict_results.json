{
    "predict_GPE_f1": 0.7398464242760663,
    "predict_GPE_number": 20409,
    "predict_GPE_precision": 0.7006614973496298,
    "predict_GPE_recall": 0.7836738693713557,
    "predict_actor_f1": 0.6021505376344086,
    "predict_actor_number": 1638,
    "predict_actor_precision": 0.6060606060606061,
    "predict_actor_recall": 0.5982905982905983,
    "predict_airplane_f1": 0.5024213075060533,
    "predict_airplane_number": 794,
    "predict_airplane_precision": 0.4836829836829837,
    "predict_airplane_recall": 0.5226700251889169,
    "predict_airport_f1": 0.603248259860789,
    "predict_airport_number": 364,
    "predict_airport_precision": 0.5220883534136547,
    "predict_airport_recall": 0.7142857142857143,
    "predict_artist/author_f1": 0.5516379094773695,
    "predict_artist/author_number": 3464,
    "predict_artist/author_precision": 0.4865460961623291,
    "predict_artist/author_recall": 0.6368360277136259,
    "predict_astronomything_f1": 0.6381805651274982,
    "predict_astronomything_number": 678,
    "predict_astronomything_precision": 0.5989650711513583,
    "predict_astronomything_recall": 0.6828908554572272,
    "predict_athlete_f1": 0.765388333870448,
    "predict_athlete_number": 2907,
    "predict_athlete_precision": 0.7199151257956957,
    "predict_athlete_recall": 0.8169934640522876,
    "predict_attack/battle/war/militaryconflict_f1": 0.6004319654427646,
    "predict_attack/battle/war/militaryconflict_number": 1099,
    "predict_attack/battle/war/militaryconflict_precision": 0.571546052631579,
    "predict_attack/battle/war/militaryconflict_recall": 0.632393084622384,
    "predict_award_f1": 0.4169068203650336,
    "predict_award_number": 941,
    "predict_award_precision": 0.3803680981595092,
    "predict_award_recall": 0.461211477151966,
    "predict_biologything_f1": 0.5176936401880723,
    "predict_biologything_number": 1875,
    "predict_biologything_precision": 0.48291782086795937,
    "predict_biologything_recall": 0.5578666666666666,
    "predict_bodiesofwater_f1": 0.6163166397415186,
    "predict_bodiesofwater_number": 1169,
    "predict_bodiesofwater_precision": 0.5837796480489671,
    "predict_bodiesofwater_recall": 0.6526946107784432,
    "predict_broadcastprogram_f1": 0.3657056145675266,
    "predict_broadcastprogram_number": 605,
    "predict_broadcastprogram_precision": 0.3380084151472651,
    "predict_broadcastprogram_recall": 0.39834710743801655,
    "predict_car_f1": 0.551994767822106,
    "predict_car_number": 688,
    "predict_car_precision": 0.5017835909631391,
    "predict_car_recall": 0.6133720930232558,
    "predict_chemicalthing_f1": 0.4900260190806591,
    "predict_chemicalthing_number": 1014,
    "predict_chemicalthing_precision": 0.4373065015479876,
    "predict_chemicalthing_recall": 0.5571992110453649,
    "predict_company_f1": 0.5244114002478314,
    "predict_company_number": 3903,
    "predict_company_precision": 0.507799376049916,
    "predict_company_recall": 0.5421470663592108,
    "predict_currency_f1": 0.6625971143174251,
    "predict_currency_number": 799,
    "predict_currency_precision": 0.5952143569292123,
    "predict_currency_recall": 0.7471839799749687,
    "predict_director_f1": 0.4885844748858448,
    "predict_director_number": 554,
    "predict_director_precision": 0.42236842105263156,
    "predict_director_recall": 0.5794223826714802,
    "predict_disaster_f1": 0.3837953091684435,
    "predict_disaster_number": 207,
    "predict_disaster_precision": 0.3435114503816794,
    "predict_disaster_recall": 0.43478260869565216,
    "predict_disease_f1": 0.5346307956496853,
    "predict_disease_number": 750,
    "predict_disease_precision": 0.4684052156469408,
    "predict_disease_recall": 0.6226666666666667,
    "predict_education_f1": 0.6121648136036625,
    "predict_education_number": 2082,
    "predict_education_precision": 0.5604790419161677,
    "predict_education_recall": 0.6743515850144092,
    "predict_educationaldegree_f1": 0.41931942919868276,
    "predict_educationaldegree_number": 367,
    "predict_educationaldegree_precision": 0.35110294117647056,
    "predict_educationaldegree_recall": 0.5204359673024523,
    "predict_election_f1": 0.11594202898550725,
    "predict_election_number": 184,
    "predict_election_precision": 0.09364548494983277,
    "predict_election_recall": 0.15217391304347827,
    "predict_film_f1": 0.5165165165165164,
    "predict_film_number": 759,
    "predict_film_precision": 0.4746136865342163,
    "predict_film_recall": 0.5665349143610013,
    "predict_food_f1": 0.4051130776794494,
    "predict_food_number": 432,
    "predict_food_precision": 0.35213675213675216,
    "predict_food_recall": 0.47685185185185186,
    "predict_game_f1": 0.4995121951219512,
    "predict_game_number": 496,
    "predict_game_precision": 0.4839319470699433,
    "predict_game_recall": 0.5161290322580645,
    "predict_god_f1": 0.5121163166397414,
    "predict_god_number": 635,
    "predict_god_precision": 0.5257048092868989,
    "predict_god_recall": 0.49921259842519683,
    "predict_government/governmentagency_f1": 0.2929411764705882,
    "predict_government/governmentagency_number": 1535,
    "predict_government/governmentagency_precision": 0.2670241286863271,
    "predict_government/governmentagency_recall": 0.3244299674267101,
    "predict_hospital_f1": 0.5234093637454982,
    "predict_hospital_number": 364,
    "predict_hospital_precision": 0.464818763326226,
    "predict_hospital_recall": 0.5989010989010989,
    "predict_hotel_f1": 0.4517304189435337,
    "predict_hotel_number": 265,
    "predict_hotel_precision": 0.43661971830985913,
    "predict_hotel_recall": 0.4679245283018868,
    "predict_island_f1": 0.5143769968051118,
    "predict_island_number": 646,
    "predict_island_precision": 0.5313531353135313,
    "predict_island_recall": 0.4984520123839009,
    "predict_language_f1": 0.614921780986763,
    "predict_language_number": 753,
    "predict_language_precision": 0.5621562156215621,
    "predict_language_recall": 0.6786188579017264,
    "predict_law_f1": 0.40802675585284276,
    "predict_law_number": 488,
    "predict_law_precision": 0.3446327683615819,
    "predict_law_recall": 0.5,
    "predict_library_f1": 0.5471923536439666,
    "predict_library_number": 357,
    "predict_library_precision": 0.47708333333333336,
    "predict_library_recall": 0.6414565826330533,
    "predict_livingthing_f1": 0.4866920152091255,
    "predict_livingthing_number": 865,
    "predict_livingthing_precision": 0.45901639344262296,
    "predict_livingthing_recall": 0.5179190751445086,
    "predict_loss": 0.6241648197174072,
    "predict_media/newspaper_f1": 0.5,
    "predict_media/newspaper_number": 1234,
    "predict_media/newspaper_precision": 0.4576043068640646,
    "predict_media/newspaper_recall": 0.5510534846029174,
    "predict_medical_f1": 0.3116219667943806,
    "predict_medical_number": 397,
    "predict_medical_precision": 0.3160621761658031,
    "predict_medical_recall": 0.30730478589420657,
    "predict_mountain_f1": 0.493368700265252,
    "predict_mountain_number": 681,
    "predict_mountain_precision": 0.4498186215235792,
    "predict_mountain_recall": 0.5462555066079295,
    "predict_music_f1": 0.5641776476329917,
    "predict_music_number": 1041,
    "predict_music_precision": 0.5734126984126984,
    "predict_music_recall": 0.5552353506243997,
    "predict_other_f1": 0.41999335032694224,
    "predict_other_number": 21035,
    "predict_other_precision": 0.39343853820598007,
    "predict_other_recall": 0.4503922034704065,
    "predict_overall_accuracy": 0.8982318083413133,
    "predict_overall_f1": 0.543469010131533,
    "predict_overall_precision": 0.5062757779024916,
    "predict_overall_recall": 0.5865602939355158,
    "predict_painting_f1": 0.21818181818181817,
    "predict_painting_number": 58,
    "predict_painting_precision": 0.23076923076923078,
    "predict_painting_recall": 0.20689655172413793,
    "predict_park_f1": 0.3894523326572008,
    "predict_park_number": 458,
    "predict_park_precision": 0.36363636363636365,
    "predict_park_recall": 0.4192139737991266,
    "predict_politicalparty_f1": 0.5397472482674276,
    "predict_politicalparty_number": 1057,
    "predict_politicalparty_precision": 0.47421203438395415,
    "predict_politicalparty_recall": 0.6263008514664143,
    "predict_politician_f1": 0.48916299559471366,
    "predict_politician_number": 2859,
    "predict_politician_precision": 0.4928977272727273,
    "predict_politician_recall": 0.4854844351171738,
    "predict_protest_f1": 0.20556745182012848,
    "predict_protest_number": 166,
    "predict_protest_precision": 0.15946843853820597,
    "predict_protest_recall": 0.2891566265060241,
    "predict_religion_f1": 0.45311589982527667,
    "predict_religion_number": 676,
    "predict_religion_precision": 0.37367915465898177,
    "predict_religion_recall": 0.5754437869822485,
    "predict_restaurant_f1": 0.33128834355828224,
    "predict_restaurant_number": 232,
    "predict_restaurant_precision": 0.3151750972762646,
    "predict_restaurant_recall": 0.34913793103448276,
    "predict_road/railway/highway/transit_f1": 0.5593091186182373,
    "predict_road/railway/highway/transit_number": 1702,
    "predict_road/railway/highway/transit_precision": 0.49261744966442955,
    "predict_road/railway/highway/transit_recall": 0.6468860164512339,
    "predict_runtime": 284.693,
    "predict_samples_per_second": 132.237,
    "predict_scholar_f1": 0.2916923076923077,
    "predict_scholar_number": 743,
    "predict_scholar_precision": 0.2687074829931973,
    "predict_scholar_recall": 0.3189771197846568,
    "predict_ship_f1": 0.5280764635603344,
    "predict_ship_number": 380,
    "predict_ship_precision": 0.48358862144420134,
    "predict_ship_recall": 0.5815789473684211,
    "predict_showorganization_f1": 0.39068100358422936,
    "predict_showorganization_number": 770,
    "predict_showorganization_precision": 0.36172566371681414,
    "predict_showorganization_recall": 0.42467532467532465,
    "predict_software_f1": 0.4804708400214018,
    "predict_software_number": 889,
    "predict_software_precision": 0.45816326530612245,
    "predict_software_recall": 0.5050618672665916,
    "predict_soldier_f1": 0.3818056512749828,
    "predict_soldier_number": 647,
    "predict_soldier_precision": 0.3445273631840796,
    "predict_soldier_recall": 0.4281298299845441,
    "predict_sportsevent_f1": 0.41230953092321476,
    "predict_sportsevent_number": 1572,
    "predict_sportsevent_precision": 0.38873239436619716,
    "predict_sportsevent_recall": 0.4389312977099237,
    "predict_sportsfacility_f1": 0.5672877846790891,
    "predict_sportsfacility_number": 420,
    "predict_sportsfacility_precision": 0.5018315018315018,
    "predict_sportsfacility_recall": 0.6523809523809524,
    "predict_sportsleague_f1": 0.4616905585763717,
    "predict_sportsleague_number": 885,
    "predict_sportsleague_precision": 0.4103690685413005,
    "predict_sportsleague_recall": 0.5276836158192091,
    "predict_sportsteam_f1": 0.6364494806421152,
    "predict_sportsteam_number": 2477,
    "predict_sportsteam_precision": 0.5979418026969482,
    "predict_sportsteam_recall": 0.6802583770690351,
    "predict_steps_per_second": 4.134,
    "predict_theater_f1": 0.5773584905660377,
    "predict_theater_number": 456,
    "predict_theater_precision": 0.5066225165562914,
    "predict_theater_recall": 0.6710526315789473,
    "predict_train_f1": 0.38016528925619836,
    "predict_train_number": 314,
    "predict_train_precision": 0.33495145631067963,
    "predict_train_recall": 0.4394904458598726,
    "predict_weapon_f1": 0.4301412872841444,
    "predict_weapon_number": 625,
    "predict_weapon_precision": 0.4221879815100154,
    "predict_weapon_recall": 0.4384,
    "predict_writtenart_f1": 0.43327702702702703,
    "predict_writtenart_number": 1032,
    "predict_writtenart_precision": 0.38398203592814373,
    "predict_writtenart_recall": 0.49709302325581395
}