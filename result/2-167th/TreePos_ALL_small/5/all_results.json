{
    "predict_GPE_f1": 0.7582827365300951,
    "predict_GPE_number": 20409,
    "predict_GPE_precision": 0.7403033838973162,
    "predict_GPE_recall": 0.7771571365574012,
    "predict_actor_f1": 0.6718249733191035,
    "predict_actor_number": 1638,
    "predict_actor_precision": 0.5966824644549763,
    "predict_actor_recall": 0.7686202686202687,
    "predict_airplane_f1": 0.5127582017010935,
    "predict_airplane_number": 794,
    "predict_airplane_precision": 0.4953051643192488,
    "predict_airplane_recall": 0.5314861460957179,
    "predict_airport_f1": 0.5946573751451801,
    "predict_airport_number": 364,
    "predict_airport_precision": 0.5150905432595574,
    "predict_airport_recall": 0.7032967032967034,
    "predict_artist/author_f1": 0.5747842082920618,
    "predict_artist/author_number": 3464,
    "predict_artist/author_precision": 0.5636969192339717,
    "predict_artist/author_recall": 0.5863163972286374,
    "predict_astronomything_f1": 0.5899379738111648,
    "predict_astronomything_number": 678,
    "predict_astronomything_precision": 0.553686934023286,
    "predict_astronomything_recall": 0.6312684365781711,
    "predict_athlete_f1": 0.7833194328607171,
    "predict_athlete_number": 2907,
    "predict_athlete_precision": 0.7603626943005182,
    "predict_athlete_recall": 0.8077055383556931,
    "predict_attack/battle/war/militaryconflict_f1": 0.6310475389145982,
    "predict_attack/battle/war/militaryconflict_number": 1099,
    "predict_attack/battle/war/militaryconflict_precision": 0.5868544600938967,
    "predict_attack/battle/war/militaryconflict_recall": 0.6824385805277525,
    "predict_award_f1": 0.47110247693054885,
    "predict_award_number": 941,
    "predict_award_precision": 0.4338103756708408,
    "predict_award_recall": 0.5154091392136025,
    "predict_biologything_f1": 0.5250236071765817,
    "predict_biologything_number": 1875,
    "predict_biologything_precision": 0.47098686997035155,
    "predict_biologything_recall": 0.5930666666666666,
    "predict_bodiesofwater_f1": 0.611764705882353,
    "predict_bodiesofwater_number": 1169,
    "predict_bodiesofwater_precision": 0.5817901234567902,
    "predict_bodiesofwater_recall": 0.6449957228400343,
    "predict_broadcastprogram_f1": 0.4,
    "predict_broadcastprogram_number": 605,
    "predict_broadcastprogram_precision": 0.40508474576271186,
    "predict_broadcastprogram_recall": 0.3950413223140496,
    "predict_car_f1": 0.5971428571428572,
    "predict_car_number": 688,
    "predict_car_precision": 0.5870786516853933,
    "predict_car_recall": 0.6075581395348837,
    "predict_chemicalthing_f1": 0.48762932973459283,
    "predict_chemicalthing_number": 1014,
    "predict_chemicalthing_precision": 0.44830438378825477,
    "predict_chemicalthing_recall": 0.534516765285996,
    "predict_company_f1": 0.5676180979683411,
    "predict_company_number": 3903,
    "predict_company_precision": 0.5526699029126214,
    "predict_company_recall": 0.5833973866256725,
    "predict_currency_f1": 0.611111111111111,
    "predict_currency_number": 799,
    "predict_currency_precision": 0.5409836065573771,
    "predict_currency_recall": 0.7021276595744681,
    "predict_director_f1": 0.5523978685612788,
    "predict_director_number": 554,
    "predict_director_precision": 0.5437062937062938,
    "predict_director_recall": 0.5613718411552346,
    "predict_disaster_f1": 0.36756756756756753,
    "predict_disaster_number": 207,
    "predict_disaster_precision": 0.29310344827586204,
    "predict_disaster_recall": 0.4927536231884058,
    "predict_disease_f1": 0.5498812351543942,
    "predict_disease_number": 750,
    "predict_disease_precision": 0.49571734475374735,
    "predict_disease_recall": 0.6173333333333333,
    "predict_education_f1": 0.650359066427289,
    "predict_education_number": 2082,
    "predict_education_precision": 0.6103622577927549,
    "predict_education_recall": 0.6959654178674352,
    "predict_educationaldegree_f1": 0.4343786295005807,
    "predict_educationaldegree_number": 367,
    "predict_educationaldegree_precision": 0.3785425101214575,
    "predict_educationaldegree_recall": 0.5095367847411444,
    "predict_election_f1": 0.12682926829268293,
    "predict_election_number": 184,
    "predict_election_precision": 0.11504424778761062,
    "predict_election_recall": 0.14130434782608695,
    "predict_film_f1": 0.5323096609085093,
    "predict_film_number": 759,
    "predict_film_precision": 0.5174129353233831,
    "predict_film_recall": 0.5480895915678524,
    "predict_food_f1": 0.41283422459893054,
    "predict_food_number": 432,
    "predict_food_precision": 0.3836978131212724,
    "predict_food_recall": 0.44675925925925924,
    "predict_game_f1": 0.5300092336103416,
    "predict_game_number": 496,
    "predict_game_precision": 0.4889267461669506,
    "predict_game_recall": 0.5786290322580645,
    "predict_god_f1": 0.5578135949544499,
    "predict_god_number": 635,
    "predict_god_precision": 0.5025252525252525,
    "predict_god_recall": 0.6267716535433071,
    "predict_government/governmentagency_f1": 0.3052862058179543,
    "predict_government/governmentagency_number": 1535,
    "predict_government/governmentagency_precision": 0.29362214199759323,
    "predict_government/governmentagency_recall": 0.31791530944625407,
    "predict_hospital_f1": 0.5846153846153846,
    "predict_hospital_number": 364,
    "predict_hospital_precision": 0.5954415954415955,
    "predict_hospital_recall": 0.5741758241758241,
    "predict_hotel_f1": 0.5392156862745099,
    "predict_hotel_number": 265,
    "predict_hotel_precision": 0.4755043227665706,
    "predict_hotel_recall": 0.6226415094339622,
    "predict_island_f1": 0.5571658615136876,
    "predict_island_number": 646,
    "predict_island_precision": 0.5805369127516778,
    "predict_island_recall": 0.5356037151702786,
    "predict_language_f1": 0.630715123094959,
    "predict_language_number": 753,
    "predict_language_precision": 0.5645330535152151,
    "predict_language_recall": 0.7144754316069057,
    "predict_law_f1": 0.44635193133047213,
    "predict_law_number": 488,
    "predict_law_precision": 0.38404726735598227,
    "predict_law_recall": 0.5327868852459017,
    "predict_library_f1": 0.5224586288416075,
    "predict_library_number": 357,
    "predict_library_precision": 0.45194274028629855,
    "predict_library_recall": 0.6190476190476191,
    "predict_livingthing_f1": 0.5378973105134474,
    "predict_livingthing_number": 865,
    "predict_livingthing_precision": 0.4661016949152542,
    "predict_livingthing_recall": 0.6358381502890174,
    "predict_loss": 0.5224116444587708,
    "predict_media/newspaper_f1": 0.5330909090909091,
    "predict_media/newspaper_number": 1234,
    "predict_media/newspaper_precision": 0.48350923482849606,
    "predict_media/newspaper_recall": 0.5940032414910859,
    "predict_medical_f1": 0.3485477178423237,
    "predict_medical_number": 397,
    "predict_medical_precision": 0.38650306748466257,
    "predict_medical_recall": 0.31738035264483627,
    "predict_mountain_f1": 0.5872053872053872,
    "predict_mountain_number": 681,
    "predict_mountain_precision": 0.5422885572139303,
    "predict_mountain_recall": 0.6402349486049926,
    "predict_music_f1": 0.6091503267973857,
    "predict_music_number": 1041,
    "predict_music_precision": 0.5574162679425837,
    "predict_music_recall": 0.6714697406340058,
    "predict_other_f1": 0.4541640399121758,
    "predict_other_number": 21035,
    "predict_other_precision": 0.4184090362893535,
    "predict_other_recall": 0.4966009032564773,
    "predict_overall_accuracy": 0.9010937469464331,
    "predict_overall_f1": 0.5677937018150181,
    "predict_overall_precision": 0.533979435054958,
    "predict_overall_recall": 0.6061800767865252,
    "predict_painting_f1": 0.0,
    "predict_painting_number": 58,
    "predict_painting_precision": 0.0,
    "predict_painting_recall": 0.0,
    "predict_park_f1": 0.43581616481774965,
    "predict_park_number": 458,
    "predict_park_precision": 0.3420398009950249,
    "predict_park_recall": 0.6004366812227074,
    "predict_politicalparty_f1": 0.5958875367184222,
    "predict_politicalparty_number": 1057,
    "predict_politicalparty_precision": 0.5354449472096531,
    "predict_politicalparty_recall": 0.6717123935666982,
    "predict_politician_f1": 0.5515418502202644,
    "predict_politician_number": 2859,
    "predict_politician_precision": 0.5557528409090909,
    "predict_politician_recall": 0.5473941937740469,
    "predict_protest_f1": 0.14556962025316456,
    "predict_protest_number": 166,
    "predict_protest_precision": 0.15333333333333332,
    "predict_protest_recall": 0.13855421686746988,
    "predict_religion_f1": 0.4190981432360743,
    "predict_religion_number": 676,
    "predict_religion_precision": 0.3798076923076923,
    "predict_religion_recall": 0.46745562130177515,
    "predict_restaurant_f1": 0.27598566308243727,
    "predict_restaurant_number": 232,
    "predict_restaurant_precision": 0.2361963190184049,
    "predict_restaurant_recall": 0.33189655172413796,
    "predict_road/railway/highway/transit_f1": 0.5801486199575372,
    "predict_road/railway/highway/transit_number": 1702,
    "predict_road/railway/highway/transit_precision": 0.5290416263310745,
    "predict_road/railway/highway/transit_recall": 0.6421856639247944,
    "predict_runtime": 284.5223,
    "predict_samples_per_second": 132.317,
    "predict_scholar_f1": 0.2766439909297052,
    "predict_scholar_number": 743,
    "predict_scholar_precision": 0.31551724137931036,
    "predict_scholar_recall": 0.24629878869448182,
    "predict_ship_f1": 0.4426229508196721,
    "predict_ship_number": 380,
    "predict_ship_precision": 0.3987341772151899,
    "predict_ship_recall": 0.49736842105263157,
    "predict_showorganization_f1": 0.45836023240800516,
    "predict_showorganization_number": 770,
    "predict_showorganization_precision": 0.4557124518613607,
    "predict_showorganization_recall": 0.461038961038961,
    "predict_software_f1": 0.47689856611789694,
    "predict_software_number": 889,
    "predict_software_precision": 0.4517102615694165,
    "predict_software_recall": 0.5050618672665916,
    "predict_soldier_f1": 0.40668896321070236,
    "predict_soldier_number": 647,
    "predict_soldier_precision": 0.3584905660377358,
    "predict_soldier_recall": 0.46986089644513135,
    "predict_sportsevent_f1": 0.46464646464646464,
    "predict_sportsevent_number": 1572,
    "predict_sportsevent_precision": 0.41566265060240964,
    "predict_sportsevent_recall": 0.5267175572519084,
    "predict_sportsfacility_f1": 0.5137816979051819,
    "predict_sportsfacility_number": 420,
    "predict_sportsfacility_precision": 0.4784394250513347,
    "predict_sportsfacility_recall": 0.5547619047619048,
    "predict_sportsleague_f1": 0.44420485175202157,
    "predict_sportsleague_number": 885,
    "predict_sportsleague_precision": 0.4247422680412371,
    "predict_sportsleague_recall": 0.4655367231638418,
    "predict_sportsteam_f1": 0.6499719678564755,
    "predict_sportsteam_number": 2477,
    "predict_sportsteam_precision": 0.605080027835769,
    "predict_sportsteam_recall": 0.7020589422688737,
    "predict_steps_per_second": 4.137,
    "predict_theater_f1": 0.5393258426966292,
    "predict_theater_number": 456,
    "predict_theater_precision": 0.5047801147227533,
    "predict_theater_recall": 0.5789473684210527,
    "predict_train_f1": 0.3815789473684211,
    "predict_train_number": 314,
    "predict_train_precision": 0.3251121076233184,
    "predict_train_recall": 0.46178343949044587,
    "predict_weapon_f1": 0.42149454240134343,
    "predict_weapon_number": 625,
    "predict_weapon_precision": 0.44346289752650175,
    "predict_weapon_recall": 0.4016,
    "predict_writtenart_f1": 0.45404018811457886,
    "predict_writtenart_number": 1032,
    "predict_writtenart_precision": 0.40627390971690897,
    "predict_writtenart_recall": 0.5145348837209303
}