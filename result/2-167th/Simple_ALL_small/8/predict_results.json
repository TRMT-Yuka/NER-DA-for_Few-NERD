{
    "predict_GPE_f1": 0.7545811986178957,
    "predict_GPE_number": 20409,
    "predict_GPE_precision": 0.7345054741139357,
    "predict_GPE_recall": 0.7757851928070949,
    "predict_actor_f1": 0.6662886305642189,
    "predict_actor_number": 1638,
    "predict_actor_precision": 0.6220222339862361,
    "predict_actor_recall": 0.7173382173382173,
    "predict_airplane_f1": 0.5226312613156306,
    "predict_airplane_number": 794,
    "predict_airplane_precision": 0.5017381228273464,
    "predict_airplane_recall": 0.5453400503778337,
    "predict_airport_f1": 0.691156462585034,
    "predict_airport_number": 364,
    "predict_airport_precision": 0.6846361185983828,
    "predict_airport_recall": 0.6978021978021978,
    "predict_artist/author_f1": 0.5706028075970272,
    "predict_artist/author_number": 3464,
    "predict_artist/author_precision": 0.5452393477117307,
    "predict_artist/author_recall": 0.5984411085450346,
    "predict_astronomything_f1": 0.6153846153846154,
    "predict_astronomything_number": 678,
    "predict_astronomything_precision": 0.5899864682002707,
    "predict_astronomything_recall": 0.6430678466076696,
    "predict_athlete_f1": 0.7809045226130653,
    "predict_athlete_number": 2907,
    "predict_athlete_precision": 0.7610186092066601,
    "predict_athlete_recall": 0.8018575851393189,
    "predict_attack/battle/war/militaryconflict_f1": 0.605274921770228,
    "predict_attack/battle/war/militaryconflict_number": 1099,
    "predict_attack/battle/war/militaryconflict_precision": 0.5949033391915641,
    "predict_attack/battle/war/militaryconflict_recall": 0.6160145586897179,
    "predict_award_f1": 0.40339302544769085,
    "predict_award_number": 941,
    "predict_award_precision": 0.3624047417442845,
    "predict_award_recall": 0.45483528161530284,
    "predict_biologything_f1": 0.5281081081081082,
    "predict_biologything_number": 1875,
    "predict_biologything_precision": 0.5353424657534247,
    "predict_biologything_recall": 0.5210666666666667,
    "predict_bodiesofwater_f1": 0.5910204081632654,
    "predict_bodiesofwater_number": 1169,
    "predict_bodiesofwater_precision": 0.565183450429352,
    "predict_bodiesofwater_recall": 0.6193327630453379,
    "predict_broadcastprogram_f1": 0.43613707165109034,
    "predict_broadcastprogram_number": 605,
    "predict_broadcastprogram_precision": 0.41237113402061853,
    "predict_broadcastprogram_recall": 0.4628099173553719,
    "predict_car_f1": 0.5652777777777779,
    "predict_car_number": 688,
    "predict_car_precision": 0.5412234042553191,
    "predict_car_recall": 0.5915697674418605,
    "predict_chemicalthing_f1": 0.4828507795100223,
    "predict_chemicalthing_number": 1014,
    "predict_chemicalthing_precision": 0.44029244516653127,
    "predict_chemicalthing_recall": 0.534516765285996,
    "predict_company_f1": 0.5469673218376944,
    "predict_company_number": 3903,
    "predict_company_precision": 0.516628701594533,
    "predict_company_recall": 0.5810914681014604,
    "predict_currency_f1": 0.6040268456375839,
    "predict_currency_number": 799,
    "predict_currency_precision": 0.5460060667340748,
    "predict_currency_recall": 0.6758448060075094,
    "predict_director_f1": 0.5587808417997098,
    "predict_director_number": 554,
    "predict_director_precision": 0.4672330097087379,
    "predict_director_recall": 0.6949458483754513,
    "predict_disaster_f1": 0.34418604651162793,
    "predict_disaster_number": 207,
    "predict_disaster_precision": 0.33183856502242154,
    "predict_disaster_recall": 0.357487922705314,
    "predict_disease_f1": 0.4829931972789115,
    "predict_disease_number": 750,
    "predict_disease_precision": 0.42011834319526625,
    "predict_disease_recall": 0.568,
    "predict_education_f1": 0.6133820506965061,
    "predict_education_number": 2082,
    "predict_education_precision": 0.5846756639094471,
    "predict_education_recall": 0.6450528338136408,
    "predict_educationaldegree_f1": 0.36942675159235666,
    "predict_educationaldegree_number": 367,
    "predict_educationaldegree_precision": 0.3026086956521739,
    "predict_educationaldegree_recall": 0.47411444141689374,
    "predict_election_f1": 0.1814516129032258,
    "predict_election_number": 184,
    "predict_election_precision": 0.14423076923076922,
    "predict_election_recall": 0.24456521739130435,
    "predict_film_f1": 0.5804962492787074,
    "predict_film_number": 759,
    "predict_film_precision": 0.5164271047227926,
    "predict_film_recall": 0.6627140974967062,
    "predict_food_f1": 0.39581351094196005,
    "predict_food_number": 432,
    "predict_food_precision": 0.3360258481421648,
    "predict_food_recall": 0.48148148148148145,
    "predict_game_f1": 0.5207226354941552,
    "predict_game_number": 496,
    "predict_game_precision": 0.550561797752809,
    "predict_game_recall": 0.4939516129032258,
    "predict_god_f1": 0.5437178545187362,
    "predict_god_number": 635,
    "predict_god_precision": 0.509641873278237,
    "predict_god_recall": 0.5826771653543307,
    "predict_government/governmentagency_f1": 0.29738280126545874,
    "predict_government/governmentagency_number": 1535,
    "predict_government/governmentagency_precision": 0.2662203913491246,
    "predict_government/governmentagency_recall": 0.33680781758957656,
    "predict_hospital_f1": 0.544757033248082,
    "predict_hospital_number": 364,
    "predict_hospital_precision": 0.5095693779904307,
    "predict_hospital_recall": 0.5851648351648352,
    "predict_hotel_f1": 0.5972495088408644,
    "predict_hotel_number": 265,
    "predict_hotel_precision": 0.6229508196721312,
    "predict_hotel_recall": 0.5735849056603773,
    "predict_island_f1": 0.5246868091378041,
    "predict_island_number": 646,
    "predict_island_precision": 0.5007032348804501,
    "predict_island_recall": 0.5510835913312694,
    "predict_language_f1": 0.5911443358251868,
    "predict_language_number": 753,
    "predict_language_precision": 0.5212981744421906,
    "predict_language_recall": 0.6826029216467463,
    "predict_law_f1": 0.4308510638297873,
    "predict_law_number": 488,
    "predict_law_precision": 0.3796875,
    "predict_law_recall": 0.4979508196721312,
    "predict_library_f1": 0.5072655217965655,
    "predict_library_number": 357,
    "predict_library_precision": 0.48,
    "predict_library_recall": 0.5378151260504201,
    "predict_livingthing_f1": 0.5218631178707225,
    "predict_livingthing_number": 865,
    "predict_livingthing_precision": 0.4430992736077482,
    "predict_livingthing_recall": 0.6346820809248555,
    "predict_loss": 0.5150159001350403,
    "predict_media/newspaper_f1": 0.5157088122605364,
    "predict_media/newspaper_number": 1234,
    "predict_media/newspaper_precision": 0.4890988372093023,
    "predict_media/newspaper_recall": 0.5453808752025932,
    "predict_medical_f1": 0.39568345323741005,
    "predict_medical_number": 397,
    "predict_medical_precision": 0.37757437070938216,
    "predict_medical_recall": 0.4156171284634761,
    "predict_mountain_f1": 0.5275016567263088,
    "predict_mountain_number": 681,
    "predict_mountain_precision": 0.4806763285024155,
    "predict_mountain_recall": 0.5844346549192364,
    "predict_music_f1": 0.6219239373601789,
    "predict_music_number": 1041,
    "predict_music_precision": 0.5820770519262981,
    "predict_music_recall": 0.6676272814601345,
    "predict_other_f1": 0.44004419889502766,
    "predict_other_number": 21035,
    "predict_other_precision": 0.41115011356597153,
    "predict_other_recall": 0.473306394105063,
    "predict_overall_accuracy": 0.9008603187218122,
    "predict_overall_f1": 0.5571012272169666,
    "predict_overall_precision": 0.5253733527211537,
    "predict_overall_recall": 0.5929075671882096,
    "predict_painting_f1": 0.0,
    "predict_painting_number": 58,
    "predict_painting_precision": 0.0,
    "predict_painting_recall": 0.0,
    "predict_park_f1": 0.35881753312945974,
    "predict_park_number": 458,
    "predict_park_precision": 0.3365200764818356,
    "predict_park_recall": 0.38427947598253276,
    "predict_politicalparty_f1": 0.5702163767501061,
    "predict_politicalparty_number": 1057,
    "predict_politicalparty_precision": 0.5169230769230769,
    "predict_politicalparty_recall": 0.6357615894039735,
    "predict_politician_f1": 0.5239744758432088,
    "predict_politician_number": 2859,
    "predict_politician_precision": 0.5472201066260473,
    "predict_politician_recall": 0.5026232948583421,
    "predict_protest_f1": 0.16384180790960454,
    "predict_protest_number": 166,
    "predict_protest_precision": 0.15425531914893617,
    "predict_protest_recall": 0.1746987951807229,
    "predict_religion_f1": 0.4499396863691194,
    "predict_religion_number": 676,
    "predict_religion_precision": 0.37983706720977595,
    "predict_religion_recall": 0.5517751479289941,
    "predict_restaurant_f1": 0.32809773123909247,
    "predict_restaurant_number": 232,
    "predict_restaurant_precision": 0.2756598240469208,
    "predict_restaurant_recall": 0.4051724137931034,
    "predict_road/railway/highway/transit_f1": 0.5691573926868044,
    "predict_road/railway/highway/transit_number": 1702,
    "predict_road/railway/highway/transit_precision": 0.5183397683397684,
    "predict_road/railway/highway/transit_recall": 0.6310223266745005,
    "predict_runtime": 274.4018,
    "predict_samples_per_second": 137.197,
    "predict_scholar_f1": 0.35354179961710275,
    "predict_scholar_number": 743,
    "predict_scholar_precision": 0.33616504854368934,
    "predict_scholar_recall": 0.3728129205921938,
    "predict_ship_f1": 0.46658851113716293,
    "predict_ship_number": 380,
    "predict_ship_precision": 0.42071881606765327,
    "predict_ship_recall": 0.5236842105263158,
    "predict_showorganization_f1": 0.4056482670089859,
    "predict_showorganization_number": 770,
    "predict_showorganization_precision": 0.4010152284263959,
    "predict_showorganization_recall": 0.4103896103896104,
    "predict_software_f1": 0.4849428868120457,
    "predict_software_number": 889,
    "predict_software_precision": 0.4503375120540019,
    "predict_software_recall": 0.5253093363329584,
    "predict_soldier_f1": 0.3994232155731796,
    "predict_soldier_number": 647,
    "predict_soldier_precision": 0.37432432432432433,
    "predict_soldier_recall": 0.4281298299845441,
    "predict_sportsevent_f1": 0.4284400858106037,
    "predict_sportsevent_number": 1572,
    "predict_sportsevent_precision": 0.413364872856298,
    "predict_sportsevent_recall": 0.4446564885496183,
    "predict_sportsfacility_f1": 0.5575501583949313,
    "predict_sportsfacility_number": 420,
    "predict_sportsfacility_precision": 0.5009487666034156,
    "predict_sportsfacility_recall": 0.6285714285714286,
    "predict_sportsleague_f1": 0.4148430066603236,
    "predict_sportsleague_number": 885,
    "predict_sportsleague_precision": 0.3582580115036976,
    "predict_sportsleague_recall": 0.49265536723163844,
    "predict_sportsteam_f1": 0.6488372093023256,
    "predict_sportsteam_number": 2477,
    "predict_sportsteam_precision": 0.6239284383153186,
    "predict_sportsteam_recall": 0.6758175211949939,
    "predict_steps_per_second": 4.289,
    "predict_theater_f1": 0.5291308500477554,
    "predict_theater_number": 456,
    "predict_theater_precision": 0.4686971235194585,
    "predict_theater_recall": 0.6074561403508771,
    "predict_train_f1": 0.41791044776119407,
    "predict_train_number": 314,
    "predict_train_precision": 0.3640661938534279,
    "predict_train_recall": 0.49044585987261147,
    "predict_weapon_f1": 0.39967637540453066,
    "predict_weapon_number": 625,
    "predict_weapon_precision": 0.40425531914893614,
    "predict_weapon_recall": 0.3952,
    "predict_writtenart_f1": 0.4519966015293118,
    "predict_writtenart_number": 1032,
    "predict_writtenart_precision": 0.40242057488653554,
    "predict_writtenart_recall": 0.5155038759689923
}