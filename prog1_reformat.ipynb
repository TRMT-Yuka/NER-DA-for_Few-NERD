{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b91ecd",
   "metadata": {},
   "source": [
    "# 元々のjsonファイルを取得し，整形したものを出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d72a332",
   "metadata": {},
   "source": [
    "・ラベル名をtagsに統一\n",
    "\n",
    "・ラベルとして使用されていた数値IDを文字型に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\"words\": [\"CHICAGO\", \"AT\", \"ATLANTA\"], \"ner\": [\"B-ORG\", \"O\", \"B-LOC\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9c5ad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93885b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/original/id2coarse_tags.json\",\"r\") as f:\n",
    "    label_coarse =json.loads(f.read())\n",
    "    \n",
    "with open(\"data/original/id2fine_tags.json\",\"r\") as f:\n",
    "    label_fine =json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0f0f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_str_tag(tags):\n",
    "    new_tag = [label_coarse[str(i)] for i in tags]\n",
    "    return new_tag\n",
    "    \n",
    "def fine_str_tag(tags):\n",
    "    new_tag = [label_fine[str(i)] for i in tags]\n",
    "    return new_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fc163c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in [\"train\",\"test\",\"dev\"]:\n",
    "    \n",
    "    data = []\n",
    "    with open(\"data/original/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    coarse = []\n",
    "    for d in data:\n",
    "        new_d = {}\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        new_d[\"tags\"] = coarse_str_tag(d[\"coarse_tags\"])\n",
    "        coarse.append(new_d)\n",
    "    with open(\"data/coarse/\"+filename+\".json\",\"w\") as f:\n",
    "        for d in coarse:\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    fine = []\n",
    "    for d in data:\n",
    "        new_d = {}\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        new_d[\"tags\"] = fine_str_tag(d[\"fine_tags\"])\n",
    "        fine.append(new_d)\n",
    "    with open(\"data/fine/\"+filename+\".json\",\"w\") as f:\n",
    "        for d in fine:\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63667777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0': 'O',\n",
       "  '1': 'art',\n",
       "  '2': 'building',\n",
       "  '3': 'event',\n",
       "  '4': 'location',\n",
       "  '5': 'organization',\n",
       "  '6': 'other',\n",
       "  '7': 'person',\n",
       "  '8': 'product'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_coarse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513b6a86",
   "metadata": {},
   "source": [
    "# Simple-BI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcf0d7e",
   "metadata": {},
   "source": [
    "・BIラベルのみに着目したデータ拡張\n",
    "\n",
    "・交換率100％"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e5e5fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]\n",
    "\n",
    "def Simple_BI(dataname,filename,n):\n",
    "    data = []\n",
    "    with open(\"data/\"+dataname+\"/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    alterna_labels = {}\n",
    "    if dataname == \"c\":\n",
    "        tag_json = \"data/original/id2coarse_tags.json\"\n",
    "    else:\n",
    "        tag_json = \"data/original/id2fine_tags.json\"\n",
    "    with open(tag_json,\"r\") as f:\n",
    "        label_coarse =json.loads(f.read())\n",
    "        label_list = list(label_coarse.values())[1:]\n",
    "    for label in label_list:\n",
    "        alterna_labels[label] = set()\n",
    "    for d in data:\n",
    "        for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "            if tag in label_list:\n",
    "                alterna_labels[tag].add(token)\n",
    "\n",
    "    data_f_path = \"data/\"+dataname+\"_BI_x\"+str(n)+\"/\"+filename+\".json\"\n",
    "    memo_f_path = \"log/\"+dataname+\"_BI_x\"+str(n)+\"_\"+filename+\"_memo.txt\"\n",
    "    \n",
    "    \n",
    "    with open(data_f_path,\"w\") as data_f, open(memo_f_path,\"w\") as memo_f:\n",
    "        for d in data:\n",
    "            data_f.write(json.dumps(d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "        add_n = n*len(data)-len(data)\n",
    "        print()\n",
    "        print(dataname,filename,\"add_n\",add_n)\n",
    "        \n",
    "        for now in range(add_n):\n",
    "            if now%10 == 0:\n",
    "                print('\\r%d / %d' %(now, add_n), end='')\n",
    "\n",
    "            x = random.randint(0,len(data)-1)\n",
    "            d = data[x]\n",
    "            new_d = dict()\n",
    "            new_d[\"tokens\"] = []\n",
    "            new_d[\"tags\"] = data[x][\"tags\"]\n",
    "\n",
    "            cnt = 0\n",
    "\n",
    "            for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "                if tag != \"O\":\n",
    "                    if tag in alterna_labels:\n",
    "                        token=random_choice_token(alterna_labels[tag]-{token})\n",
    "                        cnt = cnt + 1\n",
    "                    new_d[\"tokens\"].append(token)\n",
    "\n",
    "            data_f.write(json.dumps(new_d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "            memo_f.write(str(x)+\"\\t\"+\" \".join(d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\"\\n\")\n",
    "\n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b1b2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataname in [\"f\"]:\n",
    "#     for filename in [\"dev\",\"train\"]:\n",
    "#         Simple_BI(dataname,filename,2)\n",
    "#         Simple_BI(dataname,filename,5)\n",
    "#         Simple_BI(dataname,filename,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03873799",
   "metadata": {},
   "source": [
    "# TreePos-BI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa491372",
   "metadata": {},
   "source": [
    "・f直下のデータを取得し，各文を表すdict型を一つずつ取り出しd[\"id\"]という文書番号を表すキーと値を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef124dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "filename = \"dev\"\n",
    "# filename = \"train\"\n",
    "with open(\"data/f/\"+filename+\".json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "        \n",
    "for i,d in enumerate(data):\n",
    "    d[\"id\"] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a90a5",
   "metadata": {},
   "source": [
    "・idのキーを持つdict型の要素を500ずつまとめたjsonとし，data/f_idに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f1d2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "for i in range(0, len(data), n):\n",
    "    with open(\"data/f_id/\"+filename+\"_\"+str(i)+\"-\"+str(i+n-1)+\".json\",\"w\") as f:\n",
    "        for d in data[i: i+n]:\n",
    "                f.write(json.dumps(d))\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c062914",
   "metadata": {},
   "source": [
    "・f_id内の500ずつデータが入った細切れのjsonを全て統合してtrain.json,dev,jsonへ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a3144db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"data/f_id/\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "dev_data_list = []\n",
    "train_data_list = []\n",
    "\n",
    "for filename in filenames:\n",
    "    print(filename)\n",
    "    with open(path + filename,'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    if \"dev\" in filename:   \n",
    "        dev_data_list = dev_data_list + data\n",
    "    else:\n",
    "        train_data_list = train_data_list + data\n",
    "        \n",
    "with open(path + \"dev.json\", 'w') as ef:\n",
    "    for d in dev_data_list:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(path + \"train.json\", 'w') as ef:\n",
    "    for d in train_data_list:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031b410a",
   "metadata": {},
   "source": [
    "・ 形態素情報付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be56c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import *\n",
    "import nltk\n",
    "from nltk.tree import *\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff5f670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7991/2297462148.py:8: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
      "/tmp/ipykernel_7991/2297462148.py:10: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n"
     ]
    }
   ],
   "source": [
    "java_path = \"jre1.8.0_333/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "parser =  'stanford-corenlp/stanford-parser-full-2020-11-17/stanford-parser.jar'\n",
    "parser_model = 'stanford-corenlp/stanford-corenlp-4.2.0-models-english.jar'\n",
    "\n",
    "#POSタグの分析用\n",
    "pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "#係り受け関係の分析用\n",
    "dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "\n",
    "def POSTagAnalysis(text):\n",
    "    out = pos.raw_parse(text)\n",
    "    out = list(out)\n",
    "    tree = out[0]\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fc661",
   "metadata": {},
   "source": [
    "・filenameを受け取り，構文解析情報を付与し，Stanford_coreNLP内部に構文情報とともに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "054d5146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def TreePOS(filename):\n",
    "    data = []\n",
    "    with open(\"data/f_id/\"+filename,\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    new_document=[]\n",
    "    document_trees = []\n",
    "    print(filename)\n",
    "    \n",
    "    #タプル型で複数引数を渡す\n",
    "    for i,s in enumerate(data):\n",
    "        print('\\r%d / %d' %(i, len(data)), end='')\n",
    "        \n",
    "        text = \" \".join(s[\"tokens\"])\n",
    "        tree = POSTagAnalysis(text)\n",
    "        document_trees.append(tree)\n",
    "        new_d = dict()\n",
    "\n",
    "        new_d[\"tokens\"] = []\n",
    "        new_d[\"tree_pos\"] = []\n",
    "        new_d[\"tags\"] = []\n",
    "\n",
    "        for i in range(len(tree.pos())):\n",
    "            new_d[\"tokens\"].append(tree.pos()[i][0])\n",
    "            new_d[\"tree_pos\"].append(tree.pos()[i][1])\n",
    "\n",
    "        new_d[\"tags\"]=s[\"tags\"]\n",
    "        new_d[\"id\"] = s[\"id\"]\n",
    "        new_document.append(new_d)\n",
    "        del tree\n",
    "        \n",
    "        \n",
    "    with open(\"data/Stanford_coreNLP/\"+filename+\"_tree.binaryfile\", 'wb') as f:\n",
    "        pickle.dump(document_trees,f)\n",
    "    with open(\"data/Stanford_coreNLP_TreePos/\"+filename+\"_data.binaryfile\", 'wb') as f:\n",
    "        pickle.dump(new_document,f)\n",
    "\n",
    "    print(\"\\nSAVED ==> \"+filename+\"_tree.binaryfile\")\n",
    "    print(\"SAVED ==> \"+filename+\"_data.binaryfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c2b864",
   "metadata": {},
   "source": [
    "・errorがなくなるまで繰り返し構文解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8874ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filenames = os.listdir(\"data/f_id/\")\n",
    "errors = []\n",
    "all_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d87f2993",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filenames:\n",
    "    try:\n",
    "        TreePOS(filename)\n",
    "    except:\n",
    "        errors.append(filename)\n",
    "        \n",
    "all_errors.append(errors)\n",
    "filenames = errors\n",
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a7c116a4",
   "metadata": {},
   "outputs": [],
   "source": [
    " with open(\"data/TreePosError.binaryfile\", 'wb') as f:\n",
    "        pickle.dump(errors,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94174b32",
   "metadata": {},
   "source": [
    "・ Stanford_coreNLP内を全て統合して{train|dev}.json，{train_tree|dev_tree}.binaryfileへ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c866681d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tree_path = \"data/Stanford_coreNLP/\"\n",
    "\n",
    "dev_tree_list = []\n",
    "train_tree_list = []\n",
    "\n",
    "filenames = os.listdir(tree_path)\n",
    "for filename in filenames:\n",
    "    with open(tree_path + filename,'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        if \"dev\" in filename:   \n",
    "            dev_tree_list = dev_tree_list + data\n",
    "        else:\n",
    "            train_tree_list = train_tree_list + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3fbaa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "data_path = \"data/Stanford_coreNLP_TreePos/\"\n",
    "\n",
    "dev_data_list = []\n",
    "train_data_list = []\n",
    "\n",
    "filenames = os.listdir(data_path)\n",
    "for filename in filenames:\n",
    "    with open(data_path + filename,'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        if \"dev\" in filename:   \n",
    "            dev_data_list = dev_data_list + data\n",
    "        else:\n",
    "            train_data_list = train_data_list + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd0433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(data_path + \"dev.json\", 'w') as ef:\n",
    "    for d in dev_data_list:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "with open(data_path + \"train.json\", 'w') as ef:\n",
    "    for d in train_data_list:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9ee91cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tree_path+\"dev_tree.binaryfile\", 'wb') as f:\n",
    "    pickle.dump(dev_tree_list,f)\n",
    "    \n",
    "with open(tree_path+\"train_tree.binaryfile\", 'wb') as f:\n",
    "    pickle.dump(train_tree_list,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef294eb",
   "metadata": {},
   "source": [
    "・ f_id内を全て統合してtrain.json,dev,jsonへ\n",
    "\n",
    "・ 上の位置にコピーを動かし済み．実行後削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "856539fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"data/f_id/\"\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "dev_origin_list = []\n",
    "train_origin_list = []\n",
    "\n",
    "for filename in filenames:\n",
    "    data = []\n",
    "    with open(\"data/f_id/\"+filename,\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        if \"dev\" in filename:\n",
    "            dev_origin_list = dev_origin_list + data\n",
    "        else:\n",
    "            train_origin_list = train_origin_list + data\n",
    "        \n",
    "with open(path + \"dev.json\", 'w') as ef:\n",
    "    for d in dev_origin_list:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(path + \"train.json\", 'w') as ef:\n",
    "    for d in train_origin_list:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e858ea8e",
   "metadata": {},
   "source": [
    "・ tokenが構文解析器入力前と入力後で異なるものを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "02fe7a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_origin_list = []\n",
    "with open(\"data/f_id/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_origin_list.append(json.loads(line))\n",
    "        \n",
    "train_origin_list = []\n",
    "with open(\"data/f_id/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_origin_list.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c24bda83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18823\n",
      "131766\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_data_list))\n",
    "print(len(train_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3ff6971a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18823\n",
      "131766\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_origin_list))\n",
    "print(len(train_origin_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d12e1d6",
   "metadata": {},
   "source": [
    "・dev_bebigger.json 　　dev_besmaller.json 　　dev_same.json を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "c7dfc195",
   "metadata": {},
   "outputs": [],
   "source": [
    "bebigger = []\n",
    "besmaller = []\n",
    "same=[] #id tokens tags\n",
    "for d in dev_origin_list:\n",
    "    #構文解析によってtoken数が変化する問題に対処する必要がある\n",
    "    #解析前tokenと解析後tokenを比較し，等しくない場合は\n",
    "    #解析前後の大小関係ごとにリストに格納し，結果の比較用データとする\n",
    "    \n",
    "    new_data = data_d[d[\"id\"]]\n",
    "    d[\"new_tokens\"] = new_data[\"tokens\"]\n",
    "    if len(d['tokens']) < len(d[\"new_tokens\"]):\n",
    "        bebigger.append(d)\n",
    "    elif len(d['tokens']) > len(d[\"new_tokens\"]):\n",
    "        besmaller.append(data_d[d[\"id\"]])\n",
    "    else:\n",
    "        same.append(data_d[d[\"id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "08c858cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bebigger:3833\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_bebigger.json\", 'w') as ef:\n",
    "    for d in bebigger:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "# besmaller:1\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_besmaller.json\", 'w') as ef:\n",
    "    for d in besmaller:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "#same:14989\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_same.json\", 'w') as ef:\n",
    "    for d in same:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0baf09",
   "metadata": {},
   "source": [
    "・train_bebigger.json 　　train_besmaller.json 　　train_same.json を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "e4367a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d = dict()\n",
    "for d in train_data_list:\n",
    "    data_d[d[\"id\"]] = d\n",
    "\n",
    "bebigger = []\n",
    "besmaller = []\n",
    "same=[] #id tokens tags\n",
    "for d in train_origin_list:\n",
    "    #構文解析によってtoken数が変化する問題に対処する必要がある\n",
    "    #解析前tokenと解析後tokenを比較し，等しくない場合は\n",
    "    #解析前後の大小関係ごとにリストに格納し，結果の比較用データとする\n",
    "    \n",
    "    new_data = data_d[d[\"id\"]]\n",
    "    d[\"new_tokens\"] = new_data[\"tokens\"]\n",
    "    d[\"new_tags\"] = new_data[\"tags\"]\n",
    "    d[\"tree_pos\"] = new_data[\"tree_pos\"]\n",
    "    if len(d['tokens']) < len(d[\"new_tokens\"]):\n",
    "        bebigger.append(d)\n",
    "    elif len(d['tokens']) > len(d[\"new_tokens\"]):\n",
    "        besmaller.append(d)\n",
    "    else:\n",
    "        same.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "728faa47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104918"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "29666417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bebigger:26841\n",
    "with open(\"data/Stanford_coreNLP_interm/train_bebigger.json\", 'w') as ef:\n",
    "    for d in bebigger:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "# besmaller:7\n",
    "with open(\"data/Stanford_coreNLP_interm/train_besmaller.json\", 'w') as ef:\n",
    "    for d in besmaller:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "#same:104918\n",
    "with open(\"data/Stanford_coreNLP_interm/train_same.json\", 'w') as ef:\n",
    "    for d in same:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfdfe61",
   "metadata": {},
   "source": [
    "・{dev|train}_bebiggerについて，新たなトークン数のラベルに合わせる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c0733a",
   "metadata": {},
   "source": [
    "・{dev|train}_bebiggerにnew_tagsを付与．うまくいったもの，そうでないもので{dev|train}_successと{dev|train}_errorに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "0c85588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#単純に細分化問題のみに対応して新たなタグを付与し，うまくいったものとそうでないものを返す関数\n",
    "def GiveNewTag(input_list):\n",
    "    success = []\n",
    "    error = []\n",
    "    for d in input_list:\n",
    "        new_tags = [\"\"]*len(d[\"new_tokens\"])\n",
    "\n",
    "        old_i = 0\n",
    "        new_i = 0\n",
    "        try:\n",
    "            for _ in d[\"new_tokens\"]:\n",
    "                if d[\"new_tokens\"][new_i]==d[\"tokens\"][old_i]:\n",
    "                    new_tags[new_i] = d[\"tags\"][old_i]\n",
    "                    old_i += 1\n",
    "                    new_i += 1\n",
    "                else:\n",
    "                    if d[\"new_tokens\"][new_i] in d[\"tokens\"][old_i]:\n",
    "                        new_tags[new_i] = d[\"tags\"][old_i]\n",
    "                        new_i += 1\n",
    "                    else:\n",
    "                        old_i += 1\n",
    "                        if d[\"new_tokens\"][new_i]==d[\"tokens\"][old_i]:\n",
    "                            new_tags[new_i] = d[\"tags\"][old_i]\n",
    "                            old_i += 1\n",
    "                            new_i += 1\n",
    "\n",
    "\n",
    "            if len(new_tags)==len(d[\"new_tokens\"]):\n",
    "                d[\"new_tags\"] = new_tags\n",
    "                success.append(d)\n",
    "            else:\n",
    "                error.append(d)\n",
    "        except:\n",
    "            error.append(d)\n",
    "    return [success,error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "3374af2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev=>success/error\n",
    "import json\n",
    "        \n",
    "dev_bebigger = []\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_bebigger.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_bebigger.append(json.loads(line))\n",
    "\n",
    "dev_success,dev_error = GiveNewTag(dev_bebigger)#3001,#820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "f5e5b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train=>success/error\n",
    "import json\n",
    "\n",
    "train_bebigger = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train_bebigger.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_bebigger.append(json.loads(line))\n",
    "        \n",
    "train_success,train_error = GiveNewTag(train_bebigger)#21006,#5835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "45a61627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5835"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae288257",
   "metadata": {},
   "source": [
    "・{dev|train}_errorのうち括弧が\"-LRB-\",\"-RRB-\"になる問題を解決　うまくいったものを{dev|train}_PBに，そうでないもの{dev|train}_error_PBへ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "c4f66637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#細分化問題+タグ変更問題に対応，新たなタグを付与し，うまくいったものとそうでないものを返す関数\n",
    "def GiveNewTag_PB(input_list):\n",
    "    success = []\n",
    "    error = []\n",
    "    for d in input_list:\n",
    "        new_tags = [\"\"]*len(d[\"new_tokens\"])\n",
    "\n",
    "        old_i = 0\n",
    "        new_i = 0\n",
    "        try:\n",
    "            for _ in d[\"new_tokens\"]:\n",
    "                if d[\"new_tokens\"][new_i]==d[\"tokens\"][old_i] or d[\"new_tokens\"][new_i] in (\"-LRB-\",\"-RRB-\"):\n",
    "                    new_tags[new_i] = d[\"tags\"][old_i]\n",
    "                    old_i += 1\n",
    "                    new_i += 1\n",
    "                else:\n",
    "                    if d[\"new_tokens\"][new_i] in d[\"tokens\"][old_i]:\n",
    "                        new_tags[new_i] = d[\"tags\"][old_i]\n",
    "                        new_i += 1\n",
    "                    else:\n",
    "                        old_i += 1\n",
    "                        if d[\"new_tokens\"][new_i]==d[\"tokens\"][old_i]:\n",
    "                            new_tags[new_i] = d[\"tags\"][old_i]\n",
    "                            old_i += 1\n",
    "                            new_i += 1\n",
    "\n",
    "\n",
    "            if len(new_tags)==len(d[\"new_tokens\"]):\n",
    "                d[\"new_tags\"] = new_tags\n",
    "                success.append(d)\n",
    "            else:\n",
    "                error.append(d)\n",
    "        except:\n",
    "            error.append(d)\n",
    "    return [success,error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "72f5b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_error=>PB/error_PB\n",
    "dev_PB,dev_error_PB= GiveNewTag_PB(dev_error)#820,#12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "5836c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_error=>PB/error_PB\n",
    "train_PB,train_error_PB = GiveNewTag_PB(train_error)#820,#12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fad028",
   "metadata": {},
   "source": [
    "・dev_error_PBを修正"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3cd0e",
   "metadata": {},
   "source": [
    "・Dod. . のようにカンマが被る問題を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "63e8750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#重複カンマ削除\n",
    "def Remove_comme(input_list):\n",
    "    for d in input_list:\n",
    "        remake_tokens = []\n",
    "        for i in range(len(d[\"new_tokens\"])):\n",
    "            token = d[\"new_tokens\"][i]\n",
    "            if token != \".\":\n",
    "                remake_tokens.append(token)\n",
    "        if d[\"tokens\"][-1] == \".\":\n",
    "            remake_tokens.append(\".\")\n",
    "        d[\"new_tokens\"] = remake_tokens\n",
    "    return input_list\n",
    "\n",
    "dev_error_PB= Remove_comme(dev_error_PB)\n",
    "train_error_PB= Remove_comme(train_error_PB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d97048e",
   "metadata": {},
   "source": [
    "・カンマを削除したnew_tokenに対し改めてnew_tagsを付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "2500c2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_comma,dev_error_comma = GiveNewTag_PB(dev_error_PB)\n",
    "train_comma,train_error_comma = GiveNewTag_PB(train_error_PB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "bb412584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#devを保存\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_bebigger_only_3001.json\", 'w') as ef:\n",
    "    for d in dev_success:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/dev_BP_only_820.json\", 'w') as ef:\n",
    "    for d in dev_PB:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/dev_comma_9.json\", 'w') as ef:\n",
    "    for d in dev_comma:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "                \n",
    "with open(\"data/Stanford_coreNLP_interm/dev_fraction_3.json\", 'w') as ef:\n",
    "    for d in dev_error_comma:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "\n",
    "#trainを保存\n",
    "with open(\"data/Stanford_coreNLP_interm/train_bebigger_only_26841.json\", 'w') as ef:\n",
    "    for d in train_success:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/train_BP_only_5750.json\", 'w') as ef:\n",
    "    for d in train_PB:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/train_comma_61.json\", 'w') as ef:\n",
    "    for d in train_comma:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "                \n",
    "with open(\"data/Stanford_coreNLP_interm/train_fraction_24.json\", 'w') as ef:\n",
    "    for d in train_error_comma:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88d36f",
   "metadata": {},
   "source": [
    "・{train|dev}_sameにエラーが無いかを調査"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "e367d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_same = []\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_same.json\",\"r\") as df:\n",
    "    for line in df:\n",
    "        dev_same.append(json.loads(line))\n",
    "\n",
    "train_same = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train_same.json\", 'r') as tf:\n",
    "    for line in tf:\n",
    "        train_same.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2f9d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_same_unmatch = []\n",
    "dev_same_match = []\n",
    "for d in dev_same:\n",
    "    if d[\"tokens\"] != d[\"new_tokens\"]:\n",
    "        dev_same_unmatch.append(d)\n",
    "    else:\n",
    "        dev_same_match.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e4a32d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_same_unmatch = []\n",
    "train_same_match = []\n",
    "for d in train_same:\n",
    "    if d[\"tokens\"] != d[\"new_tokens\"]:\n",
    "        train_same_unmatch.append(d)\n",
    "    else:\n",
    "        train_same_match.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561599e",
   "metadata": {},
   "source": [
    "・dev_same_match,train_same_matchを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "232d4600",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Stanford_coreNLP_interm/dev_same_match_13024.json\", 'w') as ef:\n",
    "    for d in dev_same_match:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/train_same_match_91892.json\", 'w') as ef:\n",
    "    for d in train_same_match:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca5171a",
   "metadata": {},
   "source": [
    "・dev_same_unmatch,train_same_unmatchのうち，記号のtoken変化とそうでないものに分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d85119f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_unmatch_PB,dev_unmatch_error_PB = GiveNewTag_PB(dev_same_unmatch)\n",
    "train_unmatch_PB,train_unmatch_error_PB = GiveNewTag_PB(train_same_unmatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df4b18",
   "metadata": {},
   "source": [
    "・記号変化対処で解決できたものを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "ab92b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Stanford_coreNLP_interm/dev_unmatch_PB_1918.json\", 'w') as ef:\n",
    "    for d in dev_unmatch_PB:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/train_unmatch_PB_13022.json\", 'w') as ef:\n",
    "    for d in train_unmatch_PB:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c346c6c3",
   "metadata": {},
   "source": [
    "・対応できなかったもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "6634e7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Stanford_coreNLP_interm/dev_unmatch_error_PB_2.json\", 'w') as ef:\n",
    "    for d in dev_unmatch_error_PB:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/train_unmatch_error_PB_2.json\", 'w') as ef:\n",
    "    for d in train_unmatch_error_PB:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deb7dcc",
   "metadata": {},
   "source": [
    "・以上，これまでに出てきた処理しきれないデータについて目視で確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "5b0e4537",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_besmaller = []\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_besmaller.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_besmaller.append(json.loads(line))\n",
    "\n",
    "train_besmaller = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train_besmaller.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_besmaller.append(json.loads(line))\n",
    "        \n",
    "dev_fraction_3 = []\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_fraction_3.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_fraction_3.append(json.loads(line))\n",
    "\n",
    "train_fraction_24 = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train_fraction_24.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_fraction_24.append(json.loads(line))\n",
    "        \n",
    "dev_unmatch_error_PB = []\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_unmatch_error_PB_2.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_unmatch_error_PB.append(json.loads(line))\n",
    "\n",
    "train_unmatch_error_PB = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train_unmatch_error_PB_2.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_unmatch_error_PB.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "b8c1908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dev_unmatch_error_PB => OK\n",
    "# for i in range(2):\n",
    "#     print(len(dev_unmatch_error_PB[i][\"tokens\"]),\"_\".join(dev_unmatch_error_PB[i][\"tokens\"]))\n",
    "#     print(len(dev_unmatch_error_PB[i][\"tokens\"]),\"_\".join(dev_unmatch_error_PB[i][\"new_tokens\"]))\n",
    "#     print()\n",
    "    \n",
    "# # train_unmatch_error_PB = OK\n",
    "# for i in range(2):\n",
    "#     print(len(train_unmatch_error_PB[i][\"tokens\"]),\"_\".join(train_unmatch_error_PB[i][\"tokens\"]))\n",
    "#     print(len(train_unmatch_error_PB[i][\"tokens\"]),\"_\".join(train_unmatch_error_PB[i][\"new_tokens\"]))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "2d7bf604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_tagsを目視で入力するための出力関数\n",
    "def Add_new_tags(d):\n",
    "    print(\"ID:\",d[\"id\"])\n",
    "    new_tags = [\"\"]*len(d[\"new_tokens\"])\n",
    "    \n",
    "    olg_token_and_tag = \"\"\n",
    "    for i in range(len(d[\"tokens\"])):\n",
    "        olg_token_and_tag += \" \"+d[\"tokens\"][i]+\"_ \"+d[\"tags\"][i]+\"  \"\n",
    "    print('\\r%s' %(olg_token_and_tag), end='')\n",
    "    \n",
    "    for j in range(len(d[\"new_tokens\"])):\n",
    "        t = input(d[\"new_tokens\"][j]+\" = \")\n",
    "        if t == \"\":\n",
    "            new_tags[j] = \"O\"\n",
    "        else:\n",
    "             new_tags[j] = t\n",
    "    d[\"new_tags\"] = new_tags \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40112dc",
   "metadata": {},
   "source": [
    "・devにおいて目視チェックが必要なものを全て確認し，dev_checked_1+3+2.jsonとして保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "07bcdf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_checked = []\n",
    "\n",
    "# for d in dev_fraction_3:\n",
    "#     d = Add_new_tags(d)\n",
    "#     dev_checked.append(d)\n",
    "\n",
    "# for d in dev_besmaller:\n",
    "#     d = Add_new_tags(d)\n",
    "#     dev_checked.append(d)\n",
    "\n",
    "# for d in dev_unmatch_error_PB:\n",
    "#     dev_checked.append(d)\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_checked_1+3+2.json\", 'w') as ef:\n",
    "    for d in dev_checked:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b20d4",
   "metadata": {},
   "source": [
    "・trainにおいて目視チェックが必要なものを全て確認し，train_checked_7+24+2.jsonとして保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "d4d92c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_checked = []#24\n",
    "# for d in train_fraction_24:\n",
    "#     d = Add_new_tags(d)\n",
    "#     train_checked.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "cda8804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_checked = []\n",
    "# target_checked = []\n",
    "# for d in train_checked:\n",
    "#     if d[\"id\"] in (44882,58227,94353,94720,99241,101971,119067):\n",
    "#         target_checked.append(d)\n",
    "#     else:\n",
    "#         other_checked.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "74a294d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_checked_remake = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "45abe3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 6 #(0～7)\n",
    "# d = target_checked[i]#44882,58227,94353,94720,99241,101971,119067\n",
    "# print(i)\n",
    "# print(d[\"id\"])\n",
    "# new_d = Add_new_tags(d)\n",
    "# target_checked_remake.append(new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "917f6cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_besmaller_checked = []\n",
    "# for d in train_besmaller:\n",
    "#     new_d = Add_new_tags(d)\n",
    "#     train_besmaller_checked.append(new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea11316",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unmatch_error_PB = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train_unmatch_error_PB_2.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_unmatch_error_PB.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "1a735e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_checked = other_checked + target_checked_remake + train_besmaller_checked + train_unmatch_error_PB\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_checked_7+24+2.json\", 'w') as ef:\n",
    "    for d in new_train_checked:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846abf03",
   "metadata": {},
   "source": [
    "・{train|dev}ともに処理済みのデータを全て結合し，保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "756730b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = []#18823\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_bebigger_only_3001.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_BP_only_820.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_comma_9.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_checked_1+3+2.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_same_match_13024.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_unmatch_PB_1918.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/dev.json\", 'w') as ef:\n",
    "    for d in dev:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "4e7006c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []#131766\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_bebigger_only_26841.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_BP_only_5750.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_comma_61.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_checked_7+24+2.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_same_match_91892.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_unmatch_PB_13022.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/train.json\", 'w') as ef:\n",
    "    for d in train:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee0924",
   "metadata": {},
   "source": [
    "## TreePos_Tagの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418bfba",
   "metadata": {},
   "source": [
    "・（ここから再開する場合は）token整形，統合結果であるdevとtrainの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "687a29c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = []\n",
    "with open(\"data/Stanford_coreNLP_interm/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "        \n",
    "train = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d3f16",
   "metadata": {},
   "source": [
    "・構文解析結果であるdev_dataとtrain_dataの読込み・辞書化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0543c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = []\n",
    "with open(\"data/Stanford_coreNLP_TreePos/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_data.append(json.loads(line))\n",
    "        \n",
    "train_data = []\n",
    "with open(\"data/Stanford_coreNLP_TreePos/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line))\n",
    "        \n",
    "dev_d = dict()\n",
    "for d in dev_data:\n",
    "    dev_d[d[\"id\"]] = d\n",
    "    \n",
    "train_d = dict()\n",
    "for d in train_data:\n",
    "    train_d[d[\"id\"]] = d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab77e1bf",
   "metadata": {},
   "source": [
    "・整形後にtree_posを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0aabd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dev:\n",
    "    d[\"tree_pos\"]=dev_d[d[\"id\"]][\"tree_pos\"]\n",
    "    if \"new_tokens\" in d:\n",
    "        d[\"tokens\"] = d[\"new_tokens\"]\n",
    "        del d[\"new_tokens\"]\n",
    "    if \"new_tags\" in d:\n",
    "        d[\"tags\"] = d[\"new_tags\"]\n",
    "        del d[\"new_tags\"]\n",
    "\n",
    "for d in train:\n",
    "    d[\"tree_pos\"]=train_d[d[\"id\"]][\"tree_pos\"]\n",
    "    if \"new_tokens\" in d:\n",
    "        d[\"tokens\"] = d[\"new_tokens\"]\n",
    "        del d[\"new_tokens\"]\n",
    "    if \"new_tags\" in d:\n",
    "        d[\"tags\"] = d[\"new_tags\"]\n",
    "        del d[\"new_tags\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789a1503",
   "metadata": {},
   "source": [
    "・#Stanford_coreNLP内に作成された構文解析済みデータから，新たにTreePos_Tagを持つデータを作成\n",
    "#全て統合してtrain.json,dev.jsonへ\n",
    "\n",
    "・tokensとtree_posが合わないものはerrorへ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d9de7a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stanford_coreNLP内に作成された構文解析済みデータから，新たにTreePos_Tagを持つデータを作成\n",
    "#全て統合してtrain.json,dev,jsonへ\n",
    "def ModelFormat(data):\n",
    "    new_data = []\n",
    "    error_data = []\n",
    "    for d in data:\n",
    "        new_d = dict()\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        if len(d[\"tokens\"]) == len(d[\"tree_pos\"]):\n",
    "            new_d[\"tags\"] = [str(d[\"tree_pos\"][j])+\"_\"+str(d[\"tags\"][j]) for j in range(len(d[\"tags\"]))]\n",
    "            new_data.append(new_d)\n",
    "        else:\n",
    "            error_data.append(new_d)\n",
    "    return [new_data,error_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ee2bacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dev,error_dev = ModelFormat(dev)\n",
    "with open(\"data/TreePos_BI/dev.json\", 'w', encoding=\"utf-8\") as wf:\n",
    "    for d in new_dev:\n",
    "        wf.write(json.dumps(d))\n",
    "        wf.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/TreePos_BI/dev_error.json\", 'w',encoding=\"utf-8\") as ef:\n",
    "    for d in error_dev:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "    \n",
    "new_train,error_train = ModelFormat(train)\n",
    "with open(\"data/TreePos_BI/train.json\", 'w',encoding=\"utf-8\") as wf:\n",
    "    for d in new_train:\n",
    "        wf.write(json.dumps(d))\n",
    "        wf.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/TreePos_BI/train_error.json\", 'w',encoding=\"utf-8\") as ef:\n",
    "    for d in error_train:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9fb14e",
   "metadata": {},
   "source": [
    "・データ拡張実装前準備\n",
    "\n",
    "・alterna_labelsの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4e794edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = []\n",
    "with open(\"data/TreePos_BI/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_data.append(json.loads(line))\n",
    "train_data = []\n",
    "with open(\"data/TreePos_BI/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line))\n",
    "        \n",
    "label_list = []\n",
    "for d in dev_data:\n",
    "    label_list.extend(d[\"tags\"])\n",
    "for d in train_data:\n",
    "    label_list.extend(d[\"tags\"])\n",
    "label_list = list(set([tags for tags in label_list if \"_O\" not in tags]))\n",
    "\n",
    "alterna_labels = {}\n",
    "for label in label_list:\n",
    "    alterna_labels[label] = set()\n",
    "for d in dev_data+train_data:\n",
    "    for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "        if tag in label_list:\n",
    "            alterna_labels[tag].add(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5592dd42",
   "metadata": {},
   "source": [
    "・ここで，alterna_labelsのうち，候補が2つ以下のものは削除（下の例の2・3行目）\n",
    "\n",
    "{'VBZ_person-actor': {\"'s\",  'Brest',  'Mature',  'Torrens',  'cameos',  'follows',  'heartthrobs',  'is',  'stars'},\n",
    "\n",
    "'RBR_art-film': {'Better', 'Worse'},\n",
    "\n",
    "'TO_person-athlete': {'to'}\n",
    "\n",
    "...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "89addcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_alterna_labels = dict()\n",
    "for k,v in alterna_labels.items():\n",
    "    if len(v)>2:\n",
    "        new_alterna_labels[k] = v\n",
    "\n",
    "alterna_labels = new_alterna_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf387e7",
   "metadata": {},
   "source": [
    "・データ拡張実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e964707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]\n",
    "\n",
    "def TreePos_BI(filename,n):\n",
    "    data = []\n",
    "    with open(\"data/TreePos_BI/\"+filename+\".json\",\"r\",encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    data_f_path = \"data/TreePos_BI_x\"+str(n)+\"/\"+filename+\".json\"\n",
    "    memo_f_path = \"log/TreePos_BI_x\"+str(n)+\"_\"+filename+\"_memo.txt\"\n",
    "    \n",
    "    \n",
    "    with open(data_f_path,\"w\",encoding=\"utf-8\") as data_f, open(memo_f_path,\"w\",encoding=\"utf-8\") as memo_f:\n",
    "        for d in data:\n",
    "            data_f.write(json.dumps(d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "        add_n = n*len(data)-len(data)\n",
    "        print()\n",
    "        print(filename,\"add_n\",add_n)\n",
    "        \n",
    "        for now in range(add_n):\n",
    "            if now%10 == 0:\n",
    "                print('\\r%d / %d' %(now, add_n), end='')\n",
    "\n",
    "            x = random.randint(0,len(data)-1)\n",
    "            d = data[x]\n",
    "            new_d = dict()\n",
    "            new_d[\"tokens\"] = []\n",
    "            new_d[\"tags\"] = data[x][\"tags\"]\n",
    "            cnt = 0\n",
    "\n",
    "            for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "                if tag in alterna_labels:\n",
    "                    token=random_choice_token(alterna_labels[tag]-{token})\n",
    "                    cnt = cnt + 1\n",
    "                new_d[\"tokens\"].append(token)\n",
    "\n",
    "            data_f.write(json.dumps(new_d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "            memo_f.write(str(x)+\"\\t\"+\" \".join(d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\"\\n\")\n",
    "\n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8473a8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev add_n 18813\n",
      "18810 / 18813"
     ]
    }
   ],
   "source": [
    "TreePos_BI(\"dev\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ff9e596c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train add_n 131705\n",
      "131700 / 131705"
     ]
    }
   ],
   "source": [
    "TreePos_BI(\"train\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e7a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ラベルの長さ違うエラー出たので確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f2c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "origin = []\n",
    "with open(\"data/f/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        origin.append(json.loads(line))\n",
    "        \n",
    "dev = []\n",
    "with open(\"data/Stanford_coreNLP/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "        \n",
    "error = []\n",
    "with open(\"data/Stanford_coreNLP/error_dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        error.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26c622d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18823\n",
      "29338\n",
      "7485\n"
     ]
    }
   ],
   "source": [
    "print(len(origin))\n",
    "print(len(dev))\n",
    "print(len(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ef1907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_num = []\n",
    "error_data = []\n",
    "for i,d in enumerate(data):\n",
    "    \n",
    "    if len(d[\"tokens\"])!=len(d[\"tags\"]):\n",
    "        error_num.append(i)\n",
    "        error_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6bdc752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7484"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "521bb244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73644"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1e706eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ce6847e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error_data[4][\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71c52f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error_data[4][\"tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b22423f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "error_data = []\n",
    "for path_sub in [\"dev\"]:\n",
    "    path = \"data/Stanford_coreNLP/\"\n",
    "\n",
    "    data = []\n",
    "    with open(\"data/f/dev.json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        \n",
    "    new_data = []\n",
    "    \n",
    "    for d in data:\n",
    "        new_d = dict()\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        if len(d[\"tokens\"])==len(d[\"tags\"]):\n",
    "            new_d[\"tags\"] = d[\"tags\"]\n",
    "            new_data.append(new_d)\n",
    "            \n",
    "        else:\n",
    "            error_data.append(d)\n",
    "            \n",
    "    with open(path + \"dev_2.json\",\"w\") as wf:\n",
    "        for d in new_data:\n",
    "            wf.write(json.dumps(d))\n",
    "            wf.write(\"\\n\")\n",
    "            \n",
    "    with open(path + \"error_dev_2.json\", 'w') as ef:\n",
    "        for d in error_data:\n",
    "            ef.write(json.dumps(d))\n",
    "            ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9b3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
