{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee95bf4",
   "metadata": {},
   "source": [
    "# 元々のjsonファイルを取得し，整形したものを出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c234cee",
   "metadata": {},
   "source": [
    "・ラベル名をtagsに統一\n",
    "\n",
    "・ラベルとして使用されていた数値IDを文字型に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6db445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\"words\": [\"CHICAGO\", \"AT\", \"ATLANTA\"], \"ner\": [\"B-ORG\", \"O\", \"B-LOC\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b297eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47c9c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/original/id2coarse_tags.json\",\"r\") as f:\n",
    "    label_coarse =json.loads(f.read())\n",
    "    \n",
    "with open(\"data/original/id2fine_tags.json\",\"r\") as f:\n",
    "    label_fine =json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d941d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_str_tag(tags):\n",
    "    new_tag = [label_coarse[str(i)] for i in tags]\n",
    "    return new_tag\n",
    "    \n",
    "def fine_str_tag(tags):\n",
    "    new_tag = [label_fine[str(i)] for i in tags]\n",
    "    return new_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a61a1431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in [\"train\",\"test\",\"dev\"]:\n",
    "    \n",
    "    data = []\n",
    "    with open(\"data/original/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    coarse = []\n",
    "    for d in data:\n",
    "        new_d = {}\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        new_d[\"tags\"] = coarse_str_tag(d[\"coarse_tags\"])\n",
    "        coarse.append(new_d)\n",
    "    with open(\"data/coarse/\"+filename+\".json\",\"w\") as f:\n",
    "        for d in coarse:\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    fine = []\n",
    "    for d in data:\n",
    "        new_d = {}\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        new_d[\"tags\"] = fine_str_tag(d[\"fine_tags\"])\n",
    "        fine.append(new_d)\n",
    "    with open(\"data/fine/\"+filename+\".json\",\"w\") as f:\n",
    "        for d in fine:\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e5b3fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0': 'O',\n",
       "  '1': 'art',\n",
       "  '2': 'building',\n",
       "  '3': 'event',\n",
       "  '4': 'location',\n",
       "  '5': 'organization',\n",
       "  '6': 'other',\n",
       "  '7': 'person',\n",
       "  '8': 'product'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_coarse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94ffa3",
   "metadata": {},
   "source": [
    "# Simple-BI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54559d2f",
   "metadata": {},
   "source": [
    "・BIラベルのみに着目したデータ拡張\n",
    "\n",
    "・交換率100％"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d0a37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]\n",
    "\n",
    "def Simple_BI(dataname,filename,n):\n",
    "    data = []\n",
    "    with open(\"data/\"+dataname+\"/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    alterna_labels = {}\n",
    "    if dataname == \"c\":\n",
    "        tag_json = \"data/original/id2coarse_tags.json\"\n",
    "    else:\n",
    "        tag_json = \"data/original/id2fine_tags.json\"\n",
    "    with open(tag_json,\"r\") as f:\n",
    "        label_coarse =json.loads(f.read())\n",
    "        label_list = list(label_coarse.values())[1:]\n",
    "    for label in label_list:\n",
    "        alterna_labels[label] = set()\n",
    "    for d in data:\n",
    "        for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "            if tag in label_list:\n",
    "                alterna_labels[tag].add(token)\n",
    "\n",
    "    data_f_path = \"data/\"+dataname+\"_BI_x\"+str(n)+\"/\"+filename+\".json\"\n",
    "    memo_f_path = \"log/\"+dataname+\"_BI_x\"+str(n)+\"_\"+filename+\"_memo.txt\"\n",
    "    \n",
    "    \n",
    "    with open(data_f_path,\"w\") as data_f, open(memo_f_path,\"w\") as memo_f:\n",
    "        for d in data:\n",
    "            data_f.write(json.dumps(d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "        add_n = n*len(data)-len(data)\n",
    "        print()\n",
    "        print(dataname,filename,\"add_n\",add_n)\n",
    "        \n",
    "        for now in range(add_n):\n",
    "            if now%10 == 0:\n",
    "                print('\\r%d / %d' %(now, add_n), end='')\n",
    "\n",
    "            x = random.randint(0,len(data)-1)\n",
    "            d = data[x]\n",
    "            new_d = dict()\n",
    "            new_d[\"tokens\"] = []\n",
    "            new_d[\"tags\"] = data[x][\"tags\"]\n",
    "\n",
    "            cnt = 0\n",
    "\n",
    "            for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "                if tag != \"O\":\n",
    "                    if tag in alterna_labels:\n",
    "                        token=random_choice_token(alterna_labels[tag]-{token})\n",
    "                        cnt = cnt + 1\n",
    "                    new_d[\"tokens\"].append(token)\n",
    "\n",
    "            data_f.write(json.dumps(new_d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "            memo_f.write(str(x)+\"\\t\"+\" \".join(d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\"\\n\")\n",
    "\n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34373959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataname in [\"f\"]:\n",
    "#     for filename in [\"dev\",\"train\"]:\n",
    "#         Simple_BI(dataname,filename,2)\n",
    "#         Simple_BI(dataname,filename,5)\n",
    "#         Simple_BI(dataname,filename,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d462a",
   "metadata": {},
   "source": [
    "# TreePos-BI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf61280",
   "metadata": {},
   "source": [
    "・ 形態素情報付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c698dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import *\n",
    "import nltk\n",
    "from nltk.tree import *\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121ab005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_269207/2297462148.py:8: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
      "/tmp/ipykernel_269207/2297462148.py:10: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n"
     ]
    }
   ],
   "source": [
    "java_path = \"jre1.8.0_333/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "parser =  'stanford-corenlp/stanford-parser-full-2020-11-17/stanford-parser.jar'\n",
    "parser_model = 'stanford-corenlp/stanford-corenlp-4.2.0-models-english.jar'\n",
    "\n",
    "#POSタグの分析用\n",
    "pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "#係り受け関係の分析用\n",
    "dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "\n",
    "def POSTagAnalysis(text):\n",
    "    out = pos.raw_parse(text)\n",
    "    out = list(out)\n",
    "    tree = out[0]\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db956fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "p = 127001#止まったところからスタート\n",
    "\n",
    "def TreePOS(filename):\n",
    "    data = []\n",
    "    with open(\"data/f/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    new_document=[]\n",
    "    document_trees = []\n",
    "    c = p\n",
    "    \n",
    "    #タプル型で複数引数を渡す\n",
    "    for s in data[p:]:\n",
    "        \n",
    "        text = \" \".join(s[\"tokens\"])\n",
    "        tree = POSTagAnalysis(text)\n",
    "        document_trees.append(tree)\n",
    "        new_d = dict()\n",
    "        new_d[\"tokens\"] = []\n",
    "        new_d[\"tree_pos\"] = []\n",
    "        new_d[\"tags\"] = []\n",
    "\n",
    "        for i in range(len(tree.pos())):\n",
    "            new_d[\"tokens\"].append(tree.pos()[i][0])\n",
    "            new_d[\"tree_pos\"].append(tree.pos()[i][1])\n",
    "\n",
    "        new_d[\"tags\"]=s[\"tags\"]\n",
    "        new_document.append(new_d)\n",
    "        del tree\n",
    "        \n",
    "        print('\\r%d / %d' %(c, len(data)), end='')\n",
    "        \n",
    "        if (c % 1000 == 0 and c!=0) or c == len(data):\n",
    "            with open(\"data/Stanford_coreNLP/\"+filename+\"_\"+str(c)+\"_tree.binaryfile\", 'wb') as f:\n",
    "                pickle.dump(document_trees,f)\n",
    "            with open(\"data/Stanford_coreNLP/\"+filename+\"_\"+str(c)+\"_data.binaryfile\", 'wb') as f:\n",
    "                pickle.dump(new_document,f)\n",
    "            \n",
    "            print(\"\\nSAVED ==> \"+filename+\"_\"+str(c)+\"_tree.binaryfile\")\n",
    "            print(\"SAVED ==> \"+filename+\"_\"+str(c)+\"_data.binaryfile\")\n",
    "        c = c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f9dd3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000 / 131766\n",
      "SAVED ==> train_128000_tree.binaryfile\n",
      "SAVED ==> train_128000_data.binaryfile\n",
      "129000 / 131766\n",
      "SAVED ==> train_129000_tree.binaryfile\n",
      "SAVED ==> train_129000_data.binaryfile\n",
      "130000 / 131766\n",
      "SAVED ==> train_130000_tree.binaryfile\n",
      "SAVED ==> train_130000_data.binaryfile\n",
      "131000 / 131766\n",
      "SAVED ==> train_131000_tree.binaryfile\n",
      "SAVED ==> train_131000_data.binaryfile\n",
      "131765 / 131766"
     ]
    }
   ],
   "source": [
    "for filename in [\"train\"]:\n",
    "# for filename in [\"dev\"]:\n",
    "    TreePOS(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b4e22",
   "metadata": {},
   "source": [
    "・pickleファイルを統合\n",
    "\n",
    "・以下のtokensを持つデータに対してエラーが出現したため(tokensとtagsの長さ違い)，errorを避けて保存\n",
    "\n",
    "['The',  'Philippine',  'peso',  'sign',  '-LRB-',  '-RRB-',  'is',  'the',  'currency',  'symbol',  'used',  'for',  'Philippine',  'peso',  ',',  'the',  'official',  'currency',  'of',  'the',  'Philippines',  '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "66f78b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for path_sub in [\"dev\",\"train_1-51k\",\"train_52-81k\",\"train_82-131k\"]\n",
    "    path = \"data/Stanford_coreNLP/\"\n",
    "    files = os.listdir(path+path_sub+\"/\")\n",
    "    data_list = []\n",
    "    tree_list = []\n",
    "\n",
    "    for filename in files:\n",
    "        with open(path+path_sub+\"/\"+filename,'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        if \"data\" in filename:\n",
    "            data_list = data_list + data\n",
    "        else:\n",
    "            tree_list = tree_list + data\n",
    "\n",
    "    with open(path+path_sub+\"_data.binaryfile\", 'wb') as f:\n",
    "        pickle.dump(data_list,f)\n",
    "\n",
    "    with open(path+path_sub+\"_tree.binaryfile\", 'wb') as f:\n",
    "        pickle.dump(tree_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8b39d45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_data = []\n",
    "for path_sub in [\"dev\"]:\n",
    "    path = \"data/Stanford_coreNLP/\"\n",
    "\n",
    "    with open(path+path_sub+\"_data.binaryfile\",'rb') as rf:\n",
    "        data = pickle.load(rf)\n",
    "        \n",
    "    new_data = []\n",
    "    \n",
    "    for d in data:\n",
    "        new_d = dict()\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        try:\n",
    "            new_d[\"tags\"] = [str(d[\"tree_pos\"][j])+\"_\"+str(d[\"tags\"][j]) for j in range(len(d[\"tags\"]))]\n",
    "            new_data.append(new_d)\n",
    "            \n",
    "        except:\n",
    "            error_data.append(d)\n",
    "            \n",
    "    with open(path + \"dev.json\",\"w\") as wf:\n",
    "        for d in new_data:\n",
    "            wf.write(json.dumps(d))\n",
    "            wf.write(\"\\n\")\n",
    "            \n",
    "    with open(path + \"error_dev.json\", 'w') as ef:\n",
    "        for d in error_data:\n",
    "            ef.write(json.dumps(d))\n",
    "            ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d3217b9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "error_data = []\n",
    "data = []\n",
    "path = \"data/Stanford_coreNLP/\"\n",
    "\n",
    "for path_sub in [\"train_1-51k\",\"train_52-81k\",\"train_82-131k\"]:\n",
    "    with open(path+path_sub+\"_data.binaryfile\",'rb') as rf:\n",
    "        addition = pickle.load(rf)\n",
    "    data= data+addition\n",
    "        \n",
    "new_data = []\n",
    "for d in data:\n",
    "    new_d = dict()\n",
    "    new_d[\"tokens\"] = d[\"tokens\"]\n",
    "    try:\n",
    "        new_d[\"tags\"] = [str(d[\"tree_pos\"][j])+\"_\"+str(d[\"tags\"][j]) for j in range(len(d[\"tags\"]))]\n",
    "        new_data.append(new_d)\n",
    "\n",
    "    except:\n",
    "        error_data.append(d)\n",
    "\n",
    "with open(path + \"train.json\",\"w\") as wf:\n",
    "    for d in new_data:\n",
    "        wf.write(json.dumps(d))\n",
    "        wf.write(\"\\n\")\n",
    "\n",
    "with open(path + \"error_train.json\", 'w') as ef:\n",
    "    for d in error_data:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1a56b",
   "metadata": {},
   "source": [
    "・データ拡張実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0e6e2b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with open(\"data/Stanford_coreNLP/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line))\n",
    "\n",
    "dev_data = []\n",
    "with open(\"data/Stanford_coreNLP/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b58cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d9fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alterna_labels = {}\n",
    "if dataname == \"c\":\n",
    "    tag_json = \"data/original/id2coarse_tags.json\"\n",
    "else:\n",
    "    tag_json = \"data/original/id2fine_tags.json\"\n",
    "with open(tag_json,\"r\") as f:\n",
    "    label_coarse =json.loads(f.read())\n",
    "    label_list = list(label_coarse.values())[1:]\n",
    "for label in label_list:\n",
    "    alterna_labels[label] = set()\n",
    "for d in data:\n",
    "    for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "        if tag in label_list:\n",
    "            alterna_labels[tag].add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a79a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]\n",
    "\n",
    "def TreePos_BI(dataname,filename,n):\n",
    "    data = []\n",
    "    with open(\"data/\"+dataname+\"/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    alterna_labels = {}\n",
    "    #~~~~~~~#\n",
    "    data_f_path = \"data/\"+dataname+\"_BI_x\"+str(n)+\"/\"+filename+\".json\"\n",
    "    memo_f_path = \"log/\"+dataname+\"_BI_x\"+str(n)+\"_\"+filename+\"_memo.txt\"\n",
    "    \n",
    "    \n",
    "    with open(data_f_path,\"w\") as data_f, open(memo_f_path,\"w\") as memo_f:\n",
    "        for d in data:\n",
    "            data_f.write(json.dumps(d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "        add_n = n*len(data)-len(data)\n",
    "        print()\n",
    "        print(dataname,filename,\"add_n\",add_n)\n",
    "        \n",
    "        for now in range(add_n):\n",
    "            if now%10 == 0:\n",
    "                print('\\r%d / %d' %(now, add_n), end='')\n",
    "\n",
    "            x = random.randint(0,len(data)-1)\n",
    "            d = data[x]\n",
    "            new_d = dict()\n",
    "            new_d[\"tokens\"] = []\n",
    "            new_d[\"tags\"] = data[x][\"tags\"]\n",
    "\n",
    "            cnt = 0\n",
    "\n",
    "            for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "                if tag != \"O\":\n",
    "                    if tag in alterna_labels:\n",
    "                        token=random_choice_token(alterna_labels[tag]-{token})\n",
    "                        cnt = cnt + 1\n",
    "                    new_d[\"tokens\"].append(token)\n",
    "\n",
    "            data_f.write(json.dumps(new_d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "            memo_f.write(str(x)+\"\\t\"+\" \".join(d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\"\\n\")\n",
    "\n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
