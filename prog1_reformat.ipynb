{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee95bf4",
   "metadata": {},
   "source": [
    "# 元々のjsonファイルを取得し，整形したものを出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c234cee",
   "metadata": {},
   "source": [
    "・ラベル名をtagsに統一\n",
    "\n",
    "・ラベルとして使用されていた数値IDを文字型に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6db445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\"words\": [\"CHICAGO\", \"AT\", \"ATLANTA\"], \"ner\": [\"B-ORG\", \"O\", \"B-LOC\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b297eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47c9c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/original/id2coarse_tags.json\",\"r\") as f:\n",
    "    label_coarse =json.loads(f.read())\n",
    "    \n",
    "with open(\"data/original/id2fine_tags.json\",\"r\") as f:\n",
    "    label_fine =json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d941d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_str_tag(tags):\n",
    "    new_tag = [label_coarse[str(i)] for i in tags]\n",
    "    return new_tag\n",
    "    \n",
    "def fine_str_tag(tags):\n",
    "    new_tag = [label_fine[str(i)] for i in tags]\n",
    "    return new_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a61a1431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in [\"train\",\"test\",\"dev\"]:\n",
    "    \n",
    "    data = []\n",
    "    with open(\"data/original/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    coarse = []\n",
    "    for d in data:\n",
    "        new_d = {}\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        new_d[\"tags\"] = coarse_str_tag(d[\"coarse_tags\"])\n",
    "        coarse.append(new_d)\n",
    "    with open(\"data/coarse/\"+filename+\".json\",\"w\") as f:\n",
    "        for d in coarse:\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    fine = []\n",
    "    for d in data:\n",
    "        new_d = {}\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        new_d[\"tags\"] = fine_str_tag(d[\"fine_tags\"])\n",
    "        fine.append(new_d)\n",
    "    with open(\"data/fine/\"+filename+\".json\",\"w\") as f:\n",
    "        for d in fine:\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e5b3fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0': 'O',\n",
       "  '1': 'art',\n",
       "  '2': 'building',\n",
       "  '3': 'event',\n",
       "  '4': 'location',\n",
       "  '5': 'organization',\n",
       "  '6': 'other',\n",
       "  '7': 'person',\n",
       "  '8': 'product'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_coarse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94ffa3",
   "metadata": {},
   "source": [
    "# BI-DA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54559d2f",
   "metadata": {},
   "source": [
    "・BIラベルのみに着目したデータ拡張\n",
    "\n",
    "・交換率100％"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d0a37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]\n",
    "\n",
    "def SimpleDA_BI(dataname,filename,n):\n",
    "    data = []\n",
    "    with open(\"data/\"+dataname+\"/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    alterna_labels = {}\n",
    "    if dataname == \"c\":\n",
    "        tag_json = \"data/original/id2coarse_tags.json\"\n",
    "    else:\n",
    "        tag_json = \"data/original/id2fine_tags.json\"\n",
    "    with open(tag_json,\"r\") as f:\n",
    "        label_coarse =json.loads(f.read())\n",
    "        label_list = list(label_coarse.values())[1:]\n",
    "    for label in label_list:\n",
    "        alterna_labels[label] = set()\n",
    "    for d in data:\n",
    "        for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "            if tag in label_list:\n",
    "                alterna_labels[tag].add(token)\n",
    "\n",
    "    data_f_path = \"data/\"+dataname+\"_BI_x\"+str(n)+\"/\"+filename+\".json\"\n",
    "    memo_f_path = \"log/\"+dataname+\"_BI_x\"+str(n)+\"_\"+filename+\"_memo.txt\"\n",
    "    \n",
    "    \n",
    "    with open(data_f_path,\"w\") as data_f, open(memo_f_path,\"w\") as memo_f:\n",
    "        for d in data:\n",
    "            data_f.write(json.dumps(d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "        add_n = n*len(data)-len(data)\n",
    "        print()\n",
    "        print(dataname,filename,\"add_n\",add_n)\n",
    "        \n",
    "        for now in range(add_n):\n",
    "            if now%10 == 0:\n",
    "                print('\\r%d / %d' %(now, add_n), end='')\n",
    "\n",
    "            x = random.randint(0,len(data)-1)\n",
    "            d = data[x]\n",
    "            new_d = dict()\n",
    "            new_d[\"tokens\"] = []\n",
    "            new_d[\"tags\"] = data[x][\"tags\"]\n",
    "\n",
    "            cnt = 0\n",
    "\n",
    "            for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "                if tag != \"O\":\n",
    "                    if tag in alterna_labels:\n",
    "                        token=random_choice_token(alterna_labels[tag]-{token})\n",
    "                        cnt = cnt + 1\n",
    "                    new_d[\"tokens\"].append(token)\n",
    "\n",
    "            data_f.write(json.dumps(new_d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "            memo_f.write(str(x)+\"\\t\"+\" \".join(d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\"\\n\")\n",
    "\n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34373959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataname in [\"f\"]:\n",
    "#     for filename in [\"dev\",\"train\"]:\n",
    "#         SimpleDA_BI(dataname,filename,2)\n",
    "#         SimpleDA_BI(dataname,filename,5)\n",
    "#         SimpleDA_BI(dataname,filename,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d462a",
   "metadata": {},
   "source": [
    "# TreePOS-DA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf61280",
   "metadata": {},
   "source": [
    "・ 形態素情報付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719ad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# gpu_ids = [0, 1]\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(f'cuda:{gpu_ids[0]}')\n",
    "# else:\n",
    "#     device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c698dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import *\n",
    "import nltk\n",
    "from nltk.tree import *\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121ab005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1343504/2297462148.py:8: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
      "/tmp/ipykernel_1343504/2297462148.py:10: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n"
     ]
    }
   ],
   "source": [
    "java_path = \"jre1.8.0_333/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "parser =  'stanford-corenlp/stanford-parser-full-2020-11-17/stanford-parser.jar'\n",
    "parser_model = 'stanford-corenlp/stanford-corenlp-4.2.0-models-english.jar'\n",
    "\n",
    "#POSタグの分析用\n",
    "pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "#係り受け関係の分析用\n",
    "dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "\n",
    "def POSTagAnalysis(text):\n",
    "    out = pos.raw_parse(text)\n",
    "    out = list(out)\n",
    "    tree = out[0]\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db956fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def TreePOS(filename):\n",
    "    data = []\n",
    "    with open(\"data/f/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    new_document=[]\n",
    "    document_trees = []\n",
    "    c = 18001\n",
    "    \n",
    "    #タプル型で複数引数を渡す\n",
    "    for s in data[18001:]:\n",
    "        \n",
    "        text = \" \".join(s[\"tokens\"])\n",
    "        tree = POSTagAnalysis(text)\n",
    "        document_trees.append(tree)\n",
    "        new_d = dict()\n",
    "        new_d[\"tokens\"] = []\n",
    "        new_d[\"tree_pos\"] = []\n",
    "        new_d[\"tags\"] = []\n",
    "\n",
    "        for i in range(len(tree.pos())):\n",
    "            new_d[\"tokens\"].append(tree.pos()[i][0])\n",
    "            new_d[\"tree_pos\"].append(tree.pos()[i][1])\n",
    "\n",
    "        new_d[\"tags\"]=s[\"tags\"]\n",
    "        new_document.append(new_d)\n",
    "        print('\\r%d / %d' %(c, len(data)), end='')\n",
    "        if (c % 3000 == 0 and c!=0) or c == len(data):\n",
    "            with open(\"data/Stanford_coreNLP/\"+filename+\"_\"+str(c)+\"_tree.binaryfile\", 'wb') as f:\n",
    "                pickle.dump(document_trees,f)\n",
    "            with open(\"data/Stanford_coreNLP/\"+filename+\"_\"+str(c)+\"_data.binaryfile\", 'wb') as f:\n",
    "                pickle.dump(new_document,f)\n",
    "            \n",
    "            print(\"\\nSAVED ==> \"+filename+\"_\"+str(c)+\"_tree.binaryfile\")\n",
    "            print(\"SAVED ==> \"+filename+\"_\"+str(c)+\"_data.binaryfile\")\n",
    "        c = c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9dd3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000 / 131766\n",
      "SAVED ==> train_21000_tree.binaryfile\n",
      "SAVED ==> train_21000_data.binaryfile\n",
      "24000 / 131766\n",
      "SAVED ==> train_24000_tree.binaryfile\n",
      "SAVED ==> train_24000_data.binaryfile\n",
      "27000 / 131766\n",
      "SAVED ==> train_27000_tree.binaryfile\n",
      "SAVED ==> train_27000_data.binaryfile\n",
      "30000 / 131766\n",
      "SAVED ==> train_30000_tree.binaryfile\n",
      "SAVED ==> train_30000_data.binaryfile\n",
      "32705 / 131766"
     ]
    }
   ],
   "source": [
    "for filename in [\"train\"]:\n",
    "# for filename in [\"dev\"]:\n",
    "    TreePOS(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c1fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
