{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e16134ba",
   "metadata": {},
   "source": [
    "# 元々のjsonファイルを取得し，整形したものを出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097dea7",
   "metadata": {},
   "source": [
    "・ラベル名をtagsに統一\n",
    "\n",
    "・ラベルとして使用されていた数値IDを文字型に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2913c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\"words\": [\"CHICAGO\", \"AT\", \"ATLANTA\"], \"ner\": [\"B-ORG\", \"O\", \"B-LOC\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b7e6e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "038cda8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/original/id2coarse_tags.json\",\"r\") as f:\n",
    "    label_coarse =json.loads(f.read())\n",
    "    \n",
    "with open(\"data/original/id2fine_tags.json\",\"r\") as f:\n",
    "    label_fine =json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b6a62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_str_tag(tags):\n",
    "    new_tag = [label_coarse[str(i)] for i in tags]\n",
    "    return new_tag\n",
    "    \n",
    "def fine_str_tag(tags):\n",
    "    new_tag = [label_fine[str(i)] for i in tags]\n",
    "    return new_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c4b9767",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in [\"train\",\"test\",\"dev\"]:\n",
    "    \n",
    "    data = []\n",
    "    with open(\"data/original/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    coarse = []\n",
    "    for d in data:\n",
    "        new_d = {}\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        new_d[\"tags\"] = coarse_str_tag(d[\"coarse_tags\"])\n",
    "        coarse.append(new_d)\n",
    "    with open(\"data/coarse/\"+filename+\".json\",\"w\") as f:\n",
    "        for d in coarse:\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    fine = []\n",
    "    for d in data:\n",
    "        new_d = {}\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        new_d[\"tags\"] = fine_str_tag(d[\"fine_tags\"])\n",
    "        fine.append(new_d)\n",
    "    with open(\"data/fine/\"+filename+\".json\",\"w\") as f:\n",
    "        for d in fine:\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19802c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0': 'O',\n",
       "  '1': 'art',\n",
       "  '2': 'building',\n",
       "  '3': 'event',\n",
       "  '4': 'location',\n",
       "  '5': 'organization',\n",
       "  '6': 'other',\n",
       "  '7': 'person',\n",
       "  '8': 'product'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_coarse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e0ed75",
   "metadata": {},
   "source": [
    "# Simple-BI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e3bca",
   "metadata": {},
   "source": [
    "・BIラベルのみに着目したデータ拡張\n",
    "\n",
    "・交換率100％"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "259731a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]\n",
    "\n",
    "def Simple_BI(dataname,filename,n):\n",
    "    data = []\n",
    "    with open(\"data/\"+dataname+\"/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    alterna_labels = {}\n",
    "    if dataname == \"c\":\n",
    "        tag_json = \"data/original/id2coarse_tags.json\"\n",
    "    else:\n",
    "        tag_json = \"data/original/id2fine_tags.json\"\n",
    "    with open(tag_json,\"r\") as f:\n",
    "        label_coarse =json.loads(f.read())\n",
    "        label_list = list(label_coarse.values())[1:]\n",
    "    for label in label_list:\n",
    "        alterna_labels[label] = set()\n",
    "    for d in data:\n",
    "        for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "            if tag in label_list:\n",
    "                alterna_labels[tag].add(token)\n",
    "\n",
    "    data_f_path = \"data/\"+dataname+\"_BI_x\"+str(n)+\"/\"+filename+\".json\"\n",
    "    memo_f_path = \"log/\"+dataname+\"_BI_x\"+str(n)+\"_\"+filename+\"_memo.txt\"\n",
    "    \n",
    "    \n",
    "    with open(data_f_path,\"w\") as data_f, open(memo_f_path,\"w\") as memo_f:\n",
    "        for d in data:\n",
    "            data_f.write(json.dumps(d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "        add_n = n*len(data)-len(data)\n",
    "        print()\n",
    "        print(dataname,filename,\"add_n\",add_n)\n",
    "        \n",
    "        for now in range(add_n):\n",
    "            if now%10 == 0:\n",
    "                print('\\r%d / %d' %(now, add_n), end='')\n",
    "\n",
    "            x = random.randint(0,len(data)-1)\n",
    "            d = data[x]\n",
    "            new_d = dict()\n",
    "            new_d[\"tokens\"] = []\n",
    "            new_d[\"tags\"] = data[x][\"tags\"]\n",
    "\n",
    "            cnt = 0\n",
    "\n",
    "            for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "                if tag != \"O\":\n",
    "                    if tag in alterna_labels:\n",
    "                        token=random_choice_token(alterna_labels[tag]-{token})\n",
    "                        cnt = cnt + 1\n",
    "                    new_d[\"tokens\"].append(token)\n",
    "\n",
    "            data_f.write(json.dumps(new_d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "            memo_f.write(str(x)+\"\\t\"+\" \".join(d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\"\\n\")\n",
    "\n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0cbfdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataname in [\"f\"]:\n",
    "#     for filename in [\"dev\",\"train\"]:\n",
    "#         Simple_BI(dataname,filename,2)\n",
    "#         Simple_BI(dataname,filename,5)\n",
    "#         Simple_BI(dataname,filename,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383c936",
   "metadata": {},
   "source": [
    "# TreePos-BI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee65db",
   "metadata": {},
   "source": [
    "・f直下のデータを取得し，各文を表すdict型を一つずつ取り出しd[\"id\"]という文書番号を表すキーと値を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02992287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "filename = \"dev\"\n",
    "# filename = \"train\"\n",
    "with open(\"data/f/\"+filename+\".json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "        \n",
    "for i,d in enumerate(data):\n",
    "    d[\"id\"] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5dc744",
   "metadata": {},
   "source": [
    "・idのキーを持つdict型の要素を500ずつまとめたjsonとし，data/f_idに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9a0f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "for i in range(0, len(data), n):\n",
    "    with open(\"data/f_id/\"+filename+\"_\"+str(i)+\"-\"+str(i+n-1)+\".json\",\"w\") as f:\n",
    "        for d in data[i: i+n]:\n",
    "                f.write(json.dumps(d))\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b3e2c",
   "metadata": {},
   "source": [
    "・f_id内の500ずつデータが入った細切れのjsonを全て統合してtrain.json,dev,jsonへ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f25d95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"data/f_id/\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "dev_data_list = []\n",
    "train_data_list = []\n",
    "\n",
    "for filename in filenames:\n",
    "    print(filename)\n",
    "    with open(path + filename,'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    if \"dev\" in filename:   \n",
    "        dev_data_list = dev_data_list + data\n",
    "    else:\n",
    "        train_data_list = train_data_list + data\n",
    "        \n",
    "with open(path + \"dev.json\", 'w') as ef:\n",
    "    for d in dev_data_list:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(path + \"train.json\", 'w') as ef:\n",
    "    for d in train_data_list:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c6d09",
   "metadata": {},
   "source": [
    "・ 形態素情報付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f3c6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import *\n",
    "import nltk\n",
    "from nltk.tree import *\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c06e8c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7991/2297462148.py:8: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
      "/tmp/ipykernel_7991/2297462148.py:10: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n"
     ]
    }
   ],
   "source": [
    "java_path = \"jre1.8.0_333/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "parser =  'stanford-corenlp/stanford-parser-full-2020-11-17/stanford-parser.jar'\n",
    "parser_model = 'stanford-corenlp/stanford-corenlp-4.2.0-models-english.jar'\n",
    "\n",
    "#POSタグの分析用\n",
    "pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "#係り受け関係の分析用\n",
    "dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "\n",
    "def POSTagAnalysis(text):\n",
    "    out = pos.raw_parse(text)\n",
    "    out = list(out)\n",
    "    tree = out[0]\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7a4e51",
   "metadata": {},
   "source": [
    "・filenameを受け取り，構文解析情報を付与し，Stanford_coreNLP内部に構文情報とともに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3a0e6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def TreePOS(filename):\n",
    "    data = []\n",
    "    with open(\"data/f_id/\"+filename,\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    new_document=[]\n",
    "    document_trees = []\n",
    "    print(filename)\n",
    "    \n",
    "    #タプル型で複数引数を渡す\n",
    "    for i,s in enumerate(data):\n",
    "        print('\\r%d / %d' %(i, len(data)), end='')\n",
    "        \n",
    "        text = \" \".join(s[\"tokens\"])\n",
    "        tree = POSTagAnalysis(text)\n",
    "        document_trees.append(tree)\n",
    "        new_d = dict()\n",
    "\n",
    "        new_d[\"tokens\"] = []\n",
    "        new_d[\"tree_pos\"] = []\n",
    "        new_d[\"tags\"] = []\n",
    "\n",
    "        for i in range(len(tree.pos())):\n",
    "            new_d[\"tokens\"].append(tree.pos()[i][0])\n",
    "            new_d[\"tree_pos\"].append(tree.pos()[i][1])\n",
    "\n",
    "        new_d[\"tags\"]=s[\"tags\"]\n",
    "        new_d[\"id\"] = s[\"id\"]\n",
    "        new_document.append(new_d)\n",
    "        del tree\n",
    "        \n",
    "        \n",
    "    with open(\"data/Stanford_coreNLP/\"+filename+\"_tree.binaryfile\", 'wb') as f:\n",
    "        pickle.dump(document_trees,f)\n",
    "    with open(\"data/Stanford_coreNLP_TreePos/\"+filename+\"_data.binaryfile\", 'wb') as f:\n",
    "        pickle.dump(new_document,f)\n",
    "\n",
    "    print(\"\\nSAVED ==> \"+filename+\"_tree.binaryfile\")\n",
    "    print(\"SAVED ==> \"+filename+\"_data.binaryfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacab565",
   "metadata": {},
   "source": [
    "・errorがなくなるまで繰り返し構文解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7993c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filenames = os.listdir(\"data/f_id/\")\n",
    "errors = []\n",
    "all_errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2072910e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in filenames:\n",
    "    try:\n",
    "        TreePOS(filename)\n",
    "    except:\n",
    "        errors.append(filename)\n",
    "        \n",
    "all_errors.append(errors)\n",
    "filenames = errors\n",
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7cd04b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    " with open(\"data/TreePosError.binaryfile\", 'wb') as f:\n",
    "        pickle.dump(errors,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04a938",
   "metadata": {},
   "source": [
    "・ Stanford_coreNLP内を全て統合して{train|dev}.json，{train_tree|dev_tree}.binaryfileへ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d9eb48f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "tree_path = \"data/Stanford_coreNLP/\"\n",
    "\n",
    "dev_tree_list = []\n",
    "train_tree_list = []\n",
    "\n",
    "filenames = os.listdir(tree_path)\n",
    "for filename in filenames:\n",
    "    with open(tree_path + filename,'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        if \"dev\" in filename:   \n",
    "            dev_tree_list = dev_tree_list + data\n",
    "        else:\n",
    "            train_tree_list = train_tree_list + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "436c07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "data_path = \"data/Stanford_coreNLP_TreePos/\"\n",
    "\n",
    "dev_data_list = []\n",
    "train_data_list = []\n",
    "\n",
    "filenames = os.listdir(data_path)\n",
    "for filename in filenames:\n",
    "    with open(data_path + filename,'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        if \"dev\" in filename:   \n",
    "            dev_data_list = dev_data_list + data\n",
    "        else:\n",
    "            train_data_list = train_data_list + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68f0e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(data_path + \"dev.json\", 'w') as ef:\n",
    "    for d in dev_data_list:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "with open(data_path + \"train.json\", 'w') as ef:\n",
    "    for d in train_data_list:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "78ab1694",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(tree_path+\"dev_tree.binaryfile\", 'wb') as f:\n",
    "    pickle.dump(dev_tree_list,f)\n",
    "    \n",
    "with open(tree_path+\"train_tree.binaryfile\", 'wb') as f:\n",
    "    pickle.dump(train_tree_list,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96de1333",
   "metadata": {},
   "source": [
    "・ f_id内を全て統合してtrain.json,dev,jsonへ\n",
    "\n",
    "・ 上の位置にコピーを動かし済み．実行後削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4673b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"data/f_id/\"\n",
    "filenames = os.listdir(path)\n",
    "\n",
    "dev_origin_list = []\n",
    "train_origin_list = []\n",
    "\n",
    "for filename in filenames:\n",
    "    data = []\n",
    "    with open(\"data/f_id/\"+filename,\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "        if \"dev\" in filename:\n",
    "            dev_origin_list = dev_origin_list + data\n",
    "        else:\n",
    "            train_origin_list = train_origin_list + data\n",
    "        \n",
    "with open(path + \"dev.json\", 'w') as ef:\n",
    "    for d in dev_origin_list:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(path + \"train.json\", 'w') as ef:\n",
    "    for d in train_origin_list:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9889f4",
   "metadata": {},
   "source": [
    "・ tokenが構文解析器入力前と入力後で異なるものを取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "749ba847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dev_data_list = []\n",
    "with open(\"data/Stanford_coreNLP_TreePos/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_data_list.append(json.loads(line))\n",
    "        \n",
    "train_data_list = []\n",
    "with open(\"data/Stanford_coreNLP_TreePos/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_data_list.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a02c3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dev_origin_list = []\n",
    "with open(\"data/f_id/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_origin_list.append(json.loads(line))\n",
    "        \n",
    "train_origin_list = []\n",
    "with open(\"data/f_id/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_origin_list.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c4e0ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18823\n",
      "131766\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_data_list))\n",
    "print(len(train_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f654bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18823\n",
      "131766\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_origin_list))\n",
    "print(len(train_origin_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dd3ed9",
   "metadata": {},
   "source": [
    "・dev_bebigger.json 　　dev_besmaller.json 　　dev_same.json を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "e1e9297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bebigger = []\n",
    "besmaller = []\n",
    "same=[] #id tokens tags\n",
    "for d in dev_origin_list:\n",
    "    #構文解析によってtoken数が変化する問題に対処する必要がある\n",
    "    #解析前tokenと解析後tokenを比較し，等しくない場合は\n",
    "    #解析前後の大小関係ごとにリストに格納し，結果の比較用データとする\n",
    "    \n",
    "    new_data = data_d[d[\"id\"]]\n",
    "    d[\"new_tokens\"] = new_data[\"tokens\"]\n",
    "    if len(d['tokens']) < len(d[\"new_tokens\"]):\n",
    "        bebigger.append(d)\n",
    "    elif len(d['tokens']) > len(d[\"new_tokens\"]):\n",
    "        besmaller.append(data_d[d[\"id\"]])\n",
    "    else:\n",
    "        same.append(data_d[d[\"id\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "f1584eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bebigger:3833\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_bebigger.json\", 'w') as ef:\n",
    "    for d in bebigger:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "# besmaller:1\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_besmaller.json\", 'w') as ef:\n",
    "    for d in besmaller:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "#same:14989\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_same.json\", 'w') as ef:\n",
    "    for d in same:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79f8bba",
   "metadata": {},
   "source": [
    "・train_bebigger.json 　　train_besmaller.json 　　train_same.json を作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "1e0594fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_d = dict()\n",
    "for d in train_data_list:\n",
    "    data_d[d[\"id\"]] = d\n",
    "\n",
    "bebigger = []\n",
    "besmaller = []\n",
    "same=[] #id tokens tags\n",
    "for d in train_origin_list:\n",
    "    #構文解析によってtoken数が変化する問題に対処する必要がある\n",
    "    #解析前tokenと解析後tokenを比較し，等しくない場合は\n",
    "    #解析前後の大小関係ごとにリストに格納し，結果の比較用データとする\n",
    "    \n",
    "    new_data = data_d[d[\"id\"]]\n",
    "    d[\"new_tokens\"] = new_data[\"tokens\"]\n",
    "    d[\"new_tags\"] = new_data[\"tags\"]\n",
    "    d[\"tree_pos\"] = new_data[\"tree_pos\"]\n",
    "    if len(d['tokens']) < len(d[\"new_tokens\"]):\n",
    "        bebigger.append(d)\n",
    "    elif len(d['tokens']) > len(d[\"new_tokens\"]):\n",
    "        besmaller.append(d)\n",
    "    else:\n",
    "        same.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "50f73cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104918"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "a06e469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bebigger:26841\n",
    "with open(\"data/Stanford_coreNLP_interm/train_bebigger.json\", 'w') as ef:\n",
    "    for d in bebigger:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "# besmaller:7\n",
    "with open(\"data/Stanford_coreNLP_interm/train_besmaller.json\", 'w') as ef:\n",
    "    for d in besmaller:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "#same:104918\n",
    "with open(\"data/Stanford_coreNLP_interm/train_same.json\", 'w') as ef:\n",
    "    for d in same:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a7a8d0",
   "metadata": {},
   "source": [
    "・{dev|train}_bebiggerについて，新たなトークン数のラベルに合わせる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3d3fb",
   "metadata": {},
   "source": [
    "・{dev|train}_bebiggerにnew_tagsを付与．うまくいったもの，そうでないもので{dev|train}_successと{dev|train}_errorに分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "aae2a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#単純に細分化問題のみに対応して新たなタグを付与し，うまくいったものとそうでないものを返す関数\n",
    "def GiveNewTag(input_list):\n",
    "    success = []\n",
    "    error = []\n",
    "    for d in input_list:\n",
    "        new_tags = [\"\"]*len(d[\"new_tokens\"])\n",
    "\n",
    "        old_i = 0\n",
    "        new_i = 0\n",
    "        try:\n",
    "            for _ in d[\"new_tokens\"]:\n",
    "                if d[\"new_tokens\"][new_i]==d[\"tokens\"][old_i]:\n",
    "                    new_tags[new_i] = d[\"tags\"][old_i]\n",
    "                    old_i += 1\n",
    "                    new_i += 1\n",
    "                else:\n",
    "                    if d[\"new_tokens\"][new_i] in d[\"tokens\"][old_i]:\n",
    "                        new_tags[new_i] = d[\"tags\"][old_i]\n",
    "                        new_i += 1\n",
    "                    else:\n",
    "                        old_i += 1\n",
    "                        if d[\"new_tokens\"][new_i]==d[\"tokens\"][old_i]:\n",
    "                            new_tags[new_i] = d[\"tags\"][old_i]\n",
    "                            old_i += 1\n",
    "                            new_i += 1\n",
    "\n",
    "            if \"\" in d[\"new_tokens\"]:\n",
    "                print(\"!!!\")\n",
    "            if len(new_tags)==len(d[\"new_tokens\"]):\n",
    "                d[\"new_tags\"] = new_tags\n",
    "                success.append(d)\n",
    "            else:\n",
    "                error.append(d)\n",
    "        except:\n",
    "            error.append(d)\n",
    "    return [success,error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1d90a14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev=>success/error\n",
    "import json\n",
    "        \n",
    "dev_bebigger = []\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_bebigger.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_bebigger.append(json.loads(line))\n",
    "\n",
    "dev_success,dev_error = GiveNewTag(dev_bebigger)#3001,#820"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "36919599",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train=>success/error\n",
    "import json\n",
    "\n",
    "train_bebigger = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train_bebigger.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_bebigger.append(json.loads(line))\n",
    "        \n",
    "train_success,train_error = GiveNewTag(train_bebigger)#21006,#5835"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "eb417c88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5835"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60f0601",
   "metadata": {},
   "source": [
    "・{dev|train}_errorのうち括弧が\"-LRB-\",\"-RRB-\"になる問題を解決　うまくいったものを{dev|train}_PBに，そうでないもの{dev|train}_error_PBへ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "27f47614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#細分化問題+タグ変更問題に対応，新たなタグを付与し，うまくいったものとそうでないものを返す関数\n",
    "def GiveNewTag_PB(input_list):\n",
    "    success = []\n",
    "    error = []\n",
    "    for d in input_list:\n",
    "        new_tags = [\"\"]*len(d[\"new_tokens\"])\n",
    "\n",
    "        old_i = 0\n",
    "        new_i = 0\n",
    "        try:\n",
    "            for _ in d[\"new_tokens\"]:\n",
    "                if d[\"new_tokens\"][new_i]==d[\"tokens\"][old_i] or d[\"new_tokens\"][new_i] in (\"-LRB-\",\"-RRB-\"):\n",
    "                    new_tags[new_i] = d[\"tags\"][old_i]\n",
    "                    old_i += 1\n",
    "                    new_i += 1\n",
    "                else:\n",
    "                    if d[\"new_tokens\"][new_i] in d[\"tokens\"][old_i]:\n",
    "                        new_tags[new_i] = d[\"tags\"][old_i]\n",
    "                        new_i += 1\n",
    "                    else:\n",
    "                        old_i += 1\n",
    "                        if d[\"new_tokens\"][new_i]==d[\"tokens\"][old_i]:\n",
    "                            new_tags[new_i] = d[\"tags\"][old_i]\n",
    "                            old_i += 1\n",
    "                            new_i += 1\n",
    "            if \"\" in d[\"new_tokens\"]:\n",
    "                print(\"!!!\")\n",
    "\n",
    "            if len(new_tags)==len(d[\"new_tokens\"]):\n",
    "                d[\"new_tags\"] = new_tags\n",
    "                success.append(d)\n",
    "            else:\n",
    "                error.append(d)\n",
    "        except:\n",
    "            error.append(d)\n",
    "    return [success,error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "24a60cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dev_error=>PB/error_PB\n",
    "dev_PB,dev_error_PB= GiveNewTag_PB(dev_error)#820,#12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "223b4157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_error=>PB/error_PB\n",
    "train_PB,train_error_PB = GiveNewTag_PB(train_error)#820,#12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b010f0",
   "metadata": {},
   "source": [
    "・dev_error_PBを修正"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a7f577",
   "metadata": {},
   "source": [
    "・Dod. . のようにカンマが被る問題を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "1ce07f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#重複カンマ削除\n",
    "def Remove_comme(input_list):\n",
    "    for d in input_list:\n",
    "        remake_tokens = []\n",
    "        for i in range(len(d[\"new_tokens\"])):\n",
    "            token = d[\"new_tokens\"][i]\n",
    "            if token != \".\":\n",
    "                remake_tokens.append(token)\n",
    "        if d[\"tokens\"][-1] == \".\":\n",
    "            remake_tokens.append(\".\")\n",
    "        d[\"new_tokens\"] = remake_tokens\n",
    "    return input_list\n",
    "\n",
    "dev_error_PB= Remove_comme(dev_error_PB)\n",
    "train_error_PB= Remove_comme(train_error_PB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f9ced",
   "metadata": {},
   "source": [
    "・カンマを削除したnew_tokenに対し改めてnew_tagsを付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "f7f83466",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_comma,dev_error_comma = GiveNewTag_PB(dev_error_PB)\n",
    "train_comma,train_error_comma = GiveNewTag_PB(train_error_PB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "ce5bf5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#devを保存\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_bebigger_only_3001.json\", 'w') as ef:\n",
    "    for d in dev_success:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/dev_BP_only_820.json\", 'w') as ef:\n",
    "    for d in dev_PB:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/dev_comma_9.json\", 'w') as ef:\n",
    "    for d in dev_comma:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "                \n",
    "with open(\"data/Stanford_coreNLP_interm/dev_fraction_3.json\", 'w') as ef:\n",
    "    for d in dev_error_comma:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "\n",
    "#trainを保存\n",
    "with open(\"data/Stanford_coreNLP_interm/train_bebigger_only_26841.json\", 'w') as ef:\n",
    "    for d in train_success:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/train_BP_only_5750.json\", 'w') as ef:\n",
    "    for d in train_PB:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/train_comma_61.json\", 'w') as ef:\n",
    "    for d in train_comma:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "                \n",
    "with open(\"data/Stanford_coreNLP_interm/train_fraction_24.json\", 'w') as ef:\n",
    "    for d in train_error_comma:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983353d",
   "metadata": {},
   "source": [
    "・{train|dev}_sameにエラーが無いかを調査"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "e6c12511",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_same = []\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_same.json\",\"r\") as df:\n",
    "    for line in df:\n",
    "        dev_same.append(json.loads(line))\n",
    "\n",
    "train_same = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train_same.json\", 'r') as tf:\n",
    "    for line in tf:\n",
    "        train_same.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60665042",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_same_unmatch = []\n",
    "dev_same_match = []\n",
    "for d in dev_same:\n",
    "    if d[\"tokens\"] != d[\"new_tokens\"]:\n",
    "        dev_same_unmatch.append(d)\n",
    "    else:\n",
    "        dev_same_match.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c83a7a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_same_unmatch = []\n",
    "train_same_match = []\n",
    "for d in train_same:\n",
    "    if d[\"tokens\"] != d[\"new_tokens\"]:\n",
    "        train_same_unmatch.append(d)\n",
    "    else:\n",
    "        train_same_match.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd139f3",
   "metadata": {},
   "source": [
    "・dev_same_match,train_same_matchを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f68c438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Stanford_coreNLP_interm/dev_same_match_13024.json\", 'w') as ef:\n",
    "    for d in dev_same_match:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/train_same_match_91892.json\", 'w') as ef:\n",
    "    for d in train_same_match:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364680b",
   "metadata": {},
   "source": [
    "・dev_same_unmatch,train_same_unmatchのうち，記号のtoken変化とそうでないものに分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "494d76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_unmatch_PB,dev_unmatch_error_PB = GiveNewTag_PB(dev_same_unmatch)\n",
    "train_unmatch_PB,train_unmatch_error_PB = GiveNewTag_PB(train_same_unmatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95182a6c",
   "metadata": {},
   "source": [
    "・記号変化対処で解決できたものを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "3c5f1aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Stanford_coreNLP_interm/dev_unmatch_PB_1918.json\", 'w') as ef:\n",
    "    for d in dev_unmatch_PB:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/train_unmatch_PB_13022.json\", 'w') as ef:\n",
    "    for d in train_unmatch_PB:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dcd5d5",
   "metadata": {},
   "source": [
    "・対応できなかったもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "70317816",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Stanford_coreNLP_interm/dev_unmatch_error_PB_2.json\", 'w') as ef:\n",
    "    for d in dev_unmatch_error_PB:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/train_unmatch_error_PB_2.json\", 'w') as ef:\n",
    "    for d in train_unmatch_error_PB:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e310e7cd",
   "metadata": {},
   "source": [
    "・以上，これまでに出てきた処理しきれないデータについて目視で確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "abb202eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_besmaller = []\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_besmaller.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_besmaller.append(json.loads(line))\n",
    "\n",
    "train_besmaller = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train_besmaller.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_besmaller.append(json.loads(line))\n",
    "        \n",
    "dev_fraction_3 = []\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_fraction_3.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_fraction_3.append(json.loads(line))\n",
    "\n",
    "train_fraction_24 = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train_fraction_24.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_fraction_24.append(json.loads(line))\n",
    "        \n",
    "dev_unmatch_error_PB = []\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_unmatch_error_PB_2.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_unmatch_error_PB.append(json.loads(line))\n",
    "\n",
    "train_unmatch_error_PB = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train_unmatch_error_PB_2.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_unmatch_error_PB.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "1ad84262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dev_unmatch_error_PB => OK\n",
    "# for i in range(2):\n",
    "#     print(len(dev_unmatch_error_PB[i][\"tokens\"]),\"_\".join(dev_unmatch_error_PB[i][\"tokens\"]))\n",
    "#     print(len(dev_unmatch_error_PB[i][\"tokens\"]),\"_\".join(dev_unmatch_error_PB[i][\"new_tokens\"]))\n",
    "#     print()\n",
    "    \n",
    "# # train_unmatch_error_PB = OK\n",
    "# for i in range(2):\n",
    "#     print(len(train_unmatch_error_PB[i][\"tokens\"]),\"_\".join(train_unmatch_error_PB[i][\"tokens\"]))\n",
    "#     print(len(train_unmatch_error_PB[i][\"tokens\"]),\"_\".join(train_unmatch_error_PB[i][\"new_tokens\"]))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "66d12d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_tagsを目視で入力するための出力関数\n",
    "def Add_new_tags(d):\n",
    "    print(\"ID:\",d[\"id\"])\n",
    "    new_tags = [\"\"]*len(d[\"new_tokens\"])\n",
    "    \n",
    "    olg_token_and_tag = \"\"\n",
    "    for i in range(len(d[\"tokens\"])):\n",
    "        olg_token_and_tag += \" \"+d[\"tokens\"][i]+\"_ \"+d[\"tags\"][i]+\"  \"\n",
    "    print('\\r%s' %(olg_token_and_tag), end='')\n",
    "    \n",
    "    for j in range(len(d[\"new_tokens\"])):\n",
    "        t = input(d[\"new_tokens\"][j]+\" = \")\n",
    "        if t == \"\":\n",
    "            new_tags[j] = \"O\"\n",
    "        else:\n",
    "             new_tags[j] = t\n",
    "    d[\"new_tags\"] = new_tags \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5936a2d9",
   "metadata": {},
   "source": [
    "・devにおいて目視チェックが必要なものを全て確認し，dev_checked_1+3+2.jsonとして保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "77293c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dev_checked = []\n",
    "\n",
    "# for d in dev_fraction_3:\n",
    "#     d = Add_new_tags(d)\n",
    "#     dev_checked.append(d)\n",
    "\n",
    "# for d in dev_besmaller:\n",
    "#     d = Add_new_tags(d)\n",
    "#     dev_checked.append(d)\n",
    "\n",
    "# for d in dev_unmatch_error_PB:\n",
    "#     dev_checked.append(d)\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_checked_1+3+2.json\", 'w') as ef:\n",
    "    for d in dev_checked:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f4a488",
   "metadata": {},
   "source": [
    "・trainにおいて目視チェックが必要なものを全て確認し，train_checked_7+24+2.jsonとして保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "a9481166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_checked = []#24\n",
    "# for d in train_fraction_24:\n",
    "#     d = Add_new_tags(d)\n",
    "#     train_checked.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "c6957c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_checked = []\n",
    "# target_checked = []\n",
    "# for d in train_checked:\n",
    "#     if d[\"id\"] in (44882,58227,94353,94720,99241,101971,119067):\n",
    "#         target_checked.append(d)\n",
    "#     else:\n",
    "#         other_checked.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "6aba780c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_checked_remake = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "39c05663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 6 #(0～7)\n",
    "# d = target_checked[i]#44882,58227,94353,94720,99241,101971,119067\n",
    "# print(i)\n",
    "# print(d[\"id\"])\n",
    "# new_d = Add_new_tags(d)\n",
    "# target_checked_remake.append(new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "b2032348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_besmaller_checked = []\n",
    "# for d in train_besmaller:\n",
    "#     new_d = Add_new_tags(d)\n",
    "#     train_besmaller_checked.append(new_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4515db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unmatch_error_PB = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train_unmatch_error_PB_2.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_unmatch_error_PB.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "af045ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_checked = other_checked + target_checked_remake + train_besmaller_checked + train_unmatch_error_PB\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_checked_7+24+2.json\", 'w') as ef:\n",
    "    for d in new_train_checked:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b305256e",
   "metadata": {},
   "source": [
    "・{train|dev}ともに処理済みのデータを全て結合し，保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1aa5f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = []#18823\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_bebigger_only_3001.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_BP_only_820.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_comma_9.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_checked_1+3+2.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_same_match_13024.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/dev_unmatch_PB_1918.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/dev.json\", 'w') as ef:\n",
    "    for d in dev:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "a02e8d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []#131766\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_bebigger_only_26841.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_BP_only_5750.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_comma_61.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_checked_7+24+2.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_same_match_91892.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "with open(\"data/Stanford_coreNLP_interm/train_unmatch_PB_13022.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "        \n",
    "with open(\"data/Stanford_coreNLP_interm/train.json\", 'w') as ef:\n",
    "    for d in train:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa26b8",
   "metadata": {},
   "source": [
    "## 【8月24日　追記】拡張前後でタグの粒度が異なるままのモデル構築が問題化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "196b2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Frac_unify(new_tokens):\n",
    "    s = \"_!_\".join(new_tokens)\n",
    "    s = s.replace(\"1/2\",\"½\").replace(\"1/3\",\"⅓\").replace(\"2/3\",\"⅔\").replace(\"1/4\",\"¼\")\\\n",
    "    .replace(\"3/4\",\"¾\").replace(\"1/5\",\"⅕\").replace(\"2/5\",\"⅖\").replace(\"3/5\",\"⅗\")\\\n",
    "    .replace(\"4/5\",\"⅘\").replace(\"1/6\",\"⅙\").replace(\"5/6\",\"⅚\").replace(\"1/7\",\"⅐\")\\\n",
    "    .replace(\"1/8\",\"⅛\").replace(\"3/8\",\"⅜\").replace(\"5/8\",\"⅝\").replace(\"7/8\",\"⅞\")\\\n",
    "    .replace(\"1/9\",\"⅑\").replace(\"1/10\",\"⅒\").replace(\"0/3\",\"↉\").replace(\"1/\",\"⅟\")\n",
    "    s = s.split(\"_!_\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd03d8",
   "metadata": {},
   "source": [
    "・（ここから再開する場合は）token整形，統合結果であるdevとtrainの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "c4e4c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = []\n",
    "with open(\"data/Stanford_coreNLP_interm/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "dev_data = []\n",
    "with open(\"data/Stanford_coreNLP_TreePos/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_data.append(json.loads(line))\n",
    "        \n",
    "dev_d = dict()\n",
    "for d in dev_data:\n",
    "    dev_d[d[\"id\"]] = d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa98136",
   "metadata": {},
   "source": [
    "・構文解析結果であるdev_dataとtrain_dataの読込み・辞書化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a284c99",
   "metadata": {},
   "source": [
    "・devに対し，tree_posをひとまず追加し，拡張前後のtokenタグずれを修正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "cfcacdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dev:\n",
    "    d[\"tree_pos\"]=dev_d[d[\"id\"]][\"tree_pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "0ed91a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = []#13073(全く同じ) + 1923(括弧の記号化，分数表記の修正等) = 14996\n",
    "bebig = []#3827\n",
    "besmall = []#0\n",
    "for d in dev:\n",
    "    if len(d[\"tokens\"])==len(d[\"new_tokens\"]):\n",
    "        d[\"origin_tree_pos\"] = d[\"tree_pos\"]\n",
    "        new.append(d)\n",
    "    elif len(d[\"tokens\"])<len(d[\"new_tokens\"]):\n",
    "        bebig.append(d)\n",
    "    else:\n",
    "        besmall.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "c42faf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "ok_d = []#3827\n",
    "error_d = []#0\n",
    "for d in bebig:#3827\n",
    "    origin_tree_pos = [\"x\"]*len(d[\"tokens\"])#\n",
    "    i = 0#オリジナルtoken列の探索用\n",
    "    j = 0#解析後の新token列の探索用　tree_posもこのtokenに準拠（＜＝重要）\n",
    "    try:\n",
    "        while j < len(d[\"new_tokens\"]):#場合によっては格納はせず，添え字を移動することになるのでforはNG\n",
    "            if d[\"new_tokens\"][j] == d[\"tokens\"][i] \\\n",
    "            or ( d[\"new_tokens\"][j] == \"-LRB-\" and d[\"tokens\"][i] == \"(\" ) \\\n",
    "            or ( d[\"new_tokens\"][j] == \"-RRB-\" and d[\"tokens\"][i] == \")\" ) \\\n",
    "            or re.match(pattern,d[\"new_tokens\"][j]):\n",
    "                #もし新旧tokenの要素が同じであれば\n",
    "                origin_tree_pos[i] = d[\"tree_pos\"][j]#新tokenのtree_posを旧tokenの長さに合わせたtree_posにも導入\n",
    "                i += 1#新旧とも次の要素へ移動\n",
    "                j += 1\n",
    "            else:#新旧tokenの不一致発生時の対応\n",
    "                #基本的に，細分化はあっても結合拡大は無いので，後者は考慮しない\n",
    "                if d[\"new_tokens\"][j] in d[\"tokens\"][i]:#もし新tokenが旧tokenの要素がの一部であれば（分解）\n",
    "                    origin_tree_pos[i] = d[\"tree_pos\"][j]#新tokenのtree_posを旧tokenの長さに合わせたtree_posにも導入\n",
    "                    j += 1#新tokenのみ次を参照（次の新tokenも現在の旧tokenの一部である可能性がある為）\n",
    "                else:#完全でも部分一致でもない＝＞部分一致が終了し，旧tokenの参照を次に進める必要あり\n",
    "                    i += 1\n",
    "        d[\"origin_tree_pos\"] = origin_tree_pos\n",
    "        ok_d.append(d)\n",
    "    except:\n",
    "        error_d.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "f0bd8713",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = new+ok_d #18823"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab30a3f",
   "metadata": {},
   "source": [
    "・trainに対し，tree_posをひとまず追加し，拡張前後のtokenタグずれを修正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "8a609a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "        \n",
    "train_data = []\n",
    "with open(\"data/Stanford_coreNLP_TreePos/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line))\n",
    "        \n",
    "train_d = dict()\n",
    "for d in train_data:\n",
    "    train_d[d[\"id\"]] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "c67291a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in train:\n",
    "    d[\"tree_pos\"]=train_d[d[\"id\"]][\"tree_pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "6cbb23f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new = []#104964\n",
    "bebig = []#26790\n",
    "besmall = []#12\n",
    "for d in train:\n",
    "    if len(d[\"tokens\"])==len(d[\"new_tokens\"]):\n",
    "        d[\"origin_tree_pos\"] = d[\"tree_pos\"]\n",
    "        new.append(d)\n",
    "    elif len(d[\"tokens\"])<len(d[\"new_tokens\"]):\n",
    "        bebig.append(d)\n",
    "    else:\n",
    "        besmall.append(d)\n",
    "        \n",
    "#besmallが発生してしまった．．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "9d8c9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = r'\\d{1}/\\d{1}'\n",
    "\n",
    "ok_d = []#26782\n",
    "error_d = []#8\n",
    "\n",
    "for d in bebig:#26790\n",
    "    origin_tree_pos = [\"x\"]*len(d[\"tokens\"])#\n",
    "    i = 0#オリジナルtoken列の探索用\n",
    "    j = 0#解析後の新token列の探索用　tree_posもこのtokenに準拠（＜＝重要）\n",
    "    try:\n",
    "        while j < len(d[\"new_tokens\"]):#場合によっては格納はせず，添え字を移動することになるのでforはNG\n",
    "            if d[\"new_tokens\"][j] == d[\"tokens\"][i] \\\n",
    "            or ( d[\"new_tokens\"][j] == \"-LRB-\" and d[\"tokens\"][i] == \"(\" ) \\\n",
    "            or ( d[\"new_tokens\"][j] == \"-RRB-\" and d[\"tokens\"][i] == \")\" ) \\\n",
    "            or re.match(pattern,d[\"new_tokens\"][j]):\n",
    "                #もし新旧tokenの要素が同じであれば\n",
    "                origin_tree_pos[i] = d[\"tree_pos\"][j]#新tokenのtree_posを旧tokenの長さに合わせたtree_posにも導入\n",
    "                i += 1#新旧とも次の要素へ移動\n",
    "                j += 1\n",
    "            else:#新旧tokenの不一致発生時の対応\n",
    "                #基本的に，細分化はあっても結合拡大は無いので，後者は考慮しない\n",
    "                if d[\"new_tokens\"][j] in d[\"tokens\"][i]:#もし新tokenが旧tokenの要素がの一部であれば（分解）\n",
    "                    origin_tree_pos[i] = d[\"tree_pos\"][j]#新tokenのtree_posを旧tokenの長さに合わせたtree_posにも導入\n",
    "                    j += 1#新tokenのみ次を参照（次の新tokenも現在の旧tokenの一部である可能性がある為）\n",
    "                else:#完全でも部分一致でもない＝＞部分一致が終了し，旧tokenの参照を次に進める必要あり\n",
    "                    i += 1\n",
    "        d[\"origin_tree_pos\"] = origin_tree_pos\n",
    "        ok_d.append(d)\n",
    "    except:\n",
    "        error_d.append(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f064b0",
   "metadata": {},
   "source": [
    "・train時のerror_dとbesmallを外部に保存＝＞人手で処理する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "27e312b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "handy_fix = error_d+besmall\n",
    "with open(\"data/Stanford_coreNLP_interm/train_error_d+besmall.json\", 'w') as ef:\n",
    "    for d in handy_fix:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "be56ecf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#手動で作成したデータを付与\n",
    "import json\n",
    "\n",
    "handy_fixed = []\n",
    "with open(\"data/Stanford_coreNLP_interm/train_error_d+besmall_handy_fix.json\", 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            handy_fixed.append(json.loads(line))\n",
    "        except:\n",
    "            print(line)\n",
    "            \n",
    "for d1 in handy_fix:\n",
    "    for d2 in handy_fixed:\n",
    "        if d1[\"tokens\"] == d2[\"tokens\"]:\n",
    "            d1[\"origin_tree_pos\"] = d2[\"origin_tree_pos\"]\n",
    "#全ての要素にorigin_tree_posが付いたことを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "6066b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = new+ok_d+handy_fix #131766"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df5f6f",
   "metadata": {},
   "source": [
    "・train,devを整形し，tree_posを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "dff8152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens\n",
    "# tags\n",
    "# id\n",
    "# new_tokens\n",
    "# new_tags\n",
    "# tree_pos\n",
    "# origin_tree_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2eccf1",
   "metadata": {},
   "source": [
    "・#Stanford_coreNLP内に作成された構文解析済みデータから，新たにTreePos_Tagを持つデータを作成\n",
    "#全て統合してtrain.json,dev.jsonへ\n",
    "\n",
    "・tokensとtree_posが合わないものはerrorへ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "e62b4f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stanford_coreNLP内に作成された構文解析済みデータから，新たにTreePos_Tagを持つデータを作成\n",
    "#全て統合してtrain.json,dev.jsonへ\n",
    "def ModelFormat(data):\n",
    "    new_data = []\n",
    "    error_data = []\n",
    "    for d in data:\n",
    "        new_d = dict()\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        if len(d[\"tokens\"]) == len(d[\"origin_tree_pos\"]):\n",
    "            new_d[\"tags\"] = [str(d[\"origin_tree_pos\"][j])+\"_\"+str(d[\"tags\"][j]) for j in range(len(d[\"tags\"]))]\n",
    "            new_data.append(new_d)\n",
    "        else:\n",
    "            error_data.append(d)\n",
    "    return [new_data,error_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fcff61",
   "metadata": {},
   "source": [
    "・うまくいったもの，errorが出たものを一旦保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "cf9e8674",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dev,error_dev = ModelFormat(dev) # 18816 #7\n",
    "with open(\"data/TreePos_BI/dev_no_error.json\", 'w', encoding=\"utf-8\") as wf:\n",
    "    for d in new_dev:\n",
    "        wf.write(json.dumps(d))\n",
    "        wf.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/TreePos_BI/dev_error.json\", 'w',encoding=\"utf-8\") as ef:\n",
    "    for d in error_dev:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")\n",
    "    \n",
    "new_train,error_train = ModelFormat(train) #131719 #47\n",
    "with open(\"data/TreePos_BI/train_no_error.json\", 'w',encoding=\"utf-8\") as wf:\n",
    "    for d in new_train:\n",
    "        wf.write(json.dumps(d))\n",
    "        wf.write(\"\\n\")\n",
    "        \n",
    "with open(\"data/TreePos_BI/train_error.json\", 'w',encoding=\"utf-8\") as ef:\n",
    "    for d in error_train:\n",
    "        ef.write(json.dumps(d))\n",
    "        ef.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "066bdd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#手動修正後\n",
    "error_dev = []\n",
    "\n",
    "with open(\"data/TreePos_BI/dev_error_handy.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        error_dev.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "0ced6a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#手動修正後\n",
    "error_train = []\n",
    "\n",
    "with open(\"data/TreePos_BI/train_error_handy.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        error_train.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "c3a6f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dev_2,error_dev_2 = ModelFormat(error_dev) #7件 全て処理は成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "5faadd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_2,error_train_2 = ModelFormat(error_train) #47件 全て処理は成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "c90af712",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dev_all=new_dev+new_dev_2#18823\n",
    "with open(\"data/TreePos_BI/dev.json\", 'w', encoding=\"utf-8\") as wf:\n",
    "    for d in new_dev_all:\n",
    "        wf.write(json.dumps(d))\n",
    "        wf.write(\"\\n\")\n",
    "    \n",
    "new_train_all=new_train+error_train_2#131766\n",
    "with open(\"data/TreePos_BI/train.json\", 'w',encoding=\"utf-8\") as wf:\n",
    "    for d in new_train_all:\n",
    "        wf.write(json.dumps(d))\n",
    "        wf.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "71973f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131719"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_train_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d607bd",
   "metadata": {},
   "source": [
    "・data/TreePos_BI/{dev|train}.jsonは最終加工前の状態だが，学習には用いないのでそのままOK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e638bb",
   "metadata": {},
   "source": [
    "・データ拡張実装前準備\n",
    "\n",
    "・alterna_labelsの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0fada54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dev_data = []\n",
    "with open(\"data/TreePos_BI/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        dev_data.append(json.loads(line))\n",
    "train_data = []\n",
    "with open(\"data/TreePos_BI/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train_data.append(json.loads(line))\n",
    "        \n",
    "label_list = []\n",
    "for d in dev_data:\n",
    "    label_list.extend(d[\"tags\"])\n",
    "for d in train_data:\n",
    "    label_list.extend(d[\"tags\"])\n",
    "label_list = list(set([tags for tags in label_list if \"_O\" not in tags]))\n",
    "\n",
    "alterna_labels = {}\n",
    "for label in label_list:\n",
    "    alterna_labels[label] = set()\n",
    "for d in dev_data+train_data:\n",
    "    for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "        if tag in label_list:\n",
    "            alterna_labels[tag].add(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f18ae",
   "metadata": {},
   "source": [
    "・ここで，alterna_labelsのうち，候補が2つ以下のものは削除（下の例の2・3行目）\n",
    "\n",
    "{'VBZ_person-actor': {\"'s\",  'Brest',  'Mature',  'Torrens',  'cameos',  'follows',  'heartthrobs',  'is',  'stars'},\n",
    "\n",
    "'RBR_art-film': {'Better', 'Worse'},\n",
    "\n",
    "'TO_person-athlete': {'to'}\n",
    "\n",
    "...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa4461e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_alterna_labels = dict()\n",
    "for k,v in alterna_labels.items():\n",
    "    if len(v)>2:\n",
    "        new_alterna_labels[k] = v\n",
    "\n",
    "alterna_labels = new_alterna_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b96254f",
   "metadata": {},
   "source": [
    "・データ拡張実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "220a9fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]\n",
    "\n",
    "def TreePos_BI(filename,n):\n",
    "    data = []\n",
    "    with open(\"data/TreePos_BI/\"+filename+\".json\",\"r\",encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    data_f_path = \"data/TreePos_BI_x\"+str(n)+\"/\"+filename+\".json\"\n",
    "    memo_f_path = \"log/TreePos_BI_x\"+str(n)+\"_\"+filename+\"_memo.txt\"\n",
    "    \n",
    "    \n",
    "    with open(data_f_path,\"w\",encoding=\"utf-8\") as data_f, open(memo_f_path,\"w\",encoding=\"utf-8\") as memo_f:\n",
    "        for d in data:\n",
    "            data_f.write(json.dumps(d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "        add_n = n*len(data)-len(data)\n",
    "        print()\n",
    "        print(filename,\"add_n\",add_n)\n",
    "        \n",
    "        for now in range(add_n):\n",
    "            if now%10 == 0:\n",
    "                print('\\r%d / %d' %(now, add_n), end='')\n",
    "\n",
    "            x = random.randint(0,len(data)-1)\n",
    "            d = data[x]\n",
    "            new_d = dict()\n",
    "            new_d[\"tokens\"] = []\n",
    "            new_d[\"tags\"] = data[x][\"tags\"]\n",
    "            cnt = 0\n",
    "\n",
    "            for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "                if tag in alterna_labels:\n",
    "                    token=random_choice_token(alterna_labels[tag]-{token})\n",
    "                    cnt = cnt + 1\n",
    "                new_d[\"tokens\"].append(token)\n",
    "\n",
    "            data_f.write(json.dumps(new_d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "            memo_f.write(str(x)+\"\\t\"+\" \".join(d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\"\\n\")\n",
    "\n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6ac8f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dev add_n 18823\n",
      "18820 / 18823"
     ]
    }
   ],
   "source": [
    "TreePos_BI(\"dev\",2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d5fa2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train add_n 131719\n",
      "131710 / 131719"
     ]
    }
   ],
   "source": [
    "TreePos_BI(\"train\",2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d11de",
   "metadata": {},
   "source": [
    "・交換は実装できたので，学習ラベルエラーを防ぐためにもラベル名を元に戻す\n",
    "\n",
    "・これをやる前に加工前のデータをbefore_remove_TreePosに入れておく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf98c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dev = []\n",
    "with open(\"data/TreePos_BI_x2/before_remove_TreePos/dev.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        dev.append(json.loads(line))\n",
    "\n",
    "for d in dev:#追記　一部タグの後半部分に\"\"が混入する問題への対処\n",
    "    new_tags=[]\n",
    "    for tag in d[\"tags\"]:\n",
    "        new_tag = tag.split(\"_\")[1]\n",
    "        if new_tag == \"\":\n",
    "            new_tags.append(\"O\")#あくまで応急処置\n",
    "        else:\n",
    "            new_tags.append(new_tag)\n",
    "    d[\"tags\"]=new_tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d7ac2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "with open(\"data/TreePos_BI_x2/before_remove_TreePos/train.json\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "\n",
    "for d in train:\n",
    "    new_tags=[]\n",
    "    for tag in d[\"tags\"]:\n",
    "        new_tag = tag.split(\"_\")[1]\n",
    "        if new_tag == \"\":\n",
    "            new_tags.append(\"O\")#あくまで応急処置\n",
    "        else:\n",
    "            new_tags.append(new_tag)\n",
    "    d[\"tags\"]=new_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "df349066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2877\n"
     ]
    }
   ],
   "source": [
    "# tagsへの \"\" 混入事件実態調査\n",
    "\n",
    "# cnt = 0\n",
    "# new_d = []\n",
    "# for d in train:\n",
    "#     if \"\" in d[\"tags\"]:\n",
    "#         cnt += 1\n",
    "# #         print(\"_\".join(d[\"tokens\"]))\n",
    "# #         print(\"_\".join(d[\"tags\"]))\n",
    "# print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "273e99ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/TreePos_BI_x2/dev.json\", 'w',encoding=\"utf-8\") as wf:\n",
    "    for d in dev:\n",
    "        wf.write(json.dumps(d))\n",
    "        wf.write(\"\\n\")\n",
    "\n",
    "with open(\"data/TreePos_BI_x2/train.json\", 'w',encoding=\"utf-8\") as wf:\n",
    "    for d in train:\n",
    "        wf.write(json.dumps(d))\n",
    "        wf.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91195d3",
   "metadata": {},
   "source": [
    "# 結果の集計用プログラム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2efeefc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_name = \"f\"\n",
    "# data_name = \"f_BI_x2\"\n",
    "data_name = \"TreePos_BI_x2\"\n",
    "file_path = \"model/batch20/\"+data_name+\"/eval_results.json\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    eval_result = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8bab7f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5449510812759399, 'runtime': 112.7457, 'samples': 37626}\n",
      "{'accuracy': 0.9323885469356032, 'f1': 0.6923231148625618, 'precision': 0.6737756097560975, 'recall': 0.711920666336797}\n"
     ]
    }
   ],
   "source": [
    "new_eval_result = {}\n",
    "for k in eval_result:\n",
    "    if k != \"epoch\":\n",
    "        label_name = k.split(\"_\")[-2]\n",
    "        result_name = k.split(\"_\")[-1]\n",
    "        if label_name not in new_eval_result:\n",
    "            new_eval_result[label_name] = {}\n",
    "        new_eval_result[label_name][result_name] = eval_result[k]\n",
    "\n",
    "print(new_eval_result[\"eval\"])\n",
    "print(new_eval_result[\"overall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363dc570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
