{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # MTG 1/6\n",
    "    #POStagとENTtagの間には何らかの関係性が存在？\n",
    "    #NERにおけるPOStagアプローチはsimpleと何も変わらないのでは＝＞誤差レベルの違いである可能性が高い\n",
    "    #構文木のほうにいけるとよいのですが..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 形態素情報付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import *\n",
    "import nltk\n",
    "from nltk.tree import *\n",
    "import svgling\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\momo1\\AppData\\Local\\Temp\\ipykernel_20208\\1251703492.py:8: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
      "C:\\Users\\momo1\\AppData\\Local\\Temp\\ipykernel_20208\\1251703492.py:10: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n"
     ]
    }
   ],
   "source": [
    "java_path = \"C:/Program Files/Java/jre1.8.0_321/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "parser =  'stanford-corenlp/stanford-parser-full-2020-11-17/stanford-parser.jar'\n",
    "parser_model = 'stanford-corenlp/stanford-corenlp-4.2.0-models-english.jar'\n",
    "\n",
    "#POSタグの分析用\n",
    "pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "#係り受け関係の分析用\n",
    "dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "\n",
    "def POSTagAnalysis(text):\n",
    "    out = pos.raw_parse(text)\n",
    "    out = list(out)\n",
    "    tree = out[0]\n",
    "    return tree\n",
    "\n",
    "# def dependenceAnalysis(text):\n",
    "#     out = dep_parser.raw_parse(text)\n",
    "#     out = list(out)\n",
    "#     parse = out[0]\n",
    "#     return parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pos&NERposタグの付与"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/train.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    # data = data[1:14987] #不要部分をカット\n",
    "\n",
    "    \n",
    "# storing conll format data in multiple list\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\"\\t\")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)\n",
    "    \n",
    "# document[0] =\n",
    "# [['Paul', 'O'], ['International', 'O'], ['airport', 'O'], ['.', 'O']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ~ 30000 :  1625 / 131767[main] INFO edu.stanford.nlp.parser.lexparser.LexicalizedParser - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.6 sec].\n",
      "Parsing file: C:\\Users\\momo1\\AppData\\Local\\Temp\\tmpcifhlfrp\n",
      "Parsing [sent. 1 len. 269]: Featuring Writers : Shimon Adaf ( IL ) , Lea Aini ( IL ) , Eli Amir ( IL ) , Yair Assulin ( IL ) , Nir Baram ( IL ) , Lukas Barfuss ( SZ ) , Hanoch Bartov ( IL ) , Haim Be'er ( IL ) , Aimee Bender ( US ) , Sami Berdugo ( IL ) , Sarah Blau ( IL ) , Yochi Brandes ( IL ) , Orly Castel - Bloom ( IL ) , Tracy Chevalier ( UK ) , Solveig Eggerz ( IS \\ US ) , Alona Frankel ( IL ) , David Grossman ( IL ) , Arnon Grunberg ( NL ) , Amir Gutfreund ( IL ) , Aleksandar Hemon ( US \\ BA ) , Alon Hilu ( IL ) , Sayed Kashua ( IL ) , Etgar Keret ( IL ) , Herman Koch ( NL ) , Laszlo Krasznahorkai ( HU ) , Haggay Linik ( IL ) , Dory Manor ( IL ) , Ronit Matalon ( IL ) , Lorenza Mazzetti ( IT ) , Dror Mishani ( IL ) , Yael Neeman ( IL ) , Eshkol Nevo ( IL ) , Amos Oz ( IL ) , Claudia Pineiro ( AR ) , Dorit Rabinyan ( IL ) , Tom Rob Smith ( UK ) , Moshe Sakal ( IL ) , Boualem Sansal ( DZ ) , Meir Shalev ( IL ) , Zeruya Shalev ( IL ) , Gary Shteyngart ( US ) , Einat Yakir ( IL ) , Abraham B. Yehoshua ( IL ) .\n",
      "\n",
      "*******************************************************\n",
      "***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***\n",
      "***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***\n",
      "***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***\n",
      "***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***\n",
      "***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***\n",
      "***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***\n",
      "*******************************************************\n",
      "\n",
      "Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...\n",
      "Exception in thread \"main\" edu.stanford.nlp.parser.common.NoSuchParseException\n",
      "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.getBestParse(LexicalizedParserQuery.java:403)\n",
      "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.getBestParse(LexicalizedParserQuery.java:375)\n",
      "\tat edu.stanford.nlp.parser.lexparser.ParseFiles.processResults(ParseFiles.java:272)\n",
      "\tat edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:216)\n",
      "\tat edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:75)\n",
      "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(LexicalizedParser.java:1491)\n",
      "\n",
      " ~ 131766 :  6052 / 131767[main] INFO edu.stanford.nlp.parser.lexparser.LexicalizedParser - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... done [0.6 sec].\n",
      "Parsing file: C:\\Users\\momo1\\AppData\\Local\\Temp\\tmppgjde4ug\n",
      "Parsing [sent. 1 len. 273]: The Alpine Lakes Wilderness and its old growth forests offer critical habitat for many species on the Washington State Department of Fish and Wildlife ' s `` Species of Concern '' list which includes the Western spotted frog ( `` Rana pretiosa `` ) , Common Loon ( `` Gavia immer `` ) , Western Grebe ( `` Aechmophorus occidentalis `` ) , Goshawk ( `` Accipiter gentilis `` ) , Golden eagle ( `` Aquila chrysaetos `` ) , Bald eagle ( `` Haliaeetus leucocephalus `` ) , Peregrine falcon ( `` Falco peregrinus `` ) , Merlin ( `` Falco columbarius `` ) , Flammulated owl ( `` Otus flammeolus `` ) , Spotted owl ( `` Strix occidentalis `` ) , Vaux 's swift ( `` Chaetura vauxi `` ) , PIleated woodpecker ( `` Dryocopus pileatus `` ) , Lewis ' woodpecker ( `` Melanerpes lewis `` ) , White - headed woodpecker ( `` Picoides albolarvatus `` ) , Black - backed Three - toed woodpecker ( `` Picoides arcticus `` ) , Horned lark ( `` Eremophila alpestris `` ) , White - breasted nuthatch ( `` Sitta carolinensis `` ) , Sage thrasher ( `` Oreoscoptes montanus `` ) , Loggerhead shrike ( `` Lanius ludovicianus `` ) , Vesper sparrow ( `` Pooecetes gramineus `` ) , Sage sparrow ( `` A mphispiza belli '' ) , Townsend 's Big - eared bat ( `` Plecotus townsendi `` ) , Fisher ( `` Martes pennanti `` ) , Wolverine ( `` Gulo gulo `` ) and the Lynx ( `` Lynx canadensis `` ) .\n",
      "\n",
      "*******************************************************\n",
      "***  WARNING!! OUT OF MEMORY! THERE WAS NOT ENOUGH  ***\n",
      "***  MEMORY TO RUN ALL PARSERS.  EITHER GIVE THE    ***\n",
      "***  JVM MORE MEMORY, SET THE MAXIMUM SENTENCE      ***\n",
      "***  LENGTH WITH -maxLength, OR PERHAPS YOU ARE     ***\n",
      "***  HAPPY TO HAVE THE PARSER FALL BACK TO USING    ***\n",
      "***  A SIMPLER PARSER FOR VERY LONG SENTENCES.      ***\n",
      "*******************************************************\n",
      "\n",
      "Sentence has no parse using PCFG grammar (or no PCFG fallback).  Skipping...\n",
      "Exception in thread \"main\" edu.stanford.nlp.parser.common.NoSuchParseException\n",
      "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.getBestParse(LexicalizedParserQuery.java:403)\n",
      "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParserQuery.getBestParse(LexicalizedParserQuery.java:375)\n",
      "\tat edu.stanford.nlp.parser.lexparser.ParseFiles.processResults(ParseFiles.java:272)\n",
      "\tat edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:216)\n",
      "\tat edu.stanford.nlp.parser.lexparser.ParseFiles.parseFiles(ParseFiles.java:75)\n",
      "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(LexicalizedParser.java:1491)\n",
      "\n",
      " ~ 131766 :  11765 / 131767"
     ]
    }
   ],
   "source": [
    "n = [10000,20000,30000,40000,50000,60000,70000,80000,90000,100000,110000,120000,131766]\n",
    "\n",
    "for i in range(len(n)):\n",
    "    if i == 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = n[i-1]\n",
    "    goal = n[i]\n",
    "    \n",
    "    \n",
    "    document_trees = []\n",
    "    new_document = []\n",
    "    error_sentence = set()\n",
    "    c = 0\n",
    "    \n",
    "    \n",
    "    for sentence in document[start:goal]:\n",
    "\n",
    "        print('\\r ~ %d :  %d / %d' %(goal, c, len(document)), end='')\n",
    "        text = \" \".join([token[0] for token in sentence])\n",
    "\n",
    "        try:\n",
    "            label = [token[1] for token in sentence]\n",
    "            tree = POSTagAnalysis(text)\n",
    "            document_trees.append(tree)\n",
    "\n",
    "            new_sentence = []\n",
    "            for i in range(len(tree.pos())):\n",
    "                token = tree.pos()[i][0]\n",
    "                tag = tree.pos()[i][1]+\"_\"+label[i]\n",
    "                new_sentence.append([token,tag])\n",
    "            new_document.append(new_sentence)\n",
    "        except:\n",
    "            error_sentence.add(c)\n",
    "        c = c+1\n",
    "\n",
    "    with open(\"data_pos_binary/train_tree~\"+str(goal)+\".binaryfile\", 'wb') as f:\n",
    "        pickle.dump(document_trees,f)\n",
    "        \n",
    "    with open(\"data_pos_binary/train_data~\"+str(goal)+\".binaryfile\", 'wb') as f:\n",
    "        pickle.dump(new_document,f)\n",
    "\n",
    "    with open(\"data_pos_binary/train_error~\"+str(goal)+\".binaryfile\", 'wb') as f:\n",
    "        pickle.dump(error_sentence,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/dev.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "\n",
    "    \n",
    "# storing conll format data in multiple list\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\"\\t\")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)\n",
    "    \n",
    "# document[0] =\n",
    "# [['Paul', 'O'], ['International', 'O'], ['airport', 'O'], ['.', 'O']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18824"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ~ 18823 :  8822 / 18824"
     ]
    }
   ],
   "source": [
    "n = [10000,18823]\n",
    "\n",
    "\n",
    "for i in range(len(n)):\n",
    "    if i == 0:\n",
    "        start = 0\n",
    "    else:\n",
    "        start = n[i-1]\n",
    "    goal = n[i]\n",
    "    \n",
    "    \n",
    "    document_trees = []\n",
    "    new_document = []\n",
    "    error_sentence = set()\n",
    "    c = 0\n",
    "    \n",
    "    \n",
    "    for sentence in document[start:goal]:\n",
    "\n",
    "        print('\\r ~ %d :  %d / %d' %(goal, c, len(document)), end='')\n",
    "        text = \" \".join([token[0] for token in sentence])\n",
    "\n",
    "        try:\n",
    "            label = [token[1] for token in sentence]\n",
    "            tree = POSTagAnalysis(text)\n",
    "            document_trees.append(tree)\n",
    "\n",
    "            new_sentence = []\n",
    "            for i in range(len(tree.pos())):\n",
    "                token = tree.pos()[i][0]\n",
    "                tag = tree.pos()[i][1]+\"_\"+label[i]\n",
    "                new_sentence.append([token,tag])\n",
    "            new_document.append(new_sentence)\n",
    "        except:\n",
    "            error_sentence.add(c)\n",
    "        c = c+1\n",
    "\n",
    "    with open(\"data_pos_binary/dev_tree~\"+str(goal)+\".binaryfile\", 'wb') as f:\n",
    "        pickle.dump(document_trees,f)\n",
    "        \n",
    "    with open(\"data_pos_binary/dev_data~\"+str(goal)+\".binaryfile\", 'wb') as f:\n",
    "        pickle.dump(new_document,f)\n",
    "\n",
    "    with open(\"data_pos_binary/dev_error~\"+str(goal)+\".binaryfile\", 'wb') as f:\n",
    "        pickle.dump(error_sentence,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下のセルはprog4にて"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making dictonary object with label as key and token as value\n",
    "label_token_dic = {}\n",
    "for sentence in new_document:\n",
    "    for token_list in sentence:\n",
    "        label = token_list[1]\n",
    "        token = token_list[0]\n",
    "        if label not in label_token_dic:\n",
    "            label_token_dic[label] = set()\n",
    "        label_token_dic[label].add(token)\n",
    "        \n",
    "    #  {'NNP_B-ORG': {'Detroit',\n",
    "    #   'FK',\n",
    "    #   'ATRIA',\n",
    "    #   'DBRS',...\n",
    "    \n",
    "    # label_token_dic.keys() でkey一覧取得可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making BI label list\n",
    "BI_label_list = [label for label in list(label_token_dic.keys()) if \"B-\" in label or \"I-\" in label]\n",
    "    # ['NNP_B-ORG',\n",
    "    #  'JJ_B-MISC',\n",
    "    #  'NNP_B-PER',\n",
    "    #  'NNP_I-PER',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sent2str(sentence_list):\n",
    "    return \" \".join([token_list[0] for token_list in sentence_list])\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "\n",
    "def SimpleDA_BI(n,filename):\n",
    "    data_f_path = \"data_augment/SimpleDA_BI/\"+str(n)+\"_\"+filename+\".txt\"\n",
    "    memo_f_path = \"data_augment/SimpleDA_BI/\"+str(n)+\"_\"+filename+\"_memo.txt\"\n",
    "    with open(data_f_path,\"w\") as data_f, open(memo_f_path,\"w\") as memo_f:\n",
    "        for now in range(n):\n",
    "            if now%10 == 0:\n",
    "                print('\\r%d / %d' %(now, n), end='')\n",
    "\n",
    "            x = random.randint(0,len(new_document)-1)\n",
    "            origin_sentence = new_document[x]\n",
    "            aug_sentence = []\n",
    "            cnt = 0\n",
    "\n",
    "            for token_list in origin_sentence:\n",
    "                token = token_list[0]\n",
    "                label = token_list[1]\n",
    "                origin_label = token_list[2]\n",
    "\n",
    "                if label in BI_label_list:\n",
    "                    if len(label_token_dic[label])!=1:\n",
    "                        token=random_choice_token(label_token_dic[label]-{token})\n",
    "                        cnt = cnt + 1\n",
    "                    \n",
    "\n",
    "                aug_sentence.append([token,origin_label])\n",
    "                data_f.write(\" \".join([token,origin_label])+\"\\n\")\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "            memo_f.write(str(x)+\"\\t\"+Sent2str(new_document[x])+\"\\n\")\n",
    "            memo_f.write(\" \"*len(str(x))+\"\\t\"+Sent2str(aug_sentence)+\"\\n\")\n",
    "            memo_f.write(\"\\n\")\n",
    "        \n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #train\n",
    "n = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484990 / 1485000"
     ]
    }
   ],
   "source": [
    "SimpleDA_BI(n*1,\"train\")\n",
    "SimpleDA_BI(n*4,\"train\")\n",
    "SimpleDA_BI(n*9,\"train\")\n",
    "SimpleDA_BI(n*49,\"train\")\n",
    "SimpleDA_BI(n*99,\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_original/valid.txt', 'r', encoding='UTF-8') as f:\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\\n\")\n",
    "    data = data[1:3466]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以下同様\n",
    "document = []\n",
    "for sentence in data:#14988 sentences\n",
    "    \n",
    "    new_sentence_list = []\n",
    "    sentence_list = sentence.split(\"\\n\")\n",
    "    \n",
    "    for token in sentence_list:\n",
    "        token_list = token.split(\" \")\n",
    "        new_sentence_list.append(token_list)\n",
    "    document.append(new_sentence_list)\n",
    "\n",
    "new_document = []\n",
    "for sentence in document:\n",
    "    new_sentence_list = []\n",
    "    for token_list in sentence:\n",
    "        try:\n",
    "            new_token_list = [token_list[0],token_list[1]+\"_\"+token_list[3],token_list[3]]\n",
    "            new_sentence_list.append(new_token_list)\n",
    "        except:\n",
    "            pass\n",
    "    new_document.append(new_sentence_list)\n",
    "\n",
    "label_token_dic = {}\n",
    "for sentence in new_document:\n",
    "    for token_list in sentence:\n",
    "        label = token_list[1]\n",
    "        token = token_list[0]\n",
    "        if label not in label_token_dic:\n",
    "            label_token_dic[label] = set()\n",
    "        label_token_dic[label].add(token)\n",
    "\n",
    "        BI_label_list = [label for label in list(label_token_dic.keys()) if \"B-\" in label or \"I-\" in label]\n",
    "\n",
    "BI_label_list = [label for label in list(label_token_dic.keys()) if \"B-\" in label or \"I-\" in label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid\n",
    "n = 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346490 / 346500"
     ]
    }
   ],
   "source": [
    "SimpleDA_BI(n*1,\"valid\")\n",
    "SimpleDA_BI(n*4,\"valid\")\n",
    "SimpleDA_BI(n*9,\"valid\")\n",
    "SimpleDA_BI(n*49,\"valid\")\n",
    "SimpleDA_BI(n*99,\"valid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
