{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ee95bf4",
   "metadata": {},
   "source": [
    "# 元々のjsonファイルを取得し，整形したものを出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c234cee",
   "metadata": {},
   "source": [
    "・ラベル名をtagsに統一\n",
    "\n",
    "・ラベルとして使用されていた数値IDを文字型に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6db445",
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\"words\": [\"CHICAGO\", \"AT\", \"ATLANTA\"], \"ner\": [\"B-ORG\", \"O\", \"B-LOC\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b297eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47c9c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/original/id2coarse_tags.json\",\"r\") as f:\n",
    "    label_coarse =json.loads(f.read())\n",
    "    \n",
    "with open(\"data/original/id2fine_tags.json\",\"r\") as f:\n",
    "    label_fine =json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d941d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coarse_str_tag(tags):\n",
    "    new_tag = [label_coarse[str(i)] for i in tags]\n",
    "    return new_tag\n",
    "    \n",
    "def fine_str_tag(tags):\n",
    "    new_tag = [label_fine[str(i)] for i in tags]\n",
    "    return new_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a61a1431",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for filename in [\"train\",\"test\",\"dev\"]:\n",
    "    \n",
    "    data = []\n",
    "    with open(\"data/original/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    coarse = []\n",
    "    for d in data:\n",
    "        new_d = {}\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        new_d[\"tags\"] = coarse_str_tag(d[\"coarse_tags\"])\n",
    "        coarse.append(new_d)\n",
    "    with open(\"data/coarse/\"+filename+\".json\",\"w\") as f:\n",
    "        for d in coarse:\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "    fine = []\n",
    "    for d in data:\n",
    "        new_d = {}\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        new_d[\"tags\"] = fine_str_tag(d[\"fine_tags\"])\n",
    "        fine.append(new_d)\n",
    "    with open(\"data/fine/\"+filename+\".json\",\"w\") as f:\n",
    "        for d in fine:\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e5b3fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'0': 'O',\n",
       "  '1': 'art',\n",
       "  '2': 'building',\n",
       "  '3': 'event',\n",
       "  '4': 'location',\n",
       "  '5': 'organization',\n",
       "  '6': 'other',\n",
       "  '7': 'person',\n",
       "  '8': 'product'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_coarse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94ffa3",
   "metadata": {},
   "source": [
    "# BI-DA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54559d2f",
   "metadata": {},
   "source": [
    "・BIラベルのみに着目したデータ拡張\n",
    "\n",
    "・交換率100％"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d0a37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def random_choice_token(token_set):\n",
    "    n = random.randint(0,len(token_set)-1)\n",
    "    token_list=list(token_set)\n",
    "    return token_list[n]\n",
    "\n",
    "def SimpleDA_BI(dataname,filename,n):\n",
    "    data = []\n",
    "    with open(\"data/\"+dataname+\"/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    alterna_labels = {}\n",
    "    if dataname == \"c\":\n",
    "        tag_json = \"data/original/id2coarse_tags.json\"\n",
    "    else:\n",
    "        tag_json = \"data/original/id2fine_tags.json\"\n",
    "    with open(tag_json,\"r\") as f:\n",
    "        label_coarse =json.loads(f.read())\n",
    "        label_list = list(label_coarse.values())[1:]\n",
    "    for label in label_list:\n",
    "        alterna_labels[label] = set()\n",
    "    for d in data:\n",
    "        for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "            if tag in label_list:\n",
    "                alterna_labels[tag].add(token)\n",
    "\n",
    "    data_f_path = \"data/\"+dataname+\"_BI_x\"+str(n)+\"/\"+filename+\".json\"\n",
    "    memo_f_path = \"log/\"+dataname+\"_BI_x\"+str(n)+\"_\"+filename+\"_memo.txt\"\n",
    "    \n",
    "    \n",
    "    with open(data_f_path,\"w\") as data_f, open(memo_f_path,\"w\") as memo_f:\n",
    "        for d in data:\n",
    "            data_f.write(json.dumps(d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "        add_n = n*len(data)-len(data)\n",
    "        print()\n",
    "        print(dataname,filename,\"add_n\",add_n)\n",
    "        \n",
    "        for now in range(add_n):\n",
    "            if now%10 == 0:\n",
    "                print('\\r%d / %d' %(now, add_n), end='')\n",
    "\n",
    "            x = random.randint(0,len(data)-1)\n",
    "            d = data[x]\n",
    "            new_d = dict()\n",
    "            new_d[\"tokens\"] = []\n",
    "            new_d[\"tags\"] = data[x][\"tags\"]\n",
    "\n",
    "            cnt = 0\n",
    "\n",
    "            for token,tag in zip(d[\"tokens\"],d[\"tags\"]):\n",
    "                if tag != \"O\":\n",
    "                    if tag in alterna_labels:\n",
    "                        token=random_choice_token(alterna_labels[tag]-{token})\n",
    "                        cnt = cnt + 1\n",
    "                    new_d[\"tokens\"].append(token)\n",
    "\n",
    "            data_f.write(json.dumps(new_d))\n",
    "            data_f.write(\"\\n\")\n",
    "\n",
    "            memo_f.write(str(x)+\"\\t\"+\" \".join(d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\" \"*len(str(x))+\"\\t\"+\" \".join(new_d[\"tokens\"])+\"\\n\")\n",
    "            memo_f.write(\"\\n\")\n",
    "\n",
    "        memo_f.write(str(cnt)+\"sentences augmented!!!! \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34373959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for dataname in [\"f\"]:\n",
    "#     for filename in [\"dev\",\"train\"]:\n",
    "#         SimpleDA_BI(dataname,filename,2)\n",
    "#         SimpleDA_BI(dataname,filename,5)\n",
    "#         SimpleDA_BI(dataname,filename,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d462a",
   "metadata": {},
   "source": [
    "# TreePOS-DA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf61280",
   "metadata": {},
   "source": [
    "・ 形態素情報付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c698dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import *\n",
    "import nltk\n",
    "from nltk.tree import *\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121ab005",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_269207/2297462148.py:8: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
      "/tmp/ipykernel_269207/2297462148.py:10: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n"
     ]
    }
   ],
   "source": [
    "java_path = \"jre1.8.0_333/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "parser =  'stanford-corenlp/stanford-parser-full-2020-11-17/stanford-parser.jar'\n",
    "parser_model = 'stanford-corenlp/stanford-corenlp-4.2.0-models-english.jar'\n",
    "\n",
    "#POSタグの分析用\n",
    "pos = StanfordParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "#係り受け関係の分析用\n",
    "dep_parser = StanfordDependencyParser(path_to_jar=parser, path_to_models_jar = parser_model)\n",
    "\n",
    "def POSTagAnalysis(text):\n",
    "    out = pos.raw_parse(text)\n",
    "    out = list(out)\n",
    "    tree = out[0]\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "db956fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "p = 127001#止まったところからスタート\n",
    "\n",
    "def TreePOS(filename):\n",
    "    data = []\n",
    "    with open(\"data/f/\"+filename+\".json\",\"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "            \n",
    "    new_document=[]\n",
    "    document_trees = []\n",
    "    c = p\n",
    "    \n",
    "    #タプル型で複数引数を渡す\n",
    "    for s in data[p:]:\n",
    "        \n",
    "        text = \" \".join(s[\"tokens\"])\n",
    "        tree = POSTagAnalysis(text)\n",
    "        document_trees.append(tree)\n",
    "        new_d = dict()\n",
    "        new_d[\"tokens\"] = []\n",
    "        new_d[\"tree_pos\"] = []\n",
    "        new_d[\"tags\"] = []\n",
    "\n",
    "        for i in range(len(tree.pos())):\n",
    "            new_d[\"tokens\"].append(tree.pos()[i][0])\n",
    "            new_d[\"tree_pos\"].append(tree.pos()[i][1])\n",
    "\n",
    "        new_d[\"tags\"]=s[\"tags\"]\n",
    "        new_document.append(new_d)\n",
    "        del tree\n",
    "        \n",
    "        print('\\r%d / %d' %(c, len(data)), end='')\n",
    "        \n",
    "        if (c % 1000 == 0 and c!=0) or c == len(data):\n",
    "            with open(\"data/Stanford_coreNLP/\"+filename+\"_\"+str(c)+\"_tree.binaryfile\", 'wb') as f:\n",
    "                pickle.dump(document_trees,f)\n",
    "            with open(\"data/Stanford_coreNLP/\"+filename+\"_\"+str(c)+\"_data.binaryfile\", 'wb') as f:\n",
    "                pickle.dump(new_document,f)\n",
    "            \n",
    "            print(\"\\nSAVED ==> \"+filename+\"_\"+str(c)+\"_tree.binaryfile\")\n",
    "            print(\"SAVED ==> \"+filename+\"_\"+str(c)+\"_data.binaryfile\")\n",
    "        c = c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5f9dd3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128000 / 131766\n",
      "SAVED ==> train_128000_tree.binaryfile\n",
      "SAVED ==> train_128000_data.binaryfile\n",
      "129000 / 131766\n",
      "SAVED ==> train_129000_tree.binaryfile\n",
      "SAVED ==> train_129000_data.binaryfile\n",
      "130000 / 131766\n",
      "SAVED ==> train_130000_tree.binaryfile\n",
      "SAVED ==> train_130000_data.binaryfile\n",
      "131000 / 131766\n",
      "SAVED ==> train_131000_tree.binaryfile\n",
      "SAVED ==> train_131000_data.binaryfile\n",
      "131765 / 131766"
     ]
    }
   ],
   "source": [
    "for filename in [\"train\"]:\n",
    "# for filename in [\"dev\"]:\n",
    "    TreePOS(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771c8b77",
   "metadata": {},
   "source": [
    "・pickleファイルを統合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "00300102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for path_sub in [\"dev\",\"train_1-51k\",\"train_52-81k\",\"train_81-131k\"]\n",
    "    path = \"data/Stanford_coreNLP/\"\n",
    "    files = os.listdir(path+path_sub+\"/\")\n",
    "    data_list = []\n",
    "    tree_list = []\n",
    "\n",
    "    for filename in files:\n",
    "        with open(path+path_sub+\"/\"+filename,'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        if \"data\" in filename:\n",
    "            data_list = data_list + data\n",
    "        else:\n",
    "            tree_list = tree_list + data\n",
    "\n",
    "    with open(path+path_sub+\"_data.binaryfile\", 'wb') as f:\n",
    "        pickle.dump(data_list,f)\n",
    "\n",
    "    with open(path+path_sub+\"_tree.binaryfile\", 'wb') as f:\n",
    "        pickle.dump(tree_list,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a53e7f09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Philippine peso sign -LRB- -RRB- is the currency symbol used for Philippine peso , the official currency of the Philippines .\n",
      "DT JJ NN NN -LRB- NN VBZ DT NN NN VBN IN JJ NN , DT JJ NN IN DT NNPS .\n",
      "O other-currency other-currency O O other-currency O O O O O O O other-currency other-currency O O O O O O location-GPE O\n",
      "22\n",
      "22\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "error_d = 0\n",
    "for path_sub in [\"dev\"]:\n",
    "    path = \"data/Stanford_coreNLP/\"\n",
    "\n",
    "    with open(path+path_sub+\"_data.binaryfile\",'rb') as rf:\n",
    "        data = pickle.load(rf)\n",
    "        \n",
    "    new_data = []\n",
    "    \n",
    "    for d in data:\n",
    "        new_d = dict()\n",
    "        new_d[\"tokens\"] = d[\"tokens\"]\n",
    "        try:\n",
    "            new_d[\"tags\"] = [str(d[\"tree_pos\"][j])+\"_\"+str(d[\"tags\"][j]) for j in range(len(d[\"tags\"]))]\n",
    "            new_data.append(new_d)\n",
    "        except:    \n",
    "            with open(path + \"error.json\", 'r') as f:\n",
    "                read_data = json.load(f)\n",
    "            save_data = [read_data, weight_dict]\n",
    "\n",
    "            with open(path + \"error.json\", 'w') as f:\n",
    "                for d in save_data:\n",
    "                    wf.write(json.dumps(d))\n",
    "                    wf.write(\"\\n\")\n",
    "            \n",
    "    with open(path + \"dev.json\",\"w\") as wf:\n",
    "        for d in new_data:\n",
    "            wf.write(json.dumps(d))\n",
    "            wf.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8538d401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_d[\"tokens\"][21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0209e7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_d[\"tags\"][22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8bace97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The   O\n",
      "Philippine   other-currency\n",
      "peso   other-currency\n",
      "sign   O\n",
      "-LRB-   O\n",
      "-RRB-   other-currency\n",
      "is   O\n",
      "the   O\n",
      "currency   O\n",
      "symbol   O\n",
      "used   O\n",
      "for   O\n",
      "Philippine   O\n",
      "peso   other-currency\n",
      ",   other-currency\n",
      "the   O\n",
      "official   O\n",
      "currency   O\n",
      "of   O\n",
      "the   O\n",
      "Philippines   O\n"
     ]
    }
   ],
   "source": [
    "for i in range(21):\n",
    "    print(error_d[\"tokens\"][i],\" \",error_d[\"tags\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a43589f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_d = 0\n",
    "with open(\"data/original/dev.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "for d in data:\n",
    "    if d[\"tokens\"] == error_d[\"tokens\"]:\n",
    "        correct_d = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d9cf42c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "22\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(correct_d[\"tokens\"]))\n",
    "print(len(correct_d[\"tree_pos\"]))\n",
    "print(len(correct_d[\"tags\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7a6f1a58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['The',\n",
       "  'Philippine',\n",
       "  'peso',\n",
       "  'sign',\n",
       "  '-LRB-',\n",
       "  '-RRB-',\n",
       "  'is',\n",
       "  'the',\n",
       "  'currency',\n",
       "  'symbol',\n",
       "  'used',\n",
       "  'for',\n",
       "  'Philippine',\n",
       "  'peso',\n",
       "  ',',\n",
       "  'the',\n",
       "  'official',\n",
       "  'currency',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Philippines',\n",
       "  '.'],\n",
       " 'tree_pos': ['DT',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'NN',\n",
       "  '-LRB-',\n",
       "  'NN',\n",
       "  'VBZ',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'NN',\n",
       "  'VBN',\n",
       "  'IN',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  ',',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'NNPS',\n",
       "  '.'],\n",
       " 'tags': ['O',\n",
       "  'other-currency',\n",
       "  'other-currency',\n",
       "  'O',\n",
       "  'O',\n",
       "  'other-currency',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'other-currency',\n",
       "  'other-currency',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'location-GPE',\n",
       "  'O']}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0d8d3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_list = []\n",
    "for path_sub in [\"train_1-51k\",\"train_52-81k\",\"train_81-131k\"]:\n",
    "    path = \"data/Stanford_coreNLP/\"\n",
    "    \n",
    "\n",
    "    with open(path+path_sub+\"_data.binaryfile\",'rb') as rf:\n",
    "        data = pickle.load(rf)\n",
    "        json_list = json_list + data\n",
    "        \n",
    "with open(path + \"train.json\",\"w\") as wf:\n",
    "    for d in data:\n",
    "        wf.write(json.dumps(d))\n",
    "        wf.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d4570f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\t\t       train\t\t\t    train_53000_data.binaryfile\r\n",
      "dev_2_data.binaryfile  train_52000_data.binaryfile  train_53000_tree.binaryfile\r\n",
      "dev_2_tree.binaryfile  train_52000_tree.binaryfile\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/Stanford_coreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "657c1fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/Stanford_coreNLP/train_53000_data.binaryfile\",'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a79a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
