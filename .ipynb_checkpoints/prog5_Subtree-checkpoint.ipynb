{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe5dd0a",
   "metadata": {},
   "source": [
    "# ↓23/8/17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e3a499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読込み用の関数\n",
    "def read_bin(filename):\n",
    "    with open(filename,'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "def save_bin(data,filename):\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(data,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e16dd8",
   "metadata": {},
   "source": [
    "### 構文木情報， tokenとtagのjson情報を読込"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e60fb63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_trees = []\n",
    "for num in range(0, 131501, 500):\n",
    "    with open('data/Stanford_coreNLP/train_'+str(num)+'-'+str(num+499)+'.json_tree.binaryfile', 'rb') as f:\n",
    "        a_train_tree = pickle.load(f)\n",
    "        train_trees.extend(a_train_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0f3c2fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train = []\n",
    "with open(\"data/f_id/train.json\",\"r\") as f:\n",
    "    for line in f:\n",
    "        train.append(json.loads(line))\n",
    "    \n",
    "# id検索用の辞書作成\n",
    "idkey_train = {}\n",
    "for d in train:\n",
    "    idkey_train[d[\"id\"]] = d\n",
    "    \n",
    "del train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d0ea9e",
   "metadata": {},
   "source": [
    "### 構文解析結果の処理に必要な関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183790ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from nltk.tree import Tree\n",
    "\n",
    "#部分木から具体的な単語を削除\n",
    "def remove_words(node):\n",
    "    for i, child in enumerate(node):\n",
    "        if isinstance(child, str):  # 葉ノード（単語）の場合\n",
    "            node[i] = ''  # 単語を空文字に置換\n",
    "        elif isinstance(child, Tree):\n",
    "            remove_words(child)\n",
    "\n",
    "# 各部分木、Treeタグ、トークンリスト、深さを取得\n",
    "def get_all_subtree_info(node, depth=0):\n",
    "    \n",
    "#     subtree用\n",
    "#     node_cp = copy.deepcopy(node)\n",
    "#     remove_words(node_cp)\n",
    "    subtree_tag = node.label()\n",
    "    subtree_tokens = [child for child in node.leaves()]\n",
    "    subtree_depth = depth\n",
    "\n",
    "    # 部分木の情報を辞書として格納\n",
    "    subtree_info = {\n",
    "#         'subtree': node_cp,\n",
    "        'tree_tag': subtree_tag,\n",
    "        'tokens': subtree_tokens,\n",
    "        'depth': subtree_depth\n",
    "    }\n",
    "    # 子ノードに対して再帰的に情報を収集\n",
    "    child_subtree_info = []\n",
    "    for child in node:\n",
    "        if isinstance(child, Tree):\n",
    "            child_subtree_info.extend(get_all_subtree_info(child, depth + 1))\n",
    "            \n",
    "    # 子ノードの情報を追加\n",
    "    subtree_info['children'] = child_subtree_info\n",
    "    return [subtree_info]\n",
    "\n",
    "\n",
    "# 全ての部分木情報をリスト内の対等な深さに格納\n",
    "def display_subtree_info(subtree_info, all_subtree):\n",
    "    new_d = copy.deepcopy(subtree_info)\n",
    "    \n",
    "    if \"children\" in new_d:\n",
    "        del new_d[\"children\"]\n",
    "     \n",
    "    all_subtree.append(new_d)\n",
    "    for child_info in subtree_info.get('children', []):\n",
    "        display_subtree_info(child_info,all_subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08e1955",
   "metadata": {},
   "source": [
    "### 部分木リストall_subtree（ner-tagを付ける前）を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "all_subtree = []\n",
    "n = len(train_trees)\n",
    "for i,tree in enumerate(train_trees):\n",
    "    print('\\r%d / %d' %(i, n), end='')\n",
    "    \n",
    "    # 全ての部分木情報を取得\n",
    "    all_subtree_info = get_all_subtree_info(tree)\n",
    "    subtree = []\n",
    "    \n",
    "    # 全ての部分木をリスト内の対等な深さに格納\n",
    "    for subtree_info in all_subtree_info:\n",
    "        display_subtree_info(subtree_info,subtree)\n",
    "    for d in subtree:\n",
    "        d[\"id\"] = i\n",
    "    all_subtree.extend(subtree)\n",
    "# subtreeを含む場合の具体的単語削除    \n",
    "# for d in all_subtree:\n",
    "#     remove_words(d[\"subtree\"])\n",
    "\n",
    "# 書き出し\n",
    "# with open(\"temp/all_subtree.binaryfile\", 'wb') as f:\n",
    "#     pickle.dump(all_subtree,f)\n",
    "\n",
    "# 読込み\n",
    "# with open(\"temp/all_subtree_rm_at.binaryfile\",'rb') as f:\n",
    "#     all_subtree = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9a314f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5849551 / 5849552"
     ]
    }
   ],
   "source": [
    "# all_subtreeのうち，交換対象にふさわしくないtree_tagを持つ部分木の削除\n",
    "　# 木全体であるROOT，'．''，'-LRB-','-RRB-'等tree_tag = tokenで種類がない物\n",
    "with open(\"temp/all_subtree.binaryfile\",'rb') as f:\n",
    "    all_subtree = pickle.load(f)\n",
    "\n",
    "remove_tag ={\"''\",',','-LRB-','-RRB-','.',':','``',\"ROOT\"}\n",
    "all_subtree_rm = []\n",
    "n = len(all_subtree)\n",
    "for i,d in enumerate(all_subtree):\n",
    "    print('\\r%d / %d' %(i, n), end='')\n",
    "    if d[\"tree_tag\"] not in remove_tag:\n",
    "        all_subtree_rm.append(d)\n",
    "        \n",
    "# with open(\"temp/all_subtree_rm.binaryfile\", 'wb') as f:\n",
    "#     pickle.dump(all_subtree_rm,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d702a498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5310312 / 5310313"
     ]
    }
   ],
   "source": [
    "# all_subtree_rmのうち試験的に単一トークンの削除 (remove a token)\n",
    "all_subtree_rm_at = []\n",
    "\n",
    "n = len(all_subtree_rm)\n",
    "for i,d in enumerate(all_subtree_rm):\n",
    "    print('\\r%d / %d' %(i, n), end='')\n",
    "    if len(d[\"tokens\"]) != 1:\n",
    "        all_subtree_rm_at.append(d)\n",
    "\n",
    "#all_subtree_rm_atを格納\n",
    "# with open(\"temp/all_subtree_rm_at.binaryfile\", 'wb') as f:\n",
    "#     pickle.dump(all_subtree_rm_at,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3a79ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読込み\n",
    "# with open(\"temp/all_subtree_rm_at.binaryfile\",'rb') as f:\n",
    "#     all_subtree_rm_at = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "65aa846d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_subtree_rm_atのtokenである-LRB-と-RRB-を（）に戻す\n",
    "for d in all_subtree_rm_at:\n",
    "    new_tokens = []\n",
    "    for t in d[\"tokens\"]:\n",
    "        if t == \"-LRB-\":\n",
    "            new_tokens.append(\"(\")\n",
    "        elif t == \"-RRB-\":\n",
    "            new_tokens.append(\")\")\n",
    "        else:\n",
    "            new_tokens.append(t)\n",
    "    d[\"tokens\"] = new_tokens\n",
    "                \n",
    "\n",
    "#all_subtree_rm_at(exchang完了済)を格納\n",
    "# with open(\"temp/all_subtree_rm_at_exc.binaryfile\", 'wb') as f:\n",
    "#     pickle.dump(all_subtree_rm_at,f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "1f7d7f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_subtree = all_subtree_rm_at\n",
    "# del all_subtree_rm\n",
    "# del all_subtree_rm_at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834f6a5",
   "metadata": {},
   "source": [
    "### 部分木リストall_subtreeに対応するner-tag情報を付与"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "0cb9fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定部分tokenに対応するtag取得関数\n",
    "\n",
    "def get_tags_for_tokens(sub_tokens,d):\n",
    "    match_cnt = 0 #トークン列の連続が複数含まれていた場合の対処\n",
    "    sub_tags_and_index = dict()\n",
    "    n = len(d[\"tokens\"])\n",
    "    n_sub = len(sub_tokens)\n",
    "    \n",
    "    # d[\"token\"]列からsub_tokensの数の連続するリストを切り出しsub_tokensと比較\n",
    "    for i in range(0,n-n_sub+1):#同じトークン列の繰返し出現に備え,forは止めない\n",
    "        partof_d_tokens = d[\"tokens\"][i:i+n_sub]\n",
    "        if partof_d_tokens == sub_tokens:\n",
    "            sub_tags_and_index[\"sub_tags\"] = d[\"tags\"][i:i+n_sub]\n",
    "            sub_tags_and_index[\"start_index\"] = i\n",
    "            match_cnt += 1\n",
    "            \n",
    "    if match_cnt == 1:\n",
    "        return sub_tags_and_index\n",
    "    else:\n",
    "        return dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f330bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# all_subtree=read_bin(\"temp/all_subtree_rm_at_exc.binaryfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "04fa858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2082703 / 2082704"
     ]
    }
   ],
   "source": [
    "# 部分木のtag情報付与 更新版\n",
    "# need:idkey_train\n",
    "success_subtree = []#1854621\n",
    "fail_subtree = []#228083\n",
    "\n",
    "n = len(all_subtree)\n",
    "for i,sub_d in enumerate(all_subtree):#2082704\n",
    "    print('\\r%d / %d' %(i, n), end='')\n",
    "    try:\n",
    "        sub_tags_and_index = get_tags_for_tokens(sub_d[\"tokens\"],idkey_train[sub_d[\"id\"]])\n",
    "        \n",
    "        sub_d[\"tags\"] = sub_tags_and_index[\"sub_tags\"]\n",
    "        sub_d[\"start_index\"] = sub_tags_and_index[\"start_index\"]\n",
    "        success_subtree.append(sub_d)\n",
    "        \n",
    "    except Exception as e:\n",
    "        fail_subtree.append(sub_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6a62ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_bin(success_subtree,\"temp/success_subtree.binaryfile\") \n",
    "# save_bin(fail_subtree,\"temp/fail_subtree.binaryfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65137efa",
   "metadata": {},
   "source": [
    "### A - B をA-Bに戻す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "6531ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "success_subtree = read_bin(\"temp/success_subtree.binaryfile\")\n",
    "fail_subtree = read_bin(\"temp/fail_subtree.binaryfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "a22bf2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定部分tokenに対応するtag取得関数 A - BとA-Bの違い対応版\n",
    "\n",
    "def get_tags_for_diff_len_tokens(sub_tokens,d):\n",
    "    sub_tags_and_index = dict()\n",
    "    sub_s = \"\".join(sub_tokens)\n",
    "    stop_i_roop = 0\n",
    "    \n",
    "    for i in range(len(d[\"tokens\"])):\n",
    "        n = len(d[\"tokens\"][i])\n",
    "        if stop_i_roop == 1:\n",
    "            break\n",
    "            \n",
    "        if sub_s[0:n] == d[\"tokens\"][i]:\n",
    "            part_of_d = d[\"tokens\"][i]\n",
    "            end = 0\n",
    "            \n",
    "            for j in range(i+1,len(d[\"tokens\"])):\n",
    "                if d[\"tokens\"][j] in sub_s:\n",
    "                    part_of_d += d[\"tokens\"][j]\n",
    "                    if sub_s == part_of_d:\n",
    "                        end = j+1\n",
    "                        stop_i_roop = 1\n",
    "                        break\n",
    "    \n",
    "    sub_tags_and_index[\"sub_tokens\"] = d[\"tokens\"][i:j+1]\n",
    "    sub_tags_and_index[\"sub_tags\"] = d[\"tags\"][i:j+1]\n",
    "    sub_tags_and_index[\"start_index\"] = i\n",
    "    return sub_tags_and_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "4ef75981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228082 / 228083"
     ]
    }
   ],
   "source": [
    "fail_subtree_2 = []#5159\n",
    "\n",
    "n = len(fail_subtree)\n",
    "for i,sub_d in enumerate(fail_subtree):#228083\n",
    "    print('\\r%d / %d' %(i, n), end='')\n",
    "    try:\n",
    "        sub_tokens_tags_index = get_tags_for_diff_len_tokens(sub_d[\"tokens\"],idkey_train[sub_d[\"id\"]])\n",
    "        sub_d[\"tokens\"] = sub_tokens_tags_index[\"sub_tokens\"]\n",
    "        sub_d[\"tags\"] = sub_tokens_tags_index[\"sub_tags\"]\n",
    "        sub_d[\"start_index\"] = sub_tokens_tags_index[\"start_index\"]\n",
    "        success_subtree.append(sub_d)\n",
    "        \n",
    "    except Exception as e:\n",
    "        fail_subtree_2.append(sub_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "b6a3d0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2077544 / 2077545"
     ]
    }
   ],
   "source": [
    "# success_subtreeのうち試験的に単一トークンの削除 (remove a token)\n",
    "success_subtree_at = []\n",
    "\n",
    "n = len(success_subtree)\n",
    "for i,d in enumerate(success_subtree):\n",
    "    print('\\r%d / %d' %(i, n), end='')\n",
    "    if len(d[\"tokens\"]) != 1:\n",
    "        success_subtree_at.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "442eea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_bin(success_subtree_at,\"temp/success_subtree_A-B.binaryfile\")\n",
    "# save_bin(fail_subtree_2,\"temp/fail_subtree_A-B.binaryfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6b09c",
   "metadata": {},
   "source": [
    "# ↓23/8/18\n",
    "【今後の予定】  \n",
    "・もとのsmallにはidがついていない＝＞3/167（ID付き）を新たに10作成　【18日完了】  \n",
    "・その文番号のみのsmall_subtreeを新たに作成 【18日完了】  \n",
    "・交換先の選定　【18日完了】  \n",
    "・交換時の比率を決定＝＞1文につき1か所  【18日完了】  \n",
    "・交換候補からROOTや.を削除 【19日完了】  \n",
    "・交換後の文生成    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638dbbfd",
   "metadata": {},
   "source": [
    "### id付きの3/167データ×10を新たに準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd300cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = []\n",
    "# with open(\"data/f_id/train.json\",\"r\") as f:\n",
    "#     for line in f:\n",
    "#         train.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bce137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"art-broadcastprogram\",\"art-film\",\"art-music\",\"art-other\",\"art-painting\",\"art-writtenart\",\"building-airport\",\"building-hospital\",\"building-hotel\",\"building-library\",\"building-other\",\"building-restaurant\",\"building-sportsfacility\",\"building-theater\",\"event-attack/battle/war/militaryconflict\",\"event-disaster\",\"event-election\",\"event-other\",\"event-protest\",\"event-sportsevent\",\"location-GPE\",\"location-bodiesofwater\",\"location-island\",\"location-mountain\",\"location-other\",\"location-park\",\"location-road/railway/highway/transit\",\"organization-company\",\"organization-education\",\"organization-government/governmentagency\",\"organization-media/newspaper\",\"organization-other\",\"organization-politicalparty\",\"organization-religion\",\"organization-showorganization\",\"organization-sportsleague\",\"organization-sportsteam\",\"other-astronomything\",\"other-award\",\"other-biologything\",\"other-chemicalthing\",\"other-currency\",\"other-disease\",\"other-educationaldegree\",\"other-god\",\"other-language\",\"other-law\",\"other-livingthing\",\"other-medical\",\"person-actor\",\"person-artist/author\",\"person-athlete\",\"person-director\",\"person-other\",\"person-politician\",\"person-scholar\",\"person-soldier\",\"product-airplane\",\"product-car\",\"product-food\",\"product-game\",\"product-other\",\"product-ship\",\"product-software\",\"product-train\",\"product-weapon\",\"O\"]\n",
    "# 66種のラベルごとに対応するリスト ラベルを含むtrainの各要素を含むリストが入っている値\n",
    "labeled_data = []\n",
    "for label in labels:\n",
    "    new_l = []\n",
    "    for d in train:\n",
    "        if label == \"O\":\n",
    "            if {\"O\"} == set(d[\"tags\"]):\n",
    "                new_l.append(d)\n",
    "        else:\n",
    "            if label in set(d[\"tags\"]):\n",
    "                new_l.append(d)\n",
    "    labeled_data.append(new_l)#listに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f24299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3/167スケールのデータを新たに10作成（id付き）\n",
    "import random\n",
    "n = 3\n",
    "for ver in range(0,10):\n",
    "    labeled_data_elected = []#21548\n",
    "    for i in range(len(labeled_data)):\n",
    "        choice_num = int(len(labeled_data[i])*n/166)\n",
    "        print(choice_num,\"/\",len(labeled_data[i]),end = \"    \")\n",
    "        choice_list = random.sample(labeled_data[i],choice_num)\n",
    "        labeled_data_elected.extend(choice_list)\n",
    "    print(len(labeled_data_elected))\n",
    "\n",
    "    #f_smallを作成\n",
    "    with open(\"data/f_small_id_3-167/original/\"+str(ver)+\".json\", 'w') as f:\n",
    "        for d in labeled_data_elected:\n",
    "            f.write(json.dumps(d))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71483532",
   "metadata": {},
   "source": [
    "# ↓23/8/19\n",
    "【今後の予定】  \n",
    "・きちんと部分一致するtoken群を探すべき【20日完了】\n",
    "→ tokenひとつひとつを検索した場合，一番最初の全く違うトークンを拾ってきてしまう可能性がある  \n",
    "・交換後の文生成【20日完了】"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22633f87",
   "metadata": {},
   "source": [
    "### 新たに生成したデータの読み込み，DA加工"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91216e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 深さとタグから候補部分木をリストで返す関数\n",
    "\n",
    "# 引数であるsentidにset又はidをとり，sentidに一致するidのsubreeだけを返す関数\n",
    "def search_d_sentid(sbts_list,q_id):# max(depth):34\n",
    "    found_sbts = []\n",
    "    for d in sbts_list:\n",
    "        if type(q_id) == set:\n",
    "            if d[\"id\"] in q_id:\n",
    "                found_sbts.append(d)\n",
    "        elif type(q_id) == int:\n",
    "            if d[\"id\"] == q_id:\n",
    "                found_sbts.append(d)\n",
    "    return found_sbts\n",
    "\n",
    "# 深さとタグから候補部分木をリストで返す関数\n",
    "def search_d_depth_and_tag(sbts_list,q_depth,q_tree_tag):# max(depth):34\n",
    "    found_sbts = []\n",
    "    for d in sbts_list:\n",
    "        if d[\"depth\"] == q_depth and d[\"tree_tag\"] == q_tree_tag:\n",
    "            found_sbts.append(d)\n",
    "    return found_sbts\n",
    "\n",
    "# タグのみから候補部分木をリストで返す関数\n",
    "def search_d_tag(sbts_list,q_tree_tag):# max(depth):34\n",
    "    found_sbts = []\n",
    "    for d in sbts_list:\n",
    "        if d[\"tree_tag\"] == q_tree_tag:\n",
    "            found_sbts.append(d)\n",
    "    return found_sbts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5817a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定部分木tokenを交換用部分木tokenに入れ替え，tagを対応させた辞書を返す関数\n",
    "def swap_tokens(f_d,rand_sbt_in_s,rand_exch_pair):\n",
    "    new_d = dict()\n",
    "    i = rand_sbt_in_s[\"start_index\"]\n",
    "    n = len(rand_sbt_in_s[\"tokens\"])\n",
    "    new_d[\"tokens\"] = f_d[\"tokens\"][:i]+rand_exch_pair[\"tokens\"]+f_d[\"tokens\"][i+n:]\n",
    "    new_d[\"tags\"] = f_d[\"tags\"][:i]+rand_exch_pair[\"tags\"]+f_d[\"tags\"][i+n:]\n",
    "#     new_d[\"ids\"] = f_d[\"id\"]\n",
    "    return new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9adaab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "subtrees = read_bin(\"temp/success_subtree_A-B.binaryfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ef08614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9) 4191 / 4192"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "no_sbt_s = set()\n",
    "no_exc_sbt = set()\n",
    "\n",
    "for n in range(1,10):\n",
    "    train_f = []# 該当ファイルのjsonデータ\n",
    "    with open(\"data/f_small_id_3-167/original/\"+str(n)+\".json\", \"r\") as f:\n",
    "        for line in f:\n",
    "            train_f.append(json.loads(line))        \n",
    "    len_train = len(train_f)\n",
    "    ids_in_f = set([d[\"id\"] for d in train_f])\n",
    "    sbts_in_f = [d for d in subtrees if d[\"id\"] in ids_in_f]\n",
    "    \n",
    "    data_f_path = \"data/SubTree/\"+str(n)+\"_x10.json\"\n",
    "    log_f_path = \"log/SubTree_DA_log/\"+str(n)+\"_log.txt\"\n",
    "    \n",
    "    with open(data_f_path,\"w\",encoding='utf-8') as data_f, open(log_f_path,\"w\",encoding='utf-8') as log_f:\n",
    "        for i,d in enumerate(train_f):\n",
    "            print('\\r(%d) %d / %d' %(n,i,len_train), end='')\n",
    "\n",
    "            data_f.write(json.dumps({\"tokens\":d[\"tokens\"],\"tags\":d[\"tags\"]}))#データを含める\n",
    "            data_f.write(\"\\n\")        \n",
    "            log_f.write(\"★ \"+\" \".join(d[\"tokens\"])+\"\\n\")# 元データのログ\n",
    "\n",
    "            for _ in range(10):\n",
    "                sbts_in_s = search_d_sentid(sbts_in_f,d[\"id\"])# 部分木群\n",
    "                \n",
    "                if len(sbts_in_s) == 0:# 部分木の無い文を報告\n",
    "                    no_sbt_s.add(d[\"id\"])\n",
    "                    log_f.write(\" 　【no-subtree-ERROR】[id:\"+str(d[\"id\"])+\"]\\n\")# 交換のログ出力\n",
    "                    break\n",
    "                else:\n",
    "                    rand_sbt = random.choice(sbts_in_s)# ランダム部分木一つ\n",
    "                    \n",
    "                    exch_pairs = search_d_depth_and_tag(\n",
    "                        sbts_in_f,rand_sbt[\"depth\"],rand_sbt[\"tree_tag\"])# 交換候補群\n",
    "                    \n",
    "                    if len(exch_pairs) == 0:# 交換候補の無い部分木を報告\n",
    "                        no_exc_sbt.add(rand_sbt)\n",
    "                        log_f.write(\" 　【no-pair-ERROR】[\"+\" \".join(rand_sbt[\"tokens\"])+\"]\\n\")# 交換のログ出力\n",
    "                        break\n",
    "                    else:\n",
    "                        rand_exch_pair = random.choice(exch_pairs)# ランダム交換候補一つ\n",
    "                        \n",
    "                        log_f.write(\" 　・ \"+\" \".join(rand_sbt[\"tokens\"])+\n",
    "                        \" => \"+\" \".join(rand_exch_pair[\"tokens\"])+\"\\n\")# 交換のログ出力\n",
    "\n",
    "                        new_d = swap_tokens(d,rand_sbt,rand_exch_pair)\n",
    "                        data_f.write(json.dumps(new_d))\n",
    "                        data_f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1769d87",
   "metadata": {},
   "source": [
    "# 23/8/20 & 21　\n",
    "\n",
    "【今後の予定】  \n",
    "・エラーで止まってしまったので対応【20日完了】  \n",
    "・エラーのチェック → A-Bのエラー修正【21日完了】 \n",
    "・データ完成 【21日完了】  \n",
    "・データをFlowに送り学習【22日完了】  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab472dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
